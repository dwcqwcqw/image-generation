#!/usr/bin/env python3
"""
ä¿®å¤åçš„æ™ºèƒ½Promptå‹ç¼©æµ‹è¯•è„šæœ¬
æµ‹è¯•å»é‡åŠŸèƒ½å’Œnegative promptå‹ç¼©
"""

import re

def compress_prompt_to_77_tokens(prompt: str, max_tokens: int = 75) -> str:
    """
    æ™ºèƒ½å‹ç¼©promptåˆ°æŒ‡å®štokenæ•°é‡ä»¥å†…
    ä¿ç•™æœ€é‡è¦çš„å…³é”®è¯å’Œæè¿°
    """
    # è®¡ç®—å½“å‰tokenæ•°é‡
    token_pattern = r'\w+|[^\w\s]'
    current_tokens = len(re.findall(token_pattern, prompt.lower()))
    
    if current_tokens <= max_tokens:
        return prompt
    
    print(f"ğŸ”§ å‹ç¼©prompt: {current_tokens} tokens -> {max_tokens} tokens")
    
    # å®šä¹‰é‡è¦æ€§æƒé‡
    priority_keywords = {
        # è´¨é‡æ ‡ç­¾ - æœ€é«˜ä¼˜å…ˆçº§
        'quality': ['masterpiece', 'best quality', 'amazing quality', 'high quality', 'ultra quality'],
        # ä¸»ä½“æè¿° - é«˜ä¼˜å…ˆçº§  
        'subject': ['man', 'boy', 'male', 'muscular', 'handsome', 'lean', 'naked', 'nude'],
        # èº«ä½“éƒ¨ä½ - ä¸­é«˜ä¼˜å…ˆçº§
        'anatomy': ['torso', 'chest', 'abs', 'penis', 'erect', 'flaccid', 'body'],
        # åŠ¨ä½œå§¿æ€ - ä¸­ä¼˜å…ˆçº§
        'pose': ['reclining', 'lying', 'sitting', 'standing', 'pose', 'position'],
        # ç¯å¢ƒé“å…· - ä¸­ä¼˜å…ˆçº§
        'environment': ['bed', 'sheets', 'satin', 'luxurious', 'room', 'background'],
        # å…‰å½±æ•ˆæœ - ä½ä¼˜å…ˆçº§
        'lighting': ['lighting', 'illuminated', 'soft', 'moody', 'warm', 'cinematic'],
        # æƒ…æ„Ÿè¡¨è¾¾ - ä½ä¼˜å…ˆçº§
        'emotion': ['serene', 'intense', 'confident', 'contemplation', 'allure']
    }
    
    # ğŸš¨ ä¿®å¤ï¼šä½¿ç”¨setæ¥è·Ÿè¸ªå·²æ·»åŠ çš„è¯ï¼Œé¿å…é‡å¤
    words = prompt.split()
    used_words = set()  # è·Ÿè¸ªå·²ä½¿ç”¨çš„è¯
    compressed_parts = []
    remaining_tokens = max_tokens
    
    # æŒ‰ä¼˜å…ˆçº§å¤„ç†
    priority_order = ['quality', 'subject', 'anatomy', 'pose', 'environment', 'lighting', 'emotion']
    
    for category in priority_order:
        if remaining_tokens <= 5:  # é¢„ç•™ä¸€äº›ç©ºé—´
            break
            
        category_keywords = priority_keywords[category]
        
        # æ‰¾åˆ°å±äºè¿™ä¸ªç±»åˆ«çš„è¯
        for word in words:
            if remaining_tokens <= 0:
                break
                
            word_clean = word.lower().strip('.,!?;:')
            
            # æ£€æŸ¥æ˜¯å¦å±äºå½“å‰ç±»åˆ« ä¸” æ²¡æœ‰è¢«ä½¿ç”¨è¿‡
            if word_clean not in used_words and any(keyword in word_clean for keyword in category_keywords):
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if word_tokens <= remaining_tokens:
                    compressed_parts.append(word)
                    used_words.add(word_clean)
                    remaining_tokens -= word_tokens
    
    # ğŸš¨ ä¿®å¤ï¼šå¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œæ·»åŠ å…¶ä»–é‡è¦ä½†æœªåˆ†ç±»çš„è¯ï¼ˆé¿å…é‡å¤ï¼‰
    if remaining_tokens > 0:
        for word in words:
            if remaining_tokens <= 0:
                break
                
            word_clean = word.lower().strip('.,!?;:')
            if word_clean not in used_words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if word_tokens <= remaining_tokens:
                    compressed_parts.append(word)
                    used_words.add(word_clean)
                    remaining_tokens -= word_tokens
    
    compressed_prompt = ' '.join(compressed_parts)
    final_tokens = len(re.findall(token_pattern, compressed_prompt.lower()))
    
    print(f"âœ… å‹ç¼©å®Œæˆ: '{compressed_prompt}' ({final_tokens} tokens)")
    return compressed_prompt

def count_tokens(text: str) -> int:
    """è®¡ç®—tokenæ•°é‡"""
    token_pattern = r'\w+|[^\w\s]'
    return len(re.findall(token_pattern, text.lower()))

def check_duplicates(text: str) -> list:
    """æ£€æŸ¥é‡å¤è¯æ±‡"""
    words = [word.lower().strip('.,!?;:') for word in text.split()]
    duplicates = []
    seen = set()
    
    for word in words:
        if word in seen:
            duplicates.append(word)
        else:
            seen.add(word)
    
    return duplicates

def test_fixed_compression():
    """æµ‹è¯•ä¿®å¤åçš„å‹ç¼©åŠŸèƒ½"""
    print("ğŸ§ª ä¿®å¤åçš„æ™ºèƒ½Promptå‹ç¼©æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            'name': 'è¶…é•¿positive prompt (114 tokens)',
            'prompt': 'masterpiece, best quality, amazing quality, A lean, muscular, and handsome man reclining on a bed with luxurious satin sheets. His chiseled torso is partially illuminated by soft, moody lighting that accentuates the contours of his muscles. One arm is tucked under his head, creating a relaxed yet confident pose. His expression is serene yet intense, with a gaze that suggests quiet contemplation. The satin sheets gently reflect the light, adding a sense of elegance and intimacy to the scene. The overall atmosphere is warm, sensual, and cinematic, evoking a timeless allure. erect penis, flaccid penis',
            'type': 'positive'
        },
        {
            'name': 'è¶…é•¿negative prompt (90 tokens)', 
            'prompt': 'bad quality, worst quality, low quality, normal quality, lowres, blurry, fuzzy, out of focus, bad anatomy, bad hands, missing fingers, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, ugly, bad proportions, gross proportions, disfigured, out of frame, extra limbs, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, cross-eyed',
            'type': 'negative'
        },
        {
            'name': 'ä¸­ç­‰é•¿åº¦prompt (47 tokens)',
            'prompt': 'masterpiece, best quality, muscular handsome man, naked, sitting on bed, soft lighting, detailed anatomy, erect penis',
            'type': 'positive'
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {test_case['name']}")
        print("-" * 40)
        
        original = test_case['prompt']
        original_tokens = count_tokens(original)
        
        print(f"åŸå§‹{test_case['type']} prompt ({original_tokens} tokens):")
        print(f"'{original[:200]}{'...' if len(original) > 200 else ''}'")
        print()
        
        if original_tokens > 75:
            compressed = compress_prompt_to_77_tokens(original, max_tokens=75)
            compressed_tokens = count_tokens(compressed)
            
            # æ£€æŸ¥é‡å¤è¯æ±‡
            duplicates = check_duplicates(compressed)
            
            print(f"\nğŸ“Š å‹ç¼©ç»“æœ:")
            print(f"  åŸå§‹: {original_tokens} tokens")
            print(f"  å‹ç¼©: {compressed_tokens} tokens")
            print(f"  å‹ç¼©ç‡: {((original_tokens - compressed_tokens) / original_tokens * 100):.1f}%")
            
            if duplicates:
                print(f"  âŒ å‘ç°é‡å¤è¯æ±‡: {', '.join(set(duplicates))}")
            else:
                print(f"  âœ… æ— é‡å¤è¯æ±‡")
            
            # åˆ†æä¿ç•™çš„å…³é”®è¯
            original_words = set(original.lower().split())
            compressed_words = set(compressed.lower().split())
            preserved_words = compressed_words.intersection(original_words)
            
            print(f"  ä¿ç•™è¯æ±‡: {len(preserved_words)}/{len(original_words)} ({len(preserved_words)/len(original_words)*100:.1f}%)")
        else:
            print("âœ… æ— éœ€å‹ç¼©ï¼Œå·²åœ¨75 tokené™åˆ¶å†…")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ ä¿®å¤æµ‹è¯•å®Œæˆï¼")
    print("ğŸ”§ ä¸»è¦ä¿®å¤:")
    print("  âœ… å»é™¤é‡å¤è¯æ±‡")
    print("  âœ… æ”¯æŒnegative promptå‹ç¼©")
    print("  âœ… ä¼˜åŒ–å‹ç¼©ç®—æ³•")

if __name__ == "__main__":
    test_fixed_compression() 