# æ¢è„¸ä¼˜åŒ–ç³»ç»Ÿéƒ¨ç½²æŒ‡å—
## Face Swap Optimization System Deployment Guide

### ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ä¼˜åŒ–åçš„æ¢è„¸ç³»ç»Ÿï¼Œç¡®ä¿è·å¾—æœ€ä½³æ€§èƒ½å’Œè´¨é‡ã€‚

### ğŸ“‹ ç³»ç»Ÿè¦æ±‚

#### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU with CUDA 12.x support (æ¨è RTX 4090/A100)
- **å†…å­˜**: è‡³å°‘ 16GB RAM (æ¨è 32GB+)
- **å­˜å‚¨**: è‡³å°‘ 50GB å¯ç”¨ç©ºé—´
- **CPU**: 8æ ¸å¿ƒä»¥ä¸Š (æ¨è 16æ ¸å¿ƒ+)

#### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ / CentOS 8+ / Docker
- **Python**: 3.9-3.11 (æ¨è 3.10)
- **CUDA**: 12.0+ (æ¨è 12.1)
- **Docker**: 20.10+ (å¦‚ä½¿ç”¨å®¹å™¨éƒ¨ç½²)

### ğŸš€ å¿«é€Ÿéƒ¨ç½²

#### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv face_swap_env
source face_swap_env/bin/activate  # Linux/Mac
# face_swap_env\Scripts\activate  # Windows

# å‡çº§pip
pip install --upgrade pip
```

#### 2. å®‰è£…CUDAä¾èµ–

```bash
# å®‰è£…NVIDIA PythonåŒ…ç´¢å¼•
pip install nvidia-pyindex

# å®‰è£…CUDAåº“ (ç”Ÿäº§ç¯å¢ƒå¿…éœ€)
pip install --extra-index-url https://pypi.nvidia.com nvidia-cublas-cu12
pip install --extra-index-url https://pypi.nvidia.com nvidia-cudnn-cu12
pip install --extra-index-url https://pypi.nvidia.com nvidia-cufft-cu12
pip install --extra-index-url https://pypi.nvidia.com nvidia-curand-cu12
pip install --extra-index-url https://pypi.nvidia.com nvidia-cusolver-cu12
pip install --extra-index-url https://pypi.nvidia.com nvidia-cusparse-cu12
pip install --extra-index-url https://pypi.nvidia.com nvidia-nvjitlink-cu12
```

#### 3. å®‰è£…æ ¸å¿ƒä¾èµ–

```bash
# å®‰è£…ONNX Runtime GPUç‰ˆæœ¬
pip uninstall onnxruntime -y
pip install onnxruntime-gpu

# å®‰è£…å…¶ä»–ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install insightface
pip install opencv-python
pip install gfpgan
pip install Pillow
pip install numpy
pip install boto3
```

#### 4. æ¨¡å‹æ–‡ä»¶éƒ¨ç½²

```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p /runpod-volume/models/insightface/models
mkdir -p /runpod-volume/models/gfpgan

# ä¸‹è½½æ¨¡å‹æ–‡ä»¶ (éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´è·¯å¾„)
# InsightFaceæ¨¡å‹
wget -O /runpod-volume/models/insightface/models/inswapper_128.onnx \
  "https://github.com/facefusion/facefusion-assets/releases/download/models/inswapper_128.onnx"

# Buffaloæ¨¡å‹ (é«˜ç²¾åº¦)
wget -O /runpod-volume/models/insightface/models/buffalo_sc.zip \
  "https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_sc.zip"
unzip /runpod-volume/models/insightface/models/buffalo_sc.zip -d /runpod-volume/models/insightface/models/

# GFPGANæ¨¡å‹
wget -O /runpod-volume/models/gfpgan/GFPGANv1.4.pth \
  "https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth"
```

#### 5. ç¯å¢ƒå˜é‡é…ç½®

```bash
# åˆ›å»ºç¯å¢ƒé…ç½®æ–‡ä»¶
cat > .env << EOF
# Cloudflare R2 é…ç½®
CLOUDFLARE_R2_ACCESS_KEY=your_access_key
CLOUDFLARE_R2_SECRET_KEY=your_secret_key
CLOUDFLARE_R2_BUCKET=your_bucket_name
CLOUDFLARE_R2_ENDPOINT=https://your_account_id.r2.cloudflarestorage.com
CLOUDFLARE_R2_PUBLIC_DOMAIN=https://your_custom_domain.com

# CUDAé…ç½®
CUDA_VISIBLE_DEVICES=0
NVIDIA_VISIBLE_DEVICES=all

# æ¨¡å‹è·¯å¾„é…ç½®
FACE_SWAP_MODEL_PATH=/runpod-volume/models/insightface/models/inswapper_128.onnx
FACE_ANALYSIS_MODEL_PATH=/runpod-volume/models/insightface/models/buffalo_sc
GFPGAN_MODEL_PATH=/runpod-volume/models/gfpgan/GFPGANv1.4.pth
EOF
```

### ğŸ³ Dockeréƒ¨ç½²

#### 1. Dockerfile

```dockerfile
FROM nvidia/cuda:12.1-devel-ubuntu20.04

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-pip \
    python3.10-dev \
    wget \
    unzip \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºå·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶requirementsæ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip3.10 install --no-cache-dir -r requirements.txt

# å®‰è£…CUDAä¾èµ–
RUN pip3.10 install nvidia-pyindex && \
    pip3.10 install --extra-index-url https://pypi.nvidia.com nvidia-cublas-cu12 && \
    pip3.10 install --extra-index-url https://pypi.nvidia.com nvidia-cudnn-cu12 && \
    pip3.10 install --extra-index-url https://pypi.nvidia.com nvidia-cufft-cu12 && \
    pip3.10 install --extra-index-url https://pypi.nvidia.com nvidia-curand-cu12 && \
    pip3.10 install --extra-index-url https://pypi.nvidia.com nvidia-cusolver-cu12 && \
    pip3.10 install --extra-index-url https://pypi.nvidia.com nvidia-cusparse-cu12 && \
    pip3.10 install --extra-index-url https://pypi.nvidia.com nvidia-nvjitlink-cu12

# å®‰è£…ONNX Runtime GPU
RUN pip3.10 uninstall onnxruntime -y && \
    pip3.10 install onnxruntime-gpu

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºæ¨¡å‹ç›®å½•
RUN mkdir -p /runpod-volume/models

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3.10", "handler.py"]
```

#### 2. docker-compose.yml

```yaml
version: '3.8'

services:
  face-swap-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/runpod-volume/models
      - ./logs:/app/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

#### 3. æ„å»ºå’Œè¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f face-swap-api
```

### âš™ï¸ é…ç½®ä¼˜åŒ–

#### 1. æ€§èƒ½é…ç½®

```python
# handler.py ä¸­çš„ä¼˜åŒ–é…ç½®
OPTIMIZATION_CONFIG = {
    # æ£€æµ‹é…ç½®
    "detection": {
        "high_quality": {
            "det_size": (1024, 1024),
            "det_thresh": 0.4,
            "multi_scale": True
        },
        "balanced": {
            "det_size": (640, 640),
            "det_thresh": 0.45,
            "multi_scale": False
        },
        "fast": {
            "det_size": (512, 512),
            "det_thresh": 0.5,
            "multi_scale": False
        }
    },
    
    # æ··åˆé…ç½®
    "blending": {
        "dynamic_ratio": True,
        "region_aware": True,
        "lighting_match": True,
        "base_ratio": 0.88
    },
    
    # å¢å¼ºé…ç½®
    "enhancement": {
        "adaptive_strength": True,
        "smart_blending": True,
        "preserve_original": 0.15
    }
}
```

#### 2. å†…å­˜ä¼˜åŒ–

```python
# å†…å­˜ç®¡ç†é…ç½®
MEMORY_CONFIG = {
    "max_image_size": (2048, 2048),
    "batch_size": 1,
    "cache_models": True,
    "clear_cache_interval": 100
}
```

### ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

#### 1. æ€§èƒ½ç›‘æ§

```python
# æ·»åŠ åˆ°handler.py
import psutil
import GPUtil

def monitor_system_resources():
    """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
    # CPUä½¿ç”¨ç‡
    cpu_percent = psutil.cpu_percent(interval=1)
    
    # å†…å­˜ä½¿ç”¨ç‡
    memory = psutil.virtual_memory()
    memory_percent = memory.percent
    
    # GPUä½¿ç”¨ç‡
    gpus = GPUtil.getGPUs()
    gpu_usage = gpus[0].load * 100 if gpus else 0
    gpu_memory = gpus[0].memoryUtil * 100 if gpus else 0
    
    return {
        "cpu_percent": cpu_percent,
        "memory_percent": memory_percent,
        "gpu_usage": gpu_usage,
        "gpu_memory": gpu_memory
    }
```

#### 2. æ—¥å¿—é…ç½®

```python
import logging
from logging.handlers import RotatingFileHandler

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        RotatingFileHandler(
            'logs/face_swap.log',
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        ),
        logging.StreamHandler()
    ]
)
```

### ğŸ”§ æ•…éšœæ’é™¤

#### 1. CUDAé—®é¢˜

```bash
# æ£€æŸ¥CUDAå®‰è£…
nvidia-smi
nvcc --version

# æ£€æŸ¥ONNX Runtime CUDAæ”¯æŒ
python -c "import onnxruntime as ort; print('CUDA available:', 'CUDAExecutionProvider' in ort.get_available_providers())"

# é‡æ–°å®‰è£…CUDAåº“
pip uninstall nvidia-* -y
pip install nvidia-pyindex
pip install --extra-index-url https://pypi.nvidia.com nvidia-cublas-cu12
```

#### 2. å†…å­˜é—®é¢˜

```bash
# ç›‘æ§GPUå†…å­˜
nvidia-smi -l 1

# æ¸…ç†GPUç¼“å­˜
python -c "import torch; torch.cuda.empty_cache()"
```

#### 3. æ¨¡å‹åŠ è½½é—®é¢˜

```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la /runpod-volume/models/insightface/models/
ls -la /runpod-volume/models/gfpgan/

# é‡æ–°ä¸‹è½½æ¨¡å‹
rm -rf /runpod-volume/models/*
# é‡æ–°æ‰§è¡Œæ¨¡å‹ä¸‹è½½æ­¥éª¤
```

### ğŸ§ª éªŒè¯éƒ¨ç½²

#### 1. è¿è¡Œæµ‹è¯•è„šæœ¬

```bash
python test_face_swap_optimization.py
```

#### 2. APIå¥åº·æ£€æŸ¥

```bash
curl -X GET http://localhost:8000/health
```

#### 3. åŠŸèƒ½æµ‹è¯•

```bash
# æµ‹è¯•æ¢è„¸API
curl -X POST http://localhost:8000/face-swap \
  -H "Content-Type: application/json" \
  -d '{
    "source_image": "base64_encoded_image",
    "target_image": "base64_encoded_image"
  }'
```

### ğŸ“ˆ æ€§èƒ½åŸºå‡†

#### é¢„æœŸæ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å¤‡æ³¨ |
|------|--------|------|
| æ¢è„¸å¤„ç†æ—¶é—´ | < 3ç§’ | 1024x1024å›¾åƒ |
| GPUåˆ©ç”¨ç‡ | > 80% | å¤„ç†æœŸé—´ |
| å†…å­˜ä½¿ç”¨ | < 8GB | å³°å€¼ä½¿ç”¨é‡ |
| æ£€æµ‹ç½®ä¿¡åº¦ | > 0.85 | å¹³å‡å€¼ |
| æˆåŠŸç‡ | > 95% | æœ‰æ•ˆäººè„¸å›¾åƒ |

#### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ‰¹å¤„ç†**: å¯¹äºå¤šå›¾åƒå¤„ç†ï¼Œä½¿ç”¨æ‰¹å¤„ç†æå‡æ•ˆç‡
2. **æ¨¡å‹ç¼“å­˜**: ä¿æŒæ¨¡å‹åœ¨å†…å­˜ä¸­ï¼Œé¿å…é‡å¤åŠ è½½
3. **å¼‚æ­¥å¤„ç†**: ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—å¤„ç†å¤§é‡è¯·æ±‚
4. **è´Ÿè½½å‡è¡¡**: å¤šGPUç¯å¢ƒä¸‹åˆ†é…è´Ÿè½½

### ğŸ”’ å®‰å…¨è€ƒè™‘

#### 1. è¾“å…¥éªŒè¯

```python
def validate_image_input(image_data):
    """éªŒè¯è¾“å…¥å›¾åƒ"""
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if len(image_data) > 10 * 1024 * 1024:  # 10MB
        raise ValueError("Image too large")
    
    # æ£€æŸ¥å›¾åƒæ ¼å¼
    try:
        image = Image.open(io.BytesIO(image_data))
        if image.format not in ['JPEG', 'PNG', 'WEBP']:
            raise ValueError("Unsupported image format")
    except Exception:
        raise ValueError("Invalid image data")
```

#### 2. èµ„æºé™åˆ¶

```python
# è®¾ç½®å¤„ç†è¶…æ—¶
import signal

def timeout_handler(signum, frame):
    raise TimeoutError("Processing timeout")

signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(30)  # 30ç§’è¶…æ—¶
```

### ğŸ“ ç»´æŠ¤æŒ‡å—

#### 1. å®šæœŸç»´æŠ¤ä»»åŠ¡

- **æ¯æ—¥**: æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ï¼Œç›‘æ§ç³»ç»Ÿèµ„æº
- **æ¯å‘¨**: æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œæ›´æ–°ä¾èµ–åŒ…
- **æ¯æœˆ**: å¤‡ä»½æ¨¡å‹æ–‡ä»¶ï¼Œæ€§èƒ½è¯„ä¼°

#### 2. æ›´æ–°æµç¨‹

```bash
# å¤‡ä»½å½“å‰ç‰ˆæœ¬
cp -r /app /app_backup_$(date +%Y%m%d)

# æ›´æ–°ä»£ç 
git pull origin main

# é‡å¯æœåŠ¡
docker-compose restart face-swap-api
```

### ğŸ¯ æ€»ç»“

é€šè¿‡æœ¬éƒ¨ç½²æŒ‡å—ï¼Œæ‚¨å¯ä»¥ï¼š

1. âœ… æ­£ç¡®é…ç½®CUDAç¯å¢ƒï¼Œå¯ç”¨GPUåŠ é€Ÿ
2. âœ… éƒ¨ç½²æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ï¼Œæå‡æ¢è„¸è´¨é‡
3. âœ… ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼Œç¡®ä¿ç¨³å®šè¿è¡Œ
4. âœ… å¤„ç†å¸¸è§é—®é¢˜ï¼Œå¿«é€Ÿæ•…éšœæ’é™¤

éƒ¨ç½²å®Œæˆåï¼Œæ¢è„¸ç³»ç»Ÿå°†å…·å¤‡ï¼š
- **é«˜æ€§èƒ½**: GPUåŠ é€Ÿï¼Œå¤„ç†é€Ÿåº¦æå‡10-100å€
- **é«˜è´¨é‡**: åŠ¨æ€æ··åˆã€åŒºåŸŸæ„ŸçŸ¥ã€å…‰ç…§åŒ¹é…
- **é«˜ç¨³å®šæ€§**: é”™è¯¯å¤„ç†ã€èµ„æºç›‘æ§ã€è‡ªåŠ¨æ¢å¤
- **é«˜å¯æ‰©å±•æ€§**: å®¹å™¨åŒ–éƒ¨ç½²ï¼Œæ˜“äºæ‰©å±•

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤ç« èŠ‚æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚ 