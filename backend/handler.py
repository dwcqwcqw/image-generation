import runpod
import torch
from diffusers import FluxPipeline, FluxImg2ImgPipeline
from PIL import Image
import base64
import io
import json
import os
import uuid
from datetime import datetime
import boto3
from botocore.config import Config
import sys
import traceback

# å¯¼å…¥compelç”¨äºå¤„ç†é•¿æç¤ºè¯
try:
    from compel import Compel
    COMPEL_AVAILABLE = True
    print("âœ“ Compel library loaded for long prompt support")
except ImportError:
    COMPEL_AVAILABLE = False
    print("âš ï¸  Compel library not available - long prompt support limited")

# å…¼å®¹æ€§ä¿®å¤ï¼šä¸ºæ—§ç‰ˆæœ¬PyTorchæ·»åŠ get_default_deviceå‡½æ•°
if not hasattr(torch, 'get_default_device'):
    def get_default_device():
        """Fallback implementation for torch.get_default_device()"""
        if torch.cuda.is_available():
            return torch.device('cuda')
        else:
            return torch.device('cpu')
    
    # å°†å‡½æ•°æ·»åŠ åˆ°torchæ¨¡å—ä¸­
    torch.get_default_device = get_default_device
    print("âœ“ Added fallback torch.get_default_device() function")

# æ·»åŠ å¯åŠ¨æ—¥å¿—
print("=== Starting AI Image Generation Backend ===")
print(f"Python version: {sys.version}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

# éªŒè¯å…³é”®ç¯å¢ƒå˜é‡
required_env_vars = [
    "CLOUDFLARE_R2_ACCESS_KEY",
    "CLOUDFLARE_R2_SECRET_KEY", 
    "CLOUDFLARE_R2_BUCKET",
    "CLOUDFLARE_R2_ENDPOINT"
]

missing_vars = []
for var in required_env_vars:
    if not os.getenv(var):
        missing_vars.append(var)

if missing_vars:
    print(f"WARNING: Missing environment variables: {missing_vars}")
    print("Container will start but R2 upload may fail")

# ç¯å¢ƒå˜é‡
CLOUDFLARE_R2_ACCESS_KEY = os.getenv("CLOUDFLARE_R2_ACCESS_KEY")
CLOUDFLARE_R2_SECRET_KEY = os.getenv("CLOUDFLARE_R2_SECRET_KEY") 
CLOUDFLARE_R2_BUCKET = os.getenv("CLOUDFLARE_R2_BUCKET")
CLOUDFLARE_R2_ENDPOINT = os.getenv("CLOUDFLARE_R2_ENDPOINT")
CLOUDFLARE_R2_PUBLIC_DOMAIN = os.getenv("CLOUDFLARE_R2_PUBLIC_DOMAIN")  # å¯é€‰ï¼šè‡ªå®šä¹‰å…¬å…±åŸŸå

# æ¨¡å‹è·¯å¾„
FLUX_BASE_PATH = "/runpod-volume/flux_base"
FLUX_LORA_BASE_PATH = "/runpod-volume"

# æ”¯æŒçš„LoRAæ¨¡å‹åˆ—è¡¨
AVAILABLE_LORAS = {
    "flux-nsfw": {
        "name": "FLUX NSFW",
        "path": "/runpod-volume/flux_nsfw",
        "description": "NSFW content generation model with enhanced capabilities"
    }
}

# é»˜è®¤LoRA
DEFAULT_LORA = "flux-nsfw"

# åˆå§‹åŒ– Cloudflare R2 å®¢æˆ·ç«¯
r2_client = None
if all([CLOUDFLARE_R2_ACCESS_KEY, CLOUDFLARE_R2_SECRET_KEY, CLOUDFLARE_R2_BUCKET, CLOUDFLARE_R2_ENDPOINT]):
    try:
        r2_client = boto3.client(
            's3',
            endpoint_url=CLOUDFLARE_R2_ENDPOINT,
            aws_access_key_id=CLOUDFLARE_R2_ACCESS_KEY,
            aws_secret_access_key=CLOUDFLARE_R2_SECRET_KEY,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        print("âœ“ R2 client initialized successfully")
    except Exception as e:
        print(f"âœ— Failed to initialize R2 client: {e}")
        r2_client = None
else:
    print("âœ— R2 configuration incomplete - R2 upload will be disabled")

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
txt2img_pipe = None
img2img_pipe = None
current_lora = DEFAULT_LORA

# å…¨å±€å˜é‡å­˜å‚¨compelå¤„ç†å™¨
compel_proc = None
compel_proc_neg = None

def get_device():
    """è·å–è®¾å¤‡ï¼Œå…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬"""
    if torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def load_models():
    """åŠ è½½ FLUX æ¨¡å‹ - å¤§å¹…æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
    global txt2img_pipe, img2img_pipe
    
    print("ğŸš€ Loading FLUX models with optimizations...")
    start_time = datetime.now()
    
    # CUDAå…¼å®¹æ€§æ£€æŸ¥å’Œä¿®å¤
    if torch.cuda.is_available():
        try:
            # æµ‹è¯•CUDAæ˜¯å¦å¯ç”¨
            test_tensor = torch.tensor([1.0]).cuda()
            print("âœ“ CUDA test successful")
            del test_tensor
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"âš ï¸  CUDA error detected: {e}")
            print("âš ï¸  Falling back to CPU mode")
            # å¼ºåˆ¶ä½¿ç”¨CPU
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    device = get_device()
    print(f"ğŸ“± Using device: {device}")
    
    # GPUå†…å­˜ä¼˜åŒ–
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"ğŸ’¾ GPU Memory before loading: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
    
    try:
        # ğŸ¯ ä¼˜åŒ–1: ä½¿ç”¨ä½å†…å­˜æ¨¡å¼å’Œä¼˜åŒ–é…ç½®
        print("âš¡ Loading text-to-image pipeline with optimizations...")
        
        # å†…å­˜ä¼˜åŒ–é…ç½®
        model_kwargs = {
            "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            "use_safetensors": True,
            "low_cpu_mem_usage": True,  # ä½CPUå†…å­˜ä½¿ç”¨
        }
        
        # å°è¯•ä½¿ç”¨è®¾å¤‡æ˜ å°„ä¼˜åŒ– - ä¿®å¤å…¼å®¹æ€§é—®é¢˜
        device_mapping_used = False  # æ ‡å¿—è·Ÿè¸ªæ˜¯å¦ä½¿ç”¨è®¾å¤‡æ˜ å°„
        if device == "cuda":
            try:
                # å…ˆå°è¯• "balanced" ç­–ç•¥
                model_kwargs_with_device_map = model_kwargs.copy()
                model_kwargs_with_device_map["device_map"] = "balanced"
                
                txt2img_pipe = FluxPipeline.from_pretrained(
                    FLUX_BASE_PATH,
                    **model_kwargs_with_device_map
                )
                print("âœ… Device mapping enabled with 'balanced' strategy")
                device_mapping_used = True
                
            except Exception as device_map_error:
                print(f"âš ï¸  Device mapping failed ({device_map_error}), loading without device mapping")
                # å›é€€åˆ°ä¸ä½¿ç”¨è®¾å¤‡æ˜ å°„
                txt2img_pipe = FluxPipeline.from_pretrained(
                    FLUX_BASE_PATH,
                    **model_kwargs
                )
                device_mapping_used = False
        else:
            # CPUæ¨¡å¼ç›´æ¥åŠ è½½
            txt2img_pipe = FluxPipeline.from_pretrained(
                FLUX_BASE_PATH,
                **model_kwargs
            )
            device_mapping_used = False
        
        loading_time = (datetime.now() - start_time).total_seconds()
        print(f"â±ï¸  Base model loaded in {loading_time:.2f}s")
        
        # ğŸ¯ ä¼˜åŒ–2: å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
        try:
            txt2img_pipe.enable_attention_slicing()
            print("âœ… Attention slicing enabled")
        except Exception as e:
            print(f"âš ï¸  Attention slicing not available: {e}")
            
        try:
            txt2img_pipe.enable_model_cpu_offload()
            print("âœ… CPU offload enabled")
        except Exception as e:
            print(f"âš ï¸  CPU offload not available: {e}")
        
        # ğŸ¯ ä¼˜åŒ–3: VAEå†…å­˜ä¼˜åŒ–
        try:
            txt2img_pipe.enable_vae_slicing()
            txt2img_pipe.enable_vae_tiling()
            print("âœ… VAE optimizations enabled")
        except Exception as e:
            print(f"âš ï¸  VAE optimizations not available: {e}")
        
        # åŠ è½½é»˜è®¤ LoRA æƒé‡ (å¿…é€‰)
        lora_start_time = datetime.now()
        default_lora_path = AVAILABLE_LORAS[DEFAULT_LORA]["path"]
        if os.path.exists(default_lora_path):
            print(f"ğŸ¨ Loading default LoRA: {AVAILABLE_LORAS[DEFAULT_LORA]['name']}")
            try:
                txt2img_pipe.load_lora_weights(default_lora_path)
                lora_time = (datetime.now() - lora_start_time).total_seconds()
                print(f"âœ… LoRA loaded in {lora_time:.2f}s: {AVAILABLE_LORAS[DEFAULT_LORA]['name']}")
            except ValueError as e:
                if "PEFT backend is required" in str(e):
                    print("âŒ ERROR: PEFT backend is required for LoRA support")
                    print("   Please install: pip install peft>=0.8.0")
                    raise RuntimeError("PEFT library is required but not installed")
                else:
                    print(f"âŒ ERROR: Failed to load LoRA weights: {e}")
                    raise RuntimeError(f"Failed to load required LoRA model: {e}")
            except Exception as e:
                print(f"âŒ ERROR: Failed to load LoRA weights: {e}")
                raise RuntimeError(f"Failed to load required LoRA model: {e}")
        else:
            print(f"âŒ ERROR: Default LoRA weights not found at {default_lora_path}")
            raise RuntimeError(f"Required LoRA model not found: {AVAILABLE_LORAS[DEFAULT_LORA]['name']}")
        
        # éªŒè¯å…¶ä»–å¯ç”¨çš„LoRAæ¨¡å‹
        available_loras = []
        for lora_id, lora_info in AVAILABLE_LORAS.items():
            if os.path.exists(lora_info["path"]):
                available_loras.append(lora_id)
                print(f"âœ… Available LoRA: {lora_info['name']}")
            else:
                print(f"âŒ Missing LoRA: {lora_info['name']} at {lora_info['path']}")
        
        if len(available_loras) == 0:
            raise RuntimeError("No LoRA models found. LoRA models are required for this service.")
        
        print(f"ğŸ“Š Total available LoRA models: {len(available_loras)}")
        
        # ğŸ¯ ä¼˜åŒ–4: æ™ºèƒ½è®¾å¤‡ç§»åŠ¨ï¼ˆä»…åœ¨æœªä½¿ç”¨è®¾å¤‡æ˜ å°„æ—¶ï¼‰
        if not device_mapping_used:
            device_start_time = datetime.now()
            print("ğŸšš Moving pipeline to device...")
            
            if device == "cuda":
                # æ¸è¿›å¼ç§»åŠ¨åˆ°GPUï¼Œé¿å…å†…å­˜å³°å€¼
                txt2img_pipe = txt2img_pipe.to(device)
            else:
                txt2img_pipe = txt2img_pipe.to(device)
            
            device_time = (datetime.now() - device_start_time).total_seconds()
            print(f"âœ… Device transfer completed in {device_time:.2f}s")
        else:
            print("âš¡ Skipping manual device transfer (using device mapping)")
        
        # ğŸ¯ ä¼˜åŒ–5: å›¾ç”Ÿå›¾æ¨¡å‹ä½¿ç”¨å…±äº«ç»„ä»¶ (é›¶æ‹·è´)
        print("ğŸ”— Creating image-to-image pipeline (sharing components)...")
        img_start_time = datetime.now()
        
        img2img_pipe = FluxImg2ImgPipeline(
            vae=txt2img_pipe.vae,
            text_encoder=txt2img_pipe.text_encoder,
            text_encoder_2=txt2img_pipe.text_encoder_2,
            tokenizer=txt2img_pipe.tokenizer,
            tokenizer_2=txt2img_pipe.tokenizer_2,
            transformer=txt2img_pipe.transformer,
            scheduler=txt2img_pipe.scheduler,
        )
        
        # ä¸éœ€è¦å†æ¬¡ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œå› ä¸ºå…±äº«ç»„ä»¶å·²ç»åœ¨è®¾å¤‡ä¸Š
        img_time = (datetime.now() - img_start_time).total_seconds()
        print(f"âœ… Image-to-image pipeline created in {img_time:.2f}s")
        
        # æœ€ç»ˆå†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            print(f"ğŸ’¾ GPU Memory after loading: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            print(f"ğŸ’¾ GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f}GB")
        
        total_time = (datetime.now() - start_time).total_seconds()
        print(f"ğŸ‰ All models loaded successfully in {total_time:.2f}s!")
        
        # ğŸ¯ ä¼˜åŒ–6: é¢„çƒ­æ¨ç† (å¯é€‰)
        try:
            print("ğŸ”¥ Warming up models with test inference...")
            warmup_start = datetime.now()
            with torch.no_grad():
                # å°å°ºå¯¸é¢„çƒ­æ¨ç†
                test_result = txt2img_pipe(
                    prompt="test",
                    width=512,
                    height=512,
                    num_inference_steps=1,
                    guidance_scale=1.0
                )
            warmup_time = (datetime.now() - warmup_start).total_seconds()
            print(f"âœ… Model warmup completed in {warmup_time:.2f}s")
        except Exception as e:
            print(f"âš ï¸  Model warmup failed (ä¸å½±å“æ­£å¸¸ä½¿ç”¨): {e}")
        
        print("ğŸš€ System ready for image generation!")
        
        # ğŸ¯ ä¼˜åŒ–7: åˆå§‹åŒ–Compelç”¨äºé•¿æç¤ºè¯æ”¯æŒ
        global compel_proc
        compel_proc = None
        
        if COMPEL_AVAILABLE:
            try:
                print("ğŸ”¤ Initializing Compel for long prompt support...")
                compel_proc = Compel(
                    tokenizer=[txt2img_pipe.tokenizer, txt2img_pipe.tokenizer_2],
                    text_encoder=[txt2img_pipe.text_encoder, txt2img_pipe.text_encoder_2],
                    device=txt2img_pipe.device,
                )
                print("âœ… Compel initialized - now supports prompts up to 512 tokens!")
            except Exception as e:
                print(f"âš ï¸  Compel initialization failed: {e}")
                compel_proc = None
        else:
            print("âš ï¸  Compel not available - prompt limited to 77 tokens")
        
    except Exception as e:
        print(f"âŒ Error loading models: {str(e)}")
        traceback.print_exc()
        raise e

def upload_to_r2(image_data: bytes, filename: str) -> str:
    """ä¸Šä¼ å›¾ç‰‡åˆ° Cloudflare R2"""
    try:
        # æ£€æŸ¥R2å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if r2_client is None:
            raise RuntimeError("R2 client not available - check environment variables")
            
        # ç¡®ä¿ image_data æ˜¯ bytes ç±»å‹
        if not isinstance(image_data, bytes):
            raise TypeError(f"Expected bytes, got {type(image_data)}")
            
        # éªŒè¯æ–‡ä»¶å¤§å°
        if len(image_data) == 0:
            raise ValueError("Image data is empty")
            
        if len(image_data) > 50 * 1024 * 1024:  # 50MB limit
            raise ValueError(f"Image too large: {len(image_data)} bytes")
            
        print(f"Uploading {len(image_data)} bytes to R2 as {filename}")
        
        r2_client.put_object(
            Bucket=CLOUDFLARE_R2_BUCKET,
            Key=filename,
            Body=image_data,
            ContentType='image/png',
            ACL='public-read'
        )
        
        # æ„å»ºæ­£ç¡®çš„å…¬å…± URL æ ¼å¼ - Cloudflare R2 public URL format
        # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰å…¬å…±åŸŸåï¼ˆæ¨èï¼Œå¯é¿å…CORSé—®é¢˜ï¼‰
        if CLOUDFLARE_R2_PUBLIC_DOMAIN:
            public_url = f"{CLOUDFLARE_R2_PUBLIC_DOMAIN.rstrip('/')}/{filename}"
            print(f"âœ“ Successfully uploaded to (custom domain): {public_url}")
        else:
            # å›é€€åˆ°æ ‡å‡†R2æ ¼å¼
            # æ­£ç¡®æ ¼å¼: https://{bucket}.{account_id}.r2.cloudflarestorage.com/{filename}
            # ä»endpoint URLä¸­æå–account ID
            account_id = CLOUDFLARE_R2_ENDPOINT.split('//')[1].split('.')[0]
            public_url = f"https://{CLOUDFLARE_R2_BUCKET}.{account_id}.r2.cloudflarestorage.com/{filename}"
            print(f"âœ“ Successfully uploaded to (standard R2): {public_url}")
        
        return public_url
        
    except Exception as e:
        print(f"âœ— Error uploading to R2: {str(e)}")
        print(f"Image data type: {type(image_data)}, size: {len(image_data) if hasattr(image_data, '__len__') else 'unknown'}")
        
        # å¯¹äºæ¼”ç¤ºç›®çš„ï¼Œè¿”å›ä¸€ä¸ªå ä½ç¬¦URLè€Œä¸æ˜¯å¤±è´¥
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æŠ›å‡ºå¼‚å¸¸
        placeholder_url = f"https://via.placeholder.com/512x512/cccccc/666666?text=Upload+Failed"
        print(f"Returning placeholder URL: {placeholder_url}")
        return placeholder_url

def image_to_bytes(image: Image.Image) -> bytes:
    """å°† PIL Image è½¬æ¢ä¸ºå­—èŠ‚"""
    try:
        buffer = io.BytesIO()
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image.save(buffer, format='PNG', quality=95, optimize=True)
        buffer.seek(0)  # é‡ç½®bufferä½ç½®
        return buffer.getvalue()
    except Exception as e:
        print(f"Error converting image to bytes: {str(e)}")
        raise e

def base64_to_image(base64_str: str) -> Image.Image:
    """å°† base64 å­—ç¬¦ä¸²è½¬æ¢ä¸º PIL Image"""
    if base64_str.startswith('data:image'):
        base64_str = base64_str.split(',')[1]
    
    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image.convert('RGB')

def text_to_image(params: dict) -> list:
    """æ–‡ç”Ÿå›¾ç”Ÿæˆ - ä¼˜åŒ–ç‰ˆæœ¬ with long prompt support"""
    global txt2img_pipe, compel_proc
    
    if txt2img_pipe is None:
        raise ValueError("Text-to-image model not loaded")
    
    # æå–å‚æ•°
    prompt = params.get('prompt', '')
    negative_prompt = params.get('negativePrompt', '')
    width = params.get('width', 512)
    height = params.get('height', 512)
    steps = params.get('steps', 20)
    cfg_scale = params.get('cfgScale', 7.0)
    seed = params.get('seed', -1)
    num_images = params.get('numImages', 1)
    
    # ğŸ¯ é•¿æç¤ºè¯æ”¯æŒ - è§£å†³77 tokené™åˆ¶
    print(f"ğŸ“ Processing prompt: {len(prompt)} characters")
    
    # å¤„ç†é•¿æç¤ºè¯
    processed_prompt = prompt
    processed_negative_prompt = negative_prompt
    
    if compel_proc and len(prompt) > 300:  # ä¼°ç®—è¶…è¿‡77 tokensçš„æƒ…å†µ
        try:
            print("ğŸ” Long prompt detected, using Compel for extended token support...")
            # ä½¿ç”¨compelå¤„ç†é•¿æç¤ºè¯
            prompt_embeds = compel_proc(prompt)
            
            # å¤„ç†è´Ÿé¢æç¤ºè¯
            if negative_prompt:
                negative_prompt_embeds = compel_proc(negative_prompt)
            else:
                negative_prompt_embeds = compel_proc("")
                
            print(f"âœ… Compel processed prompt successfully")
            
            # ä½¿ç”¨embeddingè€Œä¸æ˜¯æ–‡æœ¬æç¤ºè¯
            generation_kwargs = {
                "prompt_embeds": prompt_embeds,
                "negative_prompt_embeds": negative_prompt_embeds,
                "width": width,
                "height": height,
                "num_inference_steps": steps,
                "guidance_scale": cfg_scale,
                "generator": None,  # ç¨åè®¾ç½®
            }
        except Exception as e:
            print(f"âš ï¸  Compel processing failed, using standard prompt: {e}")
            # å›é€€åˆ°æ ‡å‡†æç¤ºè¯å¤„ç†
            generation_kwargs = {
                "prompt": prompt,
                "negative_prompt": negative_prompt,
                "width": width,
                "height": height,
                "num_inference_steps": steps,
                "guidance_scale": cfg_scale,
                "generator": None,  # ç¨åè®¾ç½®
            }
    else:
        # ä½¿ç”¨æ ‡å‡†æç¤ºè¯å¤„ç†
        generation_kwargs = {
            "prompt": prompt,
            "negative_prompt": negative_prompt,
            "width": width,
            "height": height,
            "num_inference_steps": steps,
            "guidance_scale": cfg_scale,
            "generator": None,  # ç¨åè®¾ç½®
        }
    
    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    results = []
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡ï¼Œè€Œä¸æ˜¯å¾ªç¯
    if num_images > 1 and num_images <= 4:  # é™åˆ¶æ‰¹é‡å¤§å°é¿å…å†…å­˜é—®é¢˜
        try:
            print(f"Batch generating {num_images} images...")
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                batch_kwargs = generation_kwargs.copy()
                batch_kwargs["num_images_per_prompt"] = num_images
                result = txt2img_pipe(**batch_kwargs)
            
            # å¤„ç†æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡
            for i, image in enumerate(result.images):
                try:
                    # ä¸Šä¼ åˆ° R2
                    image_id = str(uuid.uuid4())
                    filename = f"generated/{image_id}.png"
                    image_bytes = image_to_bytes(image)
                    image_url = upload_to_r2(image_bytes, filename)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    image_data = {
                        'id': image_id,
                        'url': image_url,
                        'prompt': prompt,
                        'negativePrompt': negative_prompt,
                        'seed': seed + i,  # æ¯å¼ å›¾ç‰‡ä¸åŒçš„ç§å­
                        'width': width,
                        'height': height,
                        'steps': steps,
                        'cfgScale': cfg_scale,
                        'createdAt': datetime.utcnow().isoformat(),
                        'type': 'text-to-image'
                    }
                    
                    results.append(image_data)
                    
                except Exception as e:
                    print(f"Error processing batch image {i+1}: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"Batch generation failed, falling back to individual generation: {str(e)}")
            # å¦‚æœæ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ ç”Ÿæˆ
            num_images = min(num_images, 1)
    
    # å•å¼ ç”Ÿæˆæˆ–æ‰¹é‡ç”Ÿæˆå¤±è´¥çš„å›é€€
    if len(results) == 0:
        for i in range(num_images):
            try:
                print(f"Generating image {i+1}/{num_images}...")
                # ç”Ÿæˆå›¾åƒ
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    # ä¼˜åŒ–ï¼šæ¸…ç†GPUç¼“å­˜
                    if torch.cuda.is_available() and i > 0:
                        torch.cuda.empty_cache()
                        
                    single_kwargs = generation_kwargs.copy()
                    single_kwargs["num_images_per_prompt"] = 1
                    result = txt2img_pipe(**single_kwargs)
                
                image = result.images[0]
                
                # ä¸Šä¼ åˆ° R2
                image_id = str(uuid.uuid4())
                filename = f"generated/{image_id}.png"
                image_bytes = image_to_bytes(image)
                image_url = upload_to_r2(image_bytes, filename)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                image_data = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed,
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'createdAt': datetime.utcnow().isoformat(),
                    'type': 'text-to-image'
                }
                
                results.append(image_data)
                
                # ä¸ºä¸‹ä¸€å¼ å›¾ç‰‡æ›´æ–°ç§å­
                if i < num_images - 1:
                    seed += 1
                    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
                    generation_kwargs["generator"] = generator
                    
            except Exception as e:
                print(f"Error generating image {i+1}: {str(e)}")
                continue
    
    # ä¼˜åŒ–ï¼šæœ€ç»ˆæ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results

def image_to_image(params: dict) -> list:
    """å›¾ç”Ÿå›¾ç”Ÿæˆ - ä¼˜åŒ–ç‰ˆæœ¬"""
    global img2img_pipe
    
    if img2img_pipe is None:
        raise ValueError("Image-to-image model not loaded")
    
    # æå–å‚æ•°
    prompt = params.get('prompt', '')
    negative_prompt = params.get('negativePrompt', '')
    image_data = params.get('image', '')
    width = params.get('width', 512)
    height = params.get('height', 512)
    steps = params.get('steps', 20)
    cfg_scale = params.get('cfgScale', 7.0)
    seed = params.get('seed', -1)
    num_images = params.get('numImages', 1)
    denoising_strength = params.get('denoisingStrength', 0.7)
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    if isinstance(image_data, str):
        source_image = base64_to_image(image_data)
    else:
        raise ValueError("Invalid image data format")
    
    # è°ƒæ•´å›¾åƒå°ºå¯¸
    source_image = source_image.resize((width, height), Image.Resampling.LANCZOS)
    
    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
    
    results = []
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡
    if num_images > 1 and num_images <= 4:  # é™åˆ¶æ‰¹é‡å¤§å°é¿å…å†…å­˜é—®é¢˜
        try:
            print(f"Batch generating {num_images} images for img2img...")
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                result = img2img_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=source_image,
                    strength=denoising_strength,
                    num_inference_steps=steps,
                    guidance_scale=cfg_scale,
                    generator=generator,
                    num_images_per_prompt=num_images
                )
            
            # å¤„ç†æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡
            for i, image in enumerate(result.images):
                try:
                    # ä¸Šä¼ åˆ° R2
                    image_id = str(uuid.uuid4())
                    filename = f"generated/{image_id}.png"
                    image_bytes = image_to_bytes(image)
                    image_url = upload_to_r2(image_bytes, filename)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    image_data = {
                        'id': image_id,
                        'url': image_url,
                        'prompt': prompt,
                        'negativePrompt': negative_prompt,
                        'seed': seed + i,
                        'width': width,
                        'height': height,
                        'steps': steps,
                        'cfgScale': cfg_scale,
                        'createdAt': datetime.utcnow().isoformat(),
                        'type': 'image-to-image',
                        'denoisingStrength': denoising_strength
                    }
                    
                    results.append(image_data)
                    
                except Exception as e:
                    print(f"Error processing batch img2img image {i+1}: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"Batch img2img generation failed, falling back to individual generation: {str(e)}")
            # å¦‚æœæ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ ç”Ÿæˆ
            num_images = min(num_images, 1)
    
    # å•å¼ ç”Ÿæˆæˆ–æ‰¹é‡ç”Ÿæˆå¤±è´¥çš„å›é€€
    if len(results) == 0:
        for i in range(num_images):
            try:
                print(f"Generating img2img image {i+1}/{num_images}...")
                # ç”Ÿæˆå›¾åƒ
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    # ä¼˜åŒ–ï¼šæ¸…ç†GPUç¼“å­˜
                    if torch.cuda.is_available() and i > 0:
                        torch.cuda.empty_cache()
                        
                    result = img2img_pipe(
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        image=source_image,
                        strength=denoising_strength,
                        num_inference_steps=steps,
                        guidance_scale=cfg_scale,
                        generator=generator,
                        num_images_per_prompt=1
                    )
                
                image = result.images[0]
                
                # ä¸Šä¼ åˆ° R2
                image_id = str(uuid.uuid4())
                filename = f"generated/{image_id}.png"
                image_bytes = image_to_bytes(image)
                image_url = upload_to_r2(image_bytes, filename)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                image_data = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed,
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'createdAt': datetime.utcnow().isoformat(),
                    'type': 'image-to-image',
                    'denoisingStrength': denoising_strength
                }
                
                results.append(image_data)
                
                # ä¸ºä¸‹ä¸€å¼ å›¾ç‰‡æ›´æ–°ç§å­
                if i < num_images - 1:
                    seed += 1
                    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
                    
            except Exception as e:
                print(f"Error generating img2img image {i+1}: {str(e)}")
                continue
    
    # ä¼˜åŒ–ï¼šæœ€ç»ˆæ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results

def get_available_loras() -> dict:
    """è·å–å¯ç”¨çš„LoRAæ¨¡å‹åˆ—è¡¨"""
    available = {}
    for lora_id, lora_info in AVAILABLE_LORAS.items():
        if os.path.exists(lora_info["path"]):
            available[lora_id] = {
                "name": lora_info["name"],
                "description": lora_info["description"],
                "is_current": lora_id == current_lora
            }
    return available

def switch_lora(lora_id: str) -> bool:
    """åˆ‡æ¢LoRAæ¨¡å‹ - ä¼˜åŒ–ç‰ˆæœ¬"""
    global txt2img_pipe, img2img_pipe, current_lora
    
    if lora_id not in AVAILABLE_LORAS:
        raise ValueError(f"Unknown LoRA model: {lora_id}")
    
    lora_info = AVAILABLE_LORAS[lora_id]
    lora_path = lora_info["path"]
    
    if not os.path.exists(lora_path):
        raise ValueError(f"LoRA model not found: {lora_info['name']} at {lora_path}")
    
    # ä¼˜åŒ–ï¼šå¦‚æœå·²ç»æ˜¯å½“å‰LoRAï¼Œç›´æ¥è¿”å›ï¼Œé¿å…ä¸å¿…è¦çš„é‡æ–°åŠ è½½
    if lora_id == current_lora:
        print(f"LoRA {lora_info['name']} is already loaded - skipping switch")
        return True
    
    try:
        print(f"Switching LoRA from {AVAILABLE_LORAS[current_lora]['name']} to {lora_info['name']}")
        
        # å¸è½½å½“å‰LoRA
        txt2img_pipe.unload_lora_weights()
        
        # åŠ è½½æ–°çš„LoRA
        txt2img_pipe.load_lora_weights(lora_path)
        
        # æ›´æ–°å½“å‰LoRA
        current_lora = lora_id
        
        print(f"Successfully switched to LoRA: {lora_info['name']}")
        return True
        
    except Exception as e:
        print(f"Failed to switch LoRA: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„LoRA
        try:
            previous_lora_path = AVAILABLE_LORAS[current_lora]["path"]
            txt2img_pipe.unload_lora_weights()
            txt2img_pipe.load_lora_weights(previous_lora_path)
            print(f"Recovered to previous LoRA: {AVAILABLE_LORAS[current_lora]['name']}")
        except Exception as recovery_error:
            print(f"Failed to recover LoRA: {recovery_error}")
        raise RuntimeError(f"Failed to switch LoRA model: {str(e)}")

def handler(job):
    """RunPod å¤„ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        job_input = job['input']
        task_type = job_input.get('task_type')
        
        if task_type == 'get-loras':
            # è·å–å¯ç”¨LoRAåˆ—è¡¨
            available_loras = get_available_loras()
            return {
                'success': True,
                'data': {
                    'loras': available_loras,
                    'current': current_lora
                }
            }
            
        elif task_type == 'switch-lora':
            # åˆ‡æ¢LoRAæ¨¡å‹
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            # ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å®é™…åˆ‡æ¢
            if lora_id == current_lora:
                return {
                    'success': True,
                    'data': {
                        'current_lora': current_lora,
                        'message': f'{AVAILABLE_LORAS[current_lora]["name"]} is already active'
                    }
                }
            
            switch_lora(lora_id)
            return {
                'success': True,
                'data': {
                    'current_lora': current_lora,
                    'message': f'Switched to {AVAILABLE_LORAS[current_lora]["name"]}'
                }
            }
        
        elif task_type == 'text-to-image':
            # ä¼˜åŒ–ï¼šåªåœ¨éœ€è¦æ—¶åˆ‡æ¢LoRA
            params = job_input.get('params', {})
            requested_lora = params.get('lora_model', current_lora)
            
            if requested_lora and requested_lora != current_lora:
                print(f"Auto-switching LoRA from {current_lora} to {requested_lora} for generation")
                switch_lora(requested_lora)
            
            results = text_to_image(params)
            return {
                'success': True,
                'data': results
            }
            
        elif task_type == 'image-to-image':
            # ä¼˜åŒ–ï¼šåªåœ¨éœ€è¦æ—¶åˆ‡æ¢LoRA
            params = job_input.get('params', {})
            requested_lora = params.get('lora_model', current_lora)
            
            if requested_lora and requested_lora != current_lora:
                print(f"Auto-switching LoRA from {current_lora} to {requested_lora} for generation")
                switch_lora(requested_lora)
            
            results = image_to_image(params)
            return {
                'success': True,
                'data': results
            }
            
        else:
            return {
                'success': False,
                'error': f'Unknown task type: {task_type}'
            }
            
    except Exception as e:
        print(f"Handler error: {str(e)}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }

if __name__ == "__main__":
    # åœ¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    try:
        load_models()
        print("Starting RunPod serverless...")
        
        # å¯åŠ¨ RunPod serverless
        runpod.serverless.start({
            "handler": handler
        })
    except Exception as e:
        print(f"Failed to start serverless: {str(e)}")
        traceback.print_exc()
        raise e 