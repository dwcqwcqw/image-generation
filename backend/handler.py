import os
import base64
import io
import time
import traceback
import uuid
import sys  # æ·»åŠ ç¼ºå¤±çš„syså¯¼å…¥
import re  # æ·»åŠ regexæ¨¡å—ç”¨äºé•¿promptå¤„ç†
from datetime import datetime
from typing import Optional, Dict, List, Tuple

import torch
import numpy as np
from PIL import Image
import runpod

# AIå’Œå›¾åƒå¤„ç†åº“
from diffusers import (
    FluxPipeline, 
    FluxImg2ImgPipeline,  # <-- Add this import
    StableDiffusionPipeline, 
    StableDiffusionImg2ImgPipeline,
    StableDiffusionXLPipeline,  # <-- Add import
    StableDiffusionXLImg2ImgPipeline,  # <-- Add import
    DPMSolverMultistepScheduler,
    EulerDiscreteScheduler
)

from transformers import T5EncoderModel, CLIPTextModel, CLIPTokenizer
import boto3
from botocore.exceptions import ClientError
from botocore.client import Config # æ·»åŠ Configå¯¼å…¥

# ğŸ”§ å…¼å®¹æ€§ä¿®å¤ï¼šæ·»åŠ å›é€€çš„torch.get_default_deviceå‡½æ•°
if not hasattr(torch, 'get_default_device'):
    def get_default_device():
        if torch.cuda.is_available():
            return torch.device('cuda')
        else:
            return torch.device('cpu')
    torch.get_default_device = get_default_device
    print("âœ“ Added fallback torch.get_default_device() function")

# å¯¼å…¥compelç”¨äºå¤„ç†é•¿æç¤ºè¯
try:
    from compel import Compel
    COMPEL_AVAILABLE = True
    print("âœ“ Compel library loaded for long prompt support")
except ImportError:
    COMPEL_AVAILABLE = False
    print("âš ï¸  Compel library not available - long prompt support limited")

# æ·»åŠ å¯åŠ¨æ—¥å¿—
print("=== Starting AI Image Generation Backend ===")
print(f"Python version: {sys.version}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

# éªŒè¯å…³é”®ç¯å¢ƒå˜é‡
required_env_vars = [
    "CLOUDFLARE_R2_ACCESS_KEY",
    "CLOUDFLARE_R2_SECRET_KEY", 
    "CLOUDFLARE_R2_BUCKET",
    "CLOUDFLARE_R2_ENDPOINT"
]

missing_vars = []
for var in required_env_vars:
    if not os.getenv(var):
        missing_vars.append(var)

if missing_vars:
    print(f"WARNING: Missing environment variables: {missing_vars}")
    print("Container will start but R2 upload may fail")

# ç¯å¢ƒå˜é‡
CLOUDFLARE_R2_ACCESS_KEY = os.getenv("CLOUDFLARE_R2_ACCESS_KEY")
CLOUDFLARE_R2_SECRET_KEY = os.getenv("CLOUDFLARE_R2_SECRET_KEY") 
CLOUDFLARE_R2_BUCKET = os.getenv("CLOUDFLARE_R2_BUCKET")
CLOUDFLARE_R2_ENDPOINT = os.getenv("CLOUDFLARE_R2_ENDPOINT")
CLOUDFLARE_R2_PUBLIC_DOMAIN = os.getenv("CLOUDFLARE_R2_PUBLIC_DOMAIN")  # å¯é€‰ï¼šè‡ªå®šä¹‰å…¬å…±åŸŸå

# æ¨¡å‹è·¯å¾„
FLUX_BASE_PATH = "/runpod-volume/flux_base"
FLUX_LORA_BASE_PATH = "/runpod-volume/lora"

# é…ç½®åŸºç¡€æ¨¡å‹ç±»å‹å’Œè·¯å¾„
BASE_MODELS = {
    "realistic": {
        "name": "çœŸäººé£æ ¼",
        "model_path": "/runpod-volume/flux_base",
        "model_type": "flux", 
        "lora_path": None,  # ğŸš¨ ä¿®å¤ï¼šä¸è‡ªåŠ¨åŠ è½½é»˜è®¤LoRA
        "lora_id": None     # ğŸš¨ ä¿®å¤ï¼šè®©ç”¨æˆ·é€‰æ‹©å†³å®šLoRA
    },
    "anime": {
        "name": "åŠ¨æ¼«é£æ ¼", 
        "model_path": "/runpod-volume/cartoon/Anime_NSFW.safetensors",
        "model_type": "diffusers",
        "lora_path": None,  # ğŸš¨ ä¿®å¤ï¼šä¸è‡ªåŠ¨åŠ è½½é»˜è®¤LoRA
        "lora_id": None     # ğŸš¨ ä¿®å¤ï¼šè®©ç”¨æˆ·é€‰æ‹©å†³å®šLoRA
    }
}

# ä¿®å¤ï¼šç§»é™¤é»˜è®¤LoRAé…ç½®ï¼Œè®©ç”¨æˆ·é€‰æ‹©å†³å®š
DEFAULT_LORA_CONFIG = {}

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
txt2img_pipe = None
img2img_pipe = None
current_lora_config = {}  # ä¿®å¤ï¼šåˆå§‹åŒ–ä¸ºç©º
current_base_model = None
device_mapping_enabled = False
current_selected_lora = None  # ä¿®å¤ï¼šåˆå§‹åŒ–ä¸ºNone

# å…¨å±€å˜é‡å­˜å‚¨compelå¤„ç†å™¨
compel_proc = None
compel_proc_neg = None

# æ”¯æŒçš„LoRAæ¨¡å‹åˆ—è¡¨ - æ›´æ–°ä¸ºæ”¯æŒä¸åŒåŸºç¡€æ¨¡å‹
AVAILABLE_LORAS = None
LORAS_LAST_SCAN = 0
LORAS_CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜

# åˆå§‹åŒ– Cloudflare R2 å®¢æˆ·ç«¯
r2_client = None
if all([CLOUDFLARE_R2_ACCESS_KEY, CLOUDFLARE_R2_SECRET_KEY, CLOUDFLARE_R2_BUCKET, CLOUDFLARE_R2_ENDPOINT]):
    try:
        r2_client = boto3.client(
            's3',
            endpoint_url=CLOUDFLARE_R2_ENDPOINT,
            aws_access_key_id=CLOUDFLARE_R2_ACCESS_KEY,
            aws_secret_access_key=CLOUDFLARE_R2_SECRET_KEY,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        print("âœ“ R2 client initialized successfully")
    except Exception as e:
        print(f"âœ— Failed to initialize R2 client: {e}")
        r2_client = None
else:
    print("âœ— R2 configuration incomplete - R2 upload will be disabled")

def get_device():
    """è·å–è®¾å¤‡ï¼Œå…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬"""
    if torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def load_models():
    """æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œä¸é¢„çƒ­"""
    global txt2img_pipe, img2img_pipe, current_base_model
    
    print("âœ“ æ¨¡å‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå°†æŒ‰éœ€åŠ è½½æ¨¡å‹")
    print(f"ğŸ“ æ”¯æŒçš„æ¨¡å‹ç±»å‹: {list(BASE_MODELS.keys())}")
    
    # ä¸é¢„çƒ­ä»»ä½•æ¨¡å‹ï¼Œç­‰å¾…ç”¨æˆ·è¯·æ±‚æ—¶åŠ è½½
    current_base_model = None
    print("ğŸ¯ ç³»ç»Ÿå°±ç»ªï¼Œç­‰å¾…æ¨¡å‹åŠ è½½è¯·æ±‚...")

def load_flux_model(base_path: str, device: str) -> tuple:
    """åŠ è½½FLUXæ¨¡å‹"""
    global device_mapping_enabled
    
    # å†…å­˜ä¼˜åŒ–é…ç½®
    model_kwargs = {
        "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
        "use_safetensors": True,
        "low_cpu_mem_usage": True,
    }
    
    # ğŸš¨ ç¦ç”¨device mappingä»¥é¿å…æ¨¡å‹åˆ‡æ¢æ—¶çš„device conflicts
    print("âš ï¸  ç¦ç”¨FLUX device mappingä»¥é¿å…æ¨¡å‹åˆ‡æ¢å†²çª")
    device_mapping_enabled = False
    
    # ç›´æ¥åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡ï¼Œä¸ä½¿ç”¨device mapping
    txt2img_pipe = FluxPipeline.from_pretrained(
        base_path,
        **model_kwargs
    ).to(device)
    
    # å¯ç”¨ä¼˜åŒ–
    try:
        txt2img_pipe.enable_attention_slicing()
        print("âœ… Attention slicing enabled")
    except Exception as e:
        print(f"âš ï¸  Attention slicing not available: {e}")
        
    # ğŸš¨ è·³è¿‡CPU offloadä»¥é¿å…device conflicts
    print("âš ï¸  è·³è¿‡FLUX CPU offloadä»¥é¿å…deviceå†²çª")
    
    try:
        txt2img_pipe.enable_vae_slicing()
        txt2img_pipe.enable_vae_tiling()
        print("âœ… VAE optimizations enabled")
    except Exception as e:
        print(f"âš ï¸  VAE optimizations not available: {e}")
    
    # åˆ›å»ºå›¾ç”Ÿå›¾ç®¡é“
    print("ğŸ”— Creating FLUX image-to-image pipeline (sharing components)...")
    img2img_pipe = FluxImg2ImgPipeline(
        vae=txt2img_pipe.vae,
        text_encoder=txt2img_pipe.text_encoder,
        text_encoder_2=txt2img_pipe.text_encoder_2,
        tokenizer=txt2img_pipe.tokenizer,
        tokenizer_2=txt2img_pipe.tokenizer_2,
        transformer=txt2img_pipe.transformer,
        scheduler=txt2img_pipe.scheduler,
    ).to(device)
    
    return txt2img_pipe, img2img_pipe

def load_diffusers_model(base_path: str, device: str) -> tuple:
    """åŠ è½½æ ‡å‡†diffusersæ¨¡å‹ - æ”¯æŒSDXLç›®å½•åŠ è½½"""
    print(f"ğŸ¨ Loading diffusers model from {base_path}")
    
    model_filename = os.path.basename(base_path)
    is_anime_nsfw_model = model_filename == "Anime_NSFW.safetensors"

    if is_anime_nsfw_model:
        print(f"ğŸ’¡ ç‰¹å®šé…ç½®: ä¸º {model_filename} ä½¿ç”¨ StableDiffusionXLPipeline å’Œ float16")
        torch_dtype = torch.float16
        variant = "fp16"
        pipeline_class = StableDiffusionXLPipeline
        img2img_pipeline_class = StableDiffusionXLImg2ImgPipeline
        # æš‚æ—¶ç¦ç”¨offloadä»¥åŒ¹é…notebookè¡Œä¸ºï¼Œåç»­å¯æ ¹æ®å†…å­˜æƒ…å†µè°ƒæ•´
        enable_offload = False 
    else:
        print(f"ğŸ’¡ æ ‡å‡†é…ç½®: ä¸º {model_filename} ä½¿ç”¨ StableDiffusionPipeline å’Œ float32 (å…¼å®¹æ€§ä¼˜å…ˆ)")
        torch_dtype = torch.float32
        variant = None # variantä¸ç”¨äºé€šç”¨SDPipelineæˆ–ç›®å½•åŠ è½½
        pipeline_class = StableDiffusionPipeline
        img2img_pipeline_class = StableDiffusionImg2ImgPipeline
        enable_offload = True # å¯¹å…¶ä»–æ¨¡å‹ä¿æŒoffload

    print(f"ğŸ’¡ ä½¿ç”¨ {torch_dtype} ç²¾åº¦åŠ è½½æ¨¡å‹")
    
    try:
        if os.path.isdir(base_path):
            print(f"ğŸ“ æ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨from_pretrainedåŠ è½½æ¨¡å‹ ({pipeline_class.__name__})")
            if pipeline_class == StableDiffusionXLPipeline:
                txt2img_pipeline = pipeline_class.from_pretrained(
                    base_path,
                    torch_dtype=torch_dtype,
                    variant=variant if variant else None,
                    use_safetensors=True,
                    # safety_checker, requires_safety_checker not valid for SDXL from_pretrained
                ).to(device)
            else:
                txt2img_pipeline = pipeline_class.from_pretrained(
                    base_path,
                    torch_dtype=torch_dtype,
                    variant=variant if variant else None, 
                    use_safetensors=True,
                    safety_checker=None,
                    requires_safety_checker=False
                ).to(device)
        else:
            print(f"ğŸ“„ æ£€æµ‹åˆ°å•æ–‡ä»¶ï¼Œä½¿ç”¨from_single_fileåŠ è½½ ({pipeline_class.__name__})")
            if pipeline_class == StableDiffusionXLPipeline:
                txt2img_pipeline = pipeline_class.from_single_file(
                    base_path,
                    torch_dtype=torch_dtype,
                    variant=variant if variant else None,
                    use_safetensors=True,
                    # safety_checker, requires_safety_checker, load_safety_checker not valid for SDXL from_single_file
                ).to(device)
            else:
                txt2img_pipeline = pipeline_class.from_single_file(
                    base_path,
                    torch_dtype=torch_dtype,
                    variant=variant if variant else None, 
                    use_safetensors=True,
                    safety_checker=None,
                    requires_safety_checker=False,
                    load_safety_checker=False 
                ).to(device)
        
        # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        txt2img_pipeline.enable_attention_slicing()
        if enable_offload:
            print("ğŸ“¦ å¯ç”¨æ¨¡å‹CPU Offload")
            txt2img_pipeline.enable_model_cpu_offload()
        else:
            print("ğŸš« æ¨¡å‹CPU Offloadå·²ç¦ç”¨ (ç‰¹å®šäºAnime_NSFW.safetensorsæµ‹è¯•)")

        if img2img_pipeline_class == StableDiffusionXLImg2ImgPipeline:
            # SDXL img2imgç®¡é“ä¸æ¥å—safety_checkerå‚æ•°
            img2img_pipeline = img2img_pipeline_class(
                vae=txt2img_pipeline.vae,
                text_encoder=txt2img_pipeline.text_encoder,
                text_encoder_2=txt2img_pipeline.text_encoder_2,
                tokenizer=txt2img_pipeline.tokenizer,
                tokenizer_2=txt2img_pipeline.tokenizer_2,
                unet=txt2img_pipeline.unet,
                scheduler=txt2img_pipeline.scheduler,
                feature_extractor=getattr(txt2img_pipeline, 'feature_extractor', None),
            ).to(device)
        else:
            # æ ‡å‡†SD img2imgç®¡é“æ¥å—safety_checkerå‚æ•°
            img2img_pipeline = img2img_pipeline_class(
                vae=txt2img_pipeline.vae,
                text_encoder=getattr(txt2img_pipeline, 'text_encoder', None),
                text_encoder_2=getattr(txt2img_pipeline, 'text_encoder_2', None),
                tokenizer=getattr(txt2img_pipeline, 'tokenizer', None),
                tokenizer_2=getattr(txt2img_pipeline, 'tokenizer_2', None),
                unet=txt2img_pipeline.unet,
                scheduler=txt2img_pipeline.scheduler,
                safety_checker=None,
                feature_extractor=getattr(txt2img_pipeline, 'feature_extractor', None),
                requires_safety_checker=False
            ).to(device)
        
        txt2img_pipeline.safety_checker = None
        txt2img_pipeline.requires_safety_checker = False
        img2img_pipeline.safety_checker = None
        img2img_pipeline.requires_safety_checker = False
        
        img2img_pipeline.enable_attention_slicing()
        if enable_offload: # Apply to img2img pipe as well
             print("ğŸ“¦ ä¸ºimg2imgç®¡é“å¯ç”¨æ¨¡å‹CPU Offload")
             img2img_pipeline.enable_model_cpu_offload()
        else:
            print("ğŸš« img2imgç®¡é“æ¨¡å‹CPU Offloadå·²ç¦ç”¨")
        
        print(f"âœ… {pipeline_class.__name__} æ¨¡å‹åŠ è½½æˆåŠŸ: {base_path}")
        return txt2img_pipeline, img2img_pipe
        
    except Exception as e:
        print(f"âŒ Error loading diffusers model ({pipeline_class.__name__}): {str(e)}")
        raise e

def load_specific_model(base_model_type: str):
    """åŠ è½½æŒ‡å®šçš„åŸºç¡€æ¨¡å‹ - ä¿®å¤ï¼šä¸è‡ªåŠ¨åŠ è½½LoRA"""
    global txt2img_pipe, img2img_pipe, current_base_model, current_lora_config, current_selected_lora
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    # ğŸš¨ å½»åº•æ¸…ç†ä¹‹å‰çš„æ¨¡å‹ï¼Œé¿å…device conflicts
    if txt2img_pipe is not None:
        print("ğŸ§¹ æ¸…ç†ä¹‹å‰çš„txt2imgæ¨¡å‹...")
        try:
            del txt2img_pipe
        except:
            pass
        txt2img_pipe = None
    
    if img2img_pipe is not None:
        print("ğŸ§¹ æ¸…ç†ä¹‹å‰çš„img2imgæ¨¡å‹...")
        try:
            del img2img_pipe
        except:
            pass
        img2img_pipe = None
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")
    
    model_config = BASE_MODELS[base_model_type]
    device = get_device()
    
    print(f"ğŸ¯ Loading {model_config['name']} model...")
    
    try:
        model_start_time = datetime.now()
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        if model_config["model_type"] == "flux":
            txt2img_pipe, img2img_pipe = load_flux_model(model_config["model_path"], device)
        elif model_config["model_type"] == "diffusers":
            txt2img_pipe, img2img_pipe = load_diffusers_model(model_config["model_path"], device)
        
        current_base_model = base_model_type
        
        # ğŸš¨ ä¿®å¤ï¼šä¸è‡ªåŠ¨åŠ è½½é»˜è®¤LoRAï¼Œä¿æŒæ¸…æ´çŠ¶æ€ï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©
        print("â„¹ï¸  åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆï¼Œæ— é»˜è®¤LoRAï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©LoRA")
        current_lora_config = {}
        current_selected_lora = None
        
        model_time = (datetime.now() - model_start_time).total_seconds()
        print(f"ğŸ‰ {model_config['name']} model loaded successfully in {model_time:.2f}s!")
        
        # ğŸš¨ è·³è¿‡åŠ¨æ¼«æ¨¡å‹çš„é¢„çƒ­æ¨ç†ï¼Œé¿å…ç²¾åº¦é—®é¢˜
        if model_config["model_type"] == "diffusers":
            print("âš¡ è·³è¿‡åŠ¨æ¼«æ¨¡å‹é¢„çƒ­æ¨ç†(é¿å…ç²¾åº¦å…¼å®¹æ€§é—®é¢˜)")
            print("âœ… åŠ¨æ¼«æ¨¡å‹ready for generation (no warmup needed)")
        else:
            # å¯¹FLUXæ¨¡å‹è¿›è¡Œé¢„çƒ­
            try:
                print("ğŸ”¥ Warming up model with test inference...")
                warmup_start = datetime.now()
                with torch.no_grad():
                    test_result = txt2img_pipe(
                        prompt="test",
                        width=512,
                        height=512,
                        num_inference_steps=1,
                        guidance_scale=1.0
                    )
                warmup_time = (datetime.now() - warmup_start).total_seconds()
                print(f"âœ… Model warmup completed in {warmup_time:.2f}s")
            except Exception as warmup_error:
                print(f"âš ï¸  Model warmup failed, but model should still work: {warmup_error}")
        
        print(f"ğŸš€ {model_config['name']} system ready for image generation!")
        
    except Exception as e:
        print(f"âŒ Failed to load {model_config['name']} model: {e}")
        # é‡ç½®å…¨å±€å˜é‡
        txt2img_pipe = None
        img2img_pipe = None
        current_base_model = None
        current_lora_config = {}
        current_selected_lora = None
        raise

def upload_to_r2(image_data: bytes, filename: str) -> str:
    """ä¸Šä¼ å›¾ç‰‡åˆ° Cloudflare R2"""
    try:
        # æ£€æŸ¥R2å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if r2_client is None:
            raise RuntimeError("R2 client not available - check environment variables")
            
        # ç¡®ä¿ image_data æ˜¯ bytes ç±»å‹
        if not isinstance(image_data, bytes):
            raise TypeError(f"Expected bytes, got {type(image_data)}")
            
        # éªŒè¯æ–‡ä»¶å¤§å°
        if len(image_data) == 0:
            raise ValueError("Image data is empty")
            
        if len(image_data) > 50 * 1024 * 1024:  # 50MB limit
            raise ValueError(f"Image too large: {len(image_data)} bytes")
            
        print(f"Uploading {len(image_data)} bytes to R2 as {filename}")
        
        r2_client.put_object(
            Bucket=CLOUDFLARE_R2_BUCKET,
            Key=filename,
            Body=image_data,
            ContentType='image/png',
            ACL='public-read'
        )
        
        # æ„å»ºæ­£ç¡®çš„å…¬å…± URL æ ¼å¼ - ä¼˜å…ˆä½¿ç”¨ R2 Public Domain
        # ä½¿ç”¨æä¾›çš„ R2 public domain (æœ€ç¨³å®šçš„è§£å†³æ–¹æ¡ˆ)
        r2_public_domain = os.getenv("CLOUDFLARE_R2_PUBLIC_BUCKET_DOMAIN")
        
        if r2_public_domain:
            # ä½¿ç”¨ R2 public domain (.r2.dev æ ¼å¼)
            public_url = f"https://{r2_public_domain}/{filename}"
            print(f"âœ“ Successfully uploaded to (R2 public domain): {public_url}")
        elif CLOUDFLARE_R2_PUBLIC_DOMAIN:
            # å¤‡é€‰ï¼šè‡ªå®šä¹‰åŸŸå
            public_url = f"{CLOUDFLARE_R2_PUBLIC_DOMAIN.rstrip('/')}/{filename}"
            print(f"âœ“ Successfully uploaded to (custom domain): {public_url}")
        else:
            # æœ€åå›é€€åˆ°æ ‡å‡†R2æ ¼å¼
            # æ­£ç¡®æ ¼å¼: https://{bucket}.{account_id}.r2.cloudflarestorage.com/{filename}
            # ä»endpoint URLä¸­æå–account ID
            account_id = CLOUDFLARE_R2_ENDPOINT.split('//')[1].split('.')[0]
            public_url = f"https://{CLOUDFLARE_R2_BUCKET}.{account_id}.r2.cloudflarestorage.com/{filename}"
            print(f"âœ“ Successfully uploaded to (standard R2): {public_url}")
            print(f"âš ï¸  æ³¨æ„ï¼šå¦‚æœå‡ºç°CORSé”™è¯¯ï¼Œå»ºè®®è®¾ç½® CLOUDFLARE_R2_PUBLIC_BUCKET_DOMAIN ç¯å¢ƒå˜é‡")
        
        return public_url
        
    except Exception as e:
        print(f"âœ— Error uploading to R2: {str(e)}")
        print(f"Image data type: {type(image_data)}, size: {len(image_data) if hasattr(image_data, '__len__') else 'unknown'}")
        
        # å¯¹äºæ¼”ç¤ºç›®çš„ï¼Œè¿”å›ä¸€ä¸ªå ä½ç¬¦URLè€Œä¸æ˜¯å¤±è´¥
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æŠ›å‡ºå¼‚å¸¸
        placeholder_url = f"https://via.placeholder.com/512x512/cccccc/666666?text=Upload+Failed"
        print(f"Returning placeholder URL: {placeholder_url}")
        return placeholder_url

def image_to_bytes(image: Image.Image) -> bytes:
    """å°† PIL Image è½¬æ¢ä¸ºå­—èŠ‚"""
    try:
        buffer = io.BytesIO()
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image.save(buffer, format='PNG', quality=95, optimize=True)
        buffer.seek(0)  # é‡ç½®bufferä½ç½®
        return buffer.getvalue()
    except Exception as e:
        print(f"Error converting image to bytes: {str(e)}")
        raise e

def base64_to_image(base64_str: str) -> Image.Image:
    """å°† base64 å­—ç¬¦ä¸²è½¬æ¢ä¸º PIL Image"""
    if base64_str.startswith('data:image'):
        base64_str = base64_str.split(',')[1]
    
    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image.convert('RGB')

def process_long_prompt(prompt: str, max_clip_tokens: int = 75, max_t5_tokens: int = 500) -> tuple:
    """
    å¤„ç†é•¿æç¤ºè¯ï¼Œä¸ºFLUXçš„åŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
    
    Args:
        prompt: è¾“å…¥æç¤ºè¯
        max_clip_tokens: CLIPç¼–ç å™¨æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤75ï¼Œç•™2ä¸ªç‰¹æ®Štokenç©ºé—´ï¼‰
        max_t5_tokens: T5ç¼–ç å™¨æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤500ï¼Œç•™ç©ºé—´ç»™ç‰¹æ®Štokenï¼‰
    
    Returns:
        tuple: (clip_prompt, t5_prompt)
    """
    if not prompt:
        return "", ""
    
    # ğŸ¯ æ›´å‡†ç¡®çš„tokenä¼°ç®—ï¼šè€ƒè™‘æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    # ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼ã€é€—å·ã€æ ‡ç‚¹ç¬¦å·åˆ†å‰²
    token_pattern = r'\w+|[^\w\s]'  # æå–regexæ¨¡å¼é¿å…f-stringä¸­çš„åæ–œæ 
    tokens = re.findall(token_pattern, prompt.lower())
    estimated_tokens = len(tokens)
    
    print(f"ğŸ“ Prompt analysis: {len(prompt)} chars, ~{estimated_tokens} tokens (improved estimation)")
    
    if estimated_tokens <= max_clip_tokens:
        # çŸ­promptï¼šä¸¤ä¸ªç¼–ç å™¨éƒ½ä½¿ç”¨å®Œæ•´prompt
        print("âœ… Short prompt: using full prompt for both CLIP and T5")
        return prompt, prompt
    else:
        # é•¿promptï¼šCLIPä½¿ç”¨æˆªæ–­ç‰ˆæœ¬ï¼ŒT5ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬
        if estimated_tokens <= max_t5_tokens:
            # ğŸ¯ æ›´æ™ºèƒ½çš„CLIPæˆªæ–­ï¼šä¿æŒå®Œæ•´çš„è¯­ä¹‰å•å…ƒ
            words = prompt.split()
            
            # ä»å‰å¾€åç´¯ç§¯tokenï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
            clip_words = []
            current_tokens = 0
            
            for word in words:
                # ä¼°ç®—å½“å‰å•è¯çš„tokenæ•°ï¼ˆè€ƒè™‘æ ‡ç‚¹ç¬¦å·ï¼‰
                word_tokens = len(re.findall(token_pattern, word.lower()))
                
                if current_tokens + word_tokens <= max_clip_tokens:
                    clip_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # å¦‚æœæˆªæ–­ç‚¹ä¸ç†æƒ³ï¼Œå°è¯•åœ¨å¥å·æˆ–é€—å·å¤„æˆªæ–­
            if len(clip_words) > 10:  # åªåœ¨æœ‰è¶³å¤Ÿè¯æ±‡æ—¶ä¼˜åŒ–æˆªæ–­ç‚¹
                for i in range(len(clip_words) - 1, max(0, len(clip_words) - 5), -1):
                    if clip_words[i].endswith(('.', ',', ';', '!')):
                        clip_words = clip_words[:i+1]
                        break
            
            clip_prompt = ' '.join(clip_words)
            clip_token_count = len(re.findall(token_pattern, clip_prompt.lower()))
            
            print(f"ğŸ“ Long prompt optimization:")
            print(f"   CLIP prompt: ~{len(clip_words)} words â†’ {clip_token_count} tokens (safe truncation)")
            print(f"   T5 prompt: ~{estimated_tokens} tokens (full prompt)")
            return clip_prompt, prompt
        else:
            # è¶…é•¿promptï¼šä¸¤ä¸ªç¼–ç å™¨éƒ½éœ€è¦æˆªæ–­
            words = prompt.split()
            
            # CLIPæˆªæ–­
            clip_words = []
            current_tokens = 0
            for word in words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if current_tokens + word_tokens <= max_clip_tokens:
                    clip_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # T5æˆªæ–­
            t5_words = []
            current_tokens = 0
            for word in words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if current_tokens + word_tokens <= max_t5_tokens:
                    t5_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # ä¼˜åŒ–æˆªæ–­ç‚¹
            if len(clip_words) > 10:
                for i in range(len(clip_words) - 1, max(0, len(clip_words) - 5), -1):
                    if clip_words[i].endswith(('.', ',', ';')):
                        clip_words = clip_words[:i+1]
                        break
                        
            if len(t5_words) > 20:
                for i in range(len(t5_words) - 1, max(0, len(t5_words) - 10), -1):
                    if t5_words[i].endswith(('.', ',', ';')):
                        t5_words = t5_words[:i+1]
                        break
            
            clip_prompt = ' '.join(clip_words)
            t5_prompt = ' '.join(t5_words)
            
            clip_token_count = len(re.findall(token_pattern, clip_prompt.lower()))
            t5_token_count = len(re.findall(token_pattern, t5_prompt.lower()))
            
            print(f"âš ï¸  Ultra-long prompt: both encoders truncated intelligently")
            print(f"   CLIP prompt: ~{len(clip_words)} words â†’ {clip_token_count} tokens")
            print(f"   T5 prompt: ~{len(t5_words)} words â†’ {t5_token_count} tokens")
            return clip_prompt, t5_prompt

def generate_flux_images(prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str) -> list:
    """FLUXæ¨¡å‹å›¾åƒç”Ÿæˆ"""
    global txt2img_pipe, device_mapping_enabled
    
    # FLUXæ¨¡å‹åŸç”Ÿæ”¯æŒé•¿æç¤ºè¯ï¼Œä½¿ç”¨ä¼˜åŒ–çš„embeddingå¤„ç†
    generation_kwargs = {
        "width": width,
        "height": height,
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating FLUX prompt embeddings using pipeline.encode_prompt()...")
    try:
        device = get_device()
        
        # ğŸš¨ ä¿®å¤ï¼šç®€åŒ–FLUXé•¿promptå¤„ç†ï¼Œé¿å…deviceå†²çª
        print(f"ğŸ’¾ GPU Memory before encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
        
        # ğŸ¯ ä¼˜åŒ–é•¿æç¤ºè¯å¤„ç†ï¼šä¸ºFLUXåŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
        clip_prompt, t5_prompt = process_long_prompt(prompt)
        print(f"ğŸ“ FLUX prompt processing:")
        print(f"   CLIP prompt: {len(clip_prompt)} chars")
        print(f"   T5 prompt: {len(t5_prompt)} chars")
        
        # ğŸš¨ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨pipeline encode_promptï¼Œä¸è¿›è¡ŒCPU/GPUåˆ‡æ¢
        with torch.cuda.amp.autocast(enabled=False):
            prompt_embeds_obj = txt2img_pipe.encode_prompt(
                prompt=clip_prompt,    # CLIPç¼–ç å™¨ä½¿ç”¨ä¼˜åŒ–åçš„prompt
                prompt_2=t5_prompt,    # T5ç¼–ç å™¨ä½¿ç”¨å®Œæ•´prompt
                device=device,
                num_images_per_prompt=1 
            )
        
        # å¤„ç†embeddings
        if hasattr(prompt_embeds_obj, 'prompt_embeds'):
            prompt_embeds = prompt_embeds_obj.prompt_embeds
            pooled_prompt_embeds = prompt_embeds_obj.pooled_prompt_embeds if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds') else None
        else:
            # Handle tuple case
            prompt_embeds = prompt_embeds_obj[0] if isinstance(prompt_embeds_obj, tuple) else None
            pooled_prompt_embeds = prompt_embeds_obj[1] if isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) > 1 else None
        
        # è®¾ç½®embeddingsåˆ°generation_kwargs
        if prompt_embeds is not None:
            generation_kwargs["prompt_embeds"] = prompt_embeds
            print("âœ… FLUX prompt embeddingsç”ŸæˆæˆåŠŸ")
        
        if pooled_prompt_embeds is not None:
            generation_kwargs["pooled_prompt_embeds"] = pooled_prompt_embeds
            print("âœ… FLUX pooled embeddingsç”ŸæˆæˆåŠŸ")

        # FLUXä½¿ç”¨ä¼ ç»Ÿçš„guidance_scaleå‚æ•°
        generation_kwargs["guidance_scale"] = cfg_scale
        print(f"ğŸ›ï¸ Using guidance_scale: {cfg_scale}")
        print(f"ğŸ’¾ GPU Memory after encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

    except Exception as e:
        print(f"âš ï¸ FLUX pipeline.encode_prompt() failed: {e}. Using raw prompts.")
        generation_kwargs["prompt"] = prompt
        # ğŸš¨ FLUXæ¨¡å‹ä¸æ”¯æŒnegative_promptï¼Œç§»é™¤æ­¤å‚æ•°
        # generation_kwargs["negative_prompt"] = negative_prompt  # <-- æ³¨é‡Šæ‰è¿™è¡Œ

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text-to-image")

def generate_diffusers_images(prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str) -> list:
    """ä½¿ç”¨æ ‡å‡†diffusersç®¡é“ç”Ÿæˆå›¾åƒ - æ”¯æŒé•¿æç¤ºè¯å¤„ç†å’ŒWAI-NSFW-illustrious-SDXLä¼˜åŒ–å‚æ•°"""
    global txt2img_pipe
    
    if txt2img_pipe is None:
        raise RuntimeError("Diffusers pipeline not loaded")
    
    print(f"ğŸ“ Processing anime model generation...")
    
    # ğŸš¨ å…¨é¢çš„å‚æ•°å®‰å…¨æ£€æŸ¥å’Œä¿®å¤
    if not prompt or prompt is None:
        prompt = "masterpiece, best quality, amazing quality, 1boy, handsome man, anime style"
        print(f"âš ï¸  ä¿®å¤ç©ºprompt: {prompt}")
    
    if negative_prompt is None:
        negative_prompt = ""
        print(f"âš ï¸  ä¿®å¤None negative_prompt")
    
    # ç¡®ä¿promptå’Œnegative_promptéƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
    prompt = str(prompt).strip()
    negative_prompt = str(negative_prompt).strip()
    
    # ğŸš¨ æ ¹æ®CivitAI WAI-NSFW-illustrious-SDXLæ¨èè®¾ç½®
    # å¼ºåˆ¶ä½¿ç”¨1024x1024æˆ–æ›´å¤§å°ºå¯¸
    if width < 1024 or height < 1024:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹éœ€è¦1024x1024æˆ–æ›´å¤§ ({width}x{height})ï¼Œè°ƒæ•´ä¸º1024x1024")
        width = max(1024, width)
        height = max(1024, height)
    
    # CFG Scale: 5-7 (CivitAIæ¨è)
    if cfg_scale < 5.0:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹CFGè¿‡ä½ ({cfg_scale})ï¼Œè°ƒæ•´ä¸º6.0 (æ¨è5-7)")
        cfg_scale = 6.0
    elif cfg_scale > 7.0:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹CFGè¿‡é«˜ ({cfg_scale})ï¼Œè°ƒæ•´ä¸º6.5 (æ¨è5-7)")
        cfg_scale = 6.5
    
    # Steps: 15-30 (v14), 25-40 (older versions) - æˆ‘ä»¬ä½¿ç”¨25-30
    if steps < 15:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹stepsè¿‡ä½ ({steps})ï¼Œè°ƒæ•´ä¸º20 (æ¨è15-30)")
        steps = 20
    elif steps > 35:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹stepsè¿‡é«˜ ({steps})ï¼Œè°ƒæ•´ä¸º30 (æ¨è15-30)")
        steps = 30
    
    # ğŸš¨ ä¿®å¤ï¼šæ·»åŠ WAI-NSFW-illustrious-SDXLæ¨èçš„è´¨é‡æ ‡ç­¾
    if not prompt.startswith("masterpiece") and "masterpiece" not in prompt.lower():
        prompt = "masterpiece, best quality, amazing quality, " + prompt
        print(f"âœ¨ æ·»åŠ WAI-NSFW-illustrious-SDXLæ¨èè´¨é‡æ ‡ç­¾")
    
    # ğŸš¨ ä¿®å¤ï¼šæ·»åŠ æ¨èçš„è´Ÿé¢æç¤º
    recommended_negative = "bad quality, worst quality, worst detail, sketch, censor"
    if negative_prompt and negative_prompt.strip():
        # å¦‚æœç”¨æˆ·æœ‰è‡ªå®šä¹‰è´Ÿé¢æç¤ºï¼Œæ·»åŠ åˆ°æ¨èè´Ÿé¢æç¤ºä¹‹å
        negative_prompt = recommended_negative + ", " + negative_prompt
    else:
        # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰è´Ÿé¢æç¤ºï¼Œä½¿ç”¨æ¨èçš„
        negative_prompt = recommended_negative
    print(f"ğŸ›¡ï¸ ä½¿ç”¨WAI-NSFW-illustrious-SDXLæ¨èè´Ÿé¢æç¤º")
    
    print(f"ğŸ” æœ€ç»ˆå‚æ•°æ£€æŸ¥:")
    print(f"  prompt: {repr(prompt)} (type: {type(prompt)})")
    print(f"  negative_prompt: {repr(negative_prompt)} (type: {type(negative_prompt)})")
    print(f"  dimensions: {width}x{height}")
    print(f"  steps: {steps}, cfg_scale: {cfg_scale}")
    
    # ğŸ¯ SDXLé•¿æç¤ºè¯å¤„ç† - ä½¿ç”¨Compelæ”¯æŒ500+ tokens
    processed_prompt = prompt
    processed_negative_prompt = negative_prompt
    
    try:
        # ğŸš¨ ä¿®å¤ï¼šä½¿ç”¨æ›´å‡†ç¡®çš„tokenä¼°ç®—æ–¹æ³•
        # è€ƒè™‘æ ‡ç‚¹ç¬¦å·ã€é€—å·åˆ†éš”ç­‰å› ç´ 
        import re
        token_pattern = r'\w+|[^\w\s]'
        estimated_tokens = len(re.findall(token_pattern, prompt.lower()))
        
        # ğŸš¨ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦åŠ è½½äº†LoRAï¼Œå¦‚æœæœ‰LoRAåˆ™ä½¿ç”¨æ™ºèƒ½å‹ç¼©é¿å…é»‘å›¾
        global current_lora_config
        has_lora = bool(current_lora_config and any(v > 0 for v in current_lora_config.values()))
        
        if has_lora:
            print(f"âš ï¸  æ£€æµ‹åˆ°LoRAé…ç½® {current_lora_config}ï¼Œä½¿ç”¨æ™ºèƒ½promptå‹ç¼©é¿å…é»‘å›¾")
            
            # ğŸš¨ ä¿®å¤ï¼šå‹ç¼©æ­£å‘prompt
            if estimated_tokens > 75:
                print(f"ğŸ“ åŸå§‹prompt({estimated_tokens} tokens): {processed_prompt[:100]}...")
                print("ğŸ”§ ä½¿ç”¨æ™ºèƒ½å‹ç¼©å¤„ç†è¶…é•¿prompt...")
                processed_prompt = compress_prompt_to_77_tokens(processed_prompt, max_tokens=75)
                print(f"âœ… æ™ºèƒ½å‹ç¼©å®Œæˆï¼Œé¿å…é»‘å›¾é—®é¢˜")
            else:
                print("âœ… promptå·²åœ¨75 tokené™åˆ¶å†…ï¼Œæ— éœ€å‹ç¼©")
            
            # ğŸš¨ ä¿®å¤ï¼šå‹ç¼©negative prompt
            negative_tokens = len(re.findall(r'\w+|[^\w\s]', processed_negative_prompt.lower()))
            if negative_tokens > 75:
                print(f"ğŸ”§ å‹ç¼©negative prompt: {negative_tokens} tokens -> 75 tokens")
                processed_negative_prompt = compress_prompt_to_77_tokens(processed_negative_prompt, max_tokens=75)
                print(f"âœ… negative promptå‹ç¼©å®Œæˆ")
            
            # ä½¿ç”¨æ ‡å‡†å¤„ç†æ–¹å¼
            generation_kwargs = {
                'prompt': processed_prompt,
                'negative_prompt': processed_negative_prompt,
                'height': height,
                'width': width,
                'num_inference_steps': steps,
                'guidance_scale': cfg_scale,
                'num_images_per_prompt': 1,
                'output_type': 'pil',
                'return_dict': False
            }
        else:
            # æ²¡æœ‰LoRAæ—¶ä½¿ç”¨æ­£å¸¸å¤„ç†
            if estimated_tokens > 50:  # åªæœ‰åœ¨æ²¡æœ‰LoRAæ—¶æ‰ä½¿ç”¨Compel
                print(f"ğŸ“ é•¿æç¤ºè¯æ£€æµ‹: {estimated_tokens} tokensï¼Œå¯ç”¨Compelå¤„ç†")
                
                from compel import Compel
                # ğŸš¨ ä¿®å¤SDXL Compelå‚æ•° - æ·»åŠ text_encoder_2å’Œpooledæ”¯æŒ
                compel = Compel(
                    tokenizer=[txt2img_pipe.tokenizer, txt2img_pipe.tokenizer_2],
                    text_encoder=[txt2img_pipe.text_encoder, txt2img_pipe.text_encoder_2],
                    requires_pooled=[False, True]  # SDXLéœ€è¦pooled embeds
                )
                
                # ç”Ÿæˆé•¿æç¤ºè¯çš„embeddings (åŒ…æ‹¬pooled_prompt_embeds)
                print("ğŸ§¬ ä½¿ç”¨Compelç”Ÿæˆé•¿æç¤ºè¯embeddings...")
                conditioning, pooled_conditioning = compel(processed_prompt)
                negative_conditioning, negative_pooled_conditioning = compel(processed_negative_prompt) if processed_negative_prompt else (None, None)
                
                # ä½¿ç”¨é¢„å¤„ç†çš„embeddings (åŒ…æ‹¬pooled)
                generation_kwargs = {
                    "prompt_embeds": conditioning,
                    "negative_prompt_embeds": negative_conditioning,
                    "pooled_prompt_embeds": pooled_conditioning,
                    "negative_pooled_prompt_embeds": negative_pooled_conditioning,
                    "height": int(height),
                    "width": int(width),
                    "num_inference_steps": int(steps),
                    "guidance_scale": float(cfg_scale),
                    "num_images_per_prompt": 1,
                    "output_type": "pil",
                    "return_dict": True
                }
                print("âœ… é•¿æç¤ºè¯embeddingsç”ŸæˆæˆåŠŸ")
            else:
                print(f"ğŸ“ æ™®é€šæç¤ºè¯é•¿åº¦: {estimated_tokens} tokensï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†")
                # æ ‡å‡†æç¤ºè¯å¤„ç†
                generation_kwargs = {
                    "prompt": processed_prompt,
                    "negative_prompt": processed_negative_prompt,
                    "height": int(height),
                    "width": int(width),
                    "num_inference_steps": int(steps),
                    "guidance_scale": float(cfg_scale),
                    "num_images_per_prompt": 1,
                    "output_type": "pil",
                    "return_dict": True
                }
        
        # ç”Ÿæˆå›¾åƒ
        try:
            print(f"ğŸ¨ ä½¿ç”¨ {current_base_model} æ¨¡å‹ç”Ÿæˆå›¾åƒ...")
            model_config = BASE_MODELS.get(current_base_model, {})
            model_type = model_config.get("model_type", "unknown")
            
            if model_type == "flux":
                print("ğŸ’¡ FLUXæ¨¡å‹æ¨è768x768åˆ†è¾¨ç‡")
                print("ğŸ”§ FLUXæ¨¡å‹ä¼˜åŒ–å‚æ•°(å®˜æ–¹æ¨è): steps=20, cfg_scale=4, size=768x768")
                images = generate_flux_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, current_base_model)
            elif model_type == "diffusers":
                print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹æ¨è1024x1024ä»¥ä¸Šåˆ†è¾¨ç‡")
                print("ğŸ”§ åŠ¨æ¼«æ¨¡å‹ä¼˜åŒ–å‚æ•°(CivitAIæ¨è): steps=20, cfg_scale=6, size=1024x1024")
                images = generate_diffusers_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, current_base_model)
            else:
                print(f"âŒ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
                return {
                    'success': False,
                    'error': f'Unknown model type: {model_type}'
                }
            
            # ğŸš¨ æ£€æŸ¥ç”Ÿæˆç»“æœæ˜¯å¦ä¸ºç©º
            if not images or len(images) == 0:
                print("âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                return {
                    'success': False,
                    'error': 'Image generation failed - no images were created. This may be due to model compatibility issues or parameter problems.'
                }
            
            # åˆ é™¤é‡å¤çš„æ—¥å¿—è¾“å‡º - å·²åœ¨generate_images_commonä¸­ç»Ÿä¸€å¤„ç†
            # print(f"âœ… æˆåŠŸç”Ÿæˆ {len(images)} å¼ å›¾åƒ")
            return {
                'success': True,
                'data': images
            }
            
        except Exception as generation_error:
            print(f"âŒ å›¾åƒç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {generation_error}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {
                'success': False,
                'error': f'Image generation failed: {str(generation_error)}'
            }
        
    except Exception as long_prompt_error:
        print(f"âš ï¸  åˆ†æ®µé•¿promptå¤„ç†å¤±è´¥: {long_prompt_error}")
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        print("ğŸ“ å›é€€åˆ°æ ‡å‡†å¤„ç†æ¨¡å¼")
        
        generation_kwargs = {
            "prompt": processed_prompt,
            "negative_prompt": processed_negative_prompt,
            "height": int(height),
            "width": int(width),
            "num_inference_steps": int(steps),
            "guidance_scale": float(cfg_scale),
            "num_images_per_prompt": 1,
            "output_type": "pil",
            "return_dict": True
        }
        print("âœ… å›é€€åˆ°æ ‡å‡†SDXLå¤„ç†")
    
    # ç§å­è®¾ç½®ç°åœ¨åœ¨generate_images_commonä¸­å¤„ç†ï¼Œæ”¯æŒå¤šå¼ ä¸åŒç§å­
    print(f"ğŸ¯ Generation kwargs: {list(generation_kwargs.keys())}")
    
    return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text_to_image")

def generate_images_common(generation_kwargs: dict, prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str, task_type: str) -> list:
    """é€šç”¨å›¾åƒç”Ÿæˆé€»è¾‘ - æ”¯æŒçœŸæ­£çš„å¤šå¼ ç”Ÿæˆ"""
    global txt2img_pipe, current_base_model
    
    # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸ºNoneï¼Œé¿å…NoneTypeé”™è¯¯
    if prompt is None or prompt == "":
        prompt = "masterpiece, best quality, 1boy"
        print(f"âš ï¸  ç©ºpromptï¼Œä½¿ç”¨é»˜è®¤: {prompt}")
    if negative_prompt is None:
        negative_prompt = ""
        print(f"âš ï¸  negative_promptä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²")
    
    print(f"ğŸ” Debug - prompt: {repr(prompt)}, negative_prompt: {repr(negative_prompt)}")
    
    results = []
    
    # è·å–å½“å‰æ¨¡å‹ç±»å‹ä»¥ç¡®å®šautocastç­–ç•¥
    model_config = BASE_MODELS.get(current_base_model, {})
    model_type = model_config.get("model_type", "unknown")
    
    # ğŸš¨ åŠ¨æ¼«æ¨¡å‹ç¦ç”¨autocasté¿å…LayerNormç²¾åº¦é—®é¢˜
    use_autocast = model_type == "flux"  # åªæœ‰FLUXæ¨¡å‹ä½¿ç”¨autocast
    
    print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆ {num_images} å¼ å›¾åƒ (æ¨¡å‹: {model_type})")
    
    # ğŸ¯ ä¿®å¤ï¼šå¾ªç¯ç”ŸæˆçœŸæ­£çš„å¤šå¼ å›¾ç‰‡
    for i in range(num_images):
        try:
            # ä¸ºæ¯å¼ å›¾ç‰‡è®¾ç½®ä¸åŒçš„éšæœºç§å­
            current_generation_kwargs = generation_kwargs.copy()
            
            if seed != -1:
                # åŸºäºåŸå§‹ç§å­ç”Ÿæˆä¸åŒçš„ç§å­
                current_seed = seed + i
                import torch
                generator = torch.Generator(device=txt2img_pipe.device).manual_seed(int(current_seed))
                current_generation_kwargs["generator"] = generator
                print(f"ğŸ² å›¾åƒ {i+1} ç§å­: {current_seed}")
            else:
                # ğŸš¨ ä¿®å¤ï¼šä¸ºéšæœºç§å­ç”Ÿæˆå…·ä½“çš„ç§å­å€¼å¹¶æ˜¾ç¤º
                import random
                current_seed = random.randint(0, 2147483647)  # ä½¿ç”¨32ä½æ•´æ•°èŒƒå›´
                import torch
                generator = torch.Generator(device=txt2img_pipe.device).manual_seed(int(current_seed))
                current_generation_kwargs["generator"] = generator
                print(f"ğŸ² å›¾åƒ {i+1} ç§å­: {current_seed} (éšæœºç”Ÿæˆ)")
            
            # ç”Ÿæˆå›¾åƒ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨autocast
            if use_autocast:
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    result = txt2img_pipe(**current_generation_kwargs)
            else:
                print(f"ğŸ’¡ åŠ¨æ¼«æ¨¡å‹: è·³è¿‡autocastä½¿ç”¨float32ç²¾åº¦ (å›¾åƒ {i+1})")
                result = txt2img_pipe(**current_generation_kwargs)
            
            # å¤„ç†ç»“æœ
            if hasattr(result, 'images') and result.images and len(result.images) > 0:
                image = result.images[0]  # å–ç¬¬ä¸€å¼ å›¾ç‰‡
                if image is not None:
                    try:
                        # ä¸Šä¼ åˆ°R2
                        filename = f"txt2img_{current_base_model}_{int(time.time())}_{i}.png"
                        image_url = upload_to_r2(image_to_bytes(image), filename)
                        
                        results.append({
                            'url': image_url,
                            'filename': filename,
                            'prompt': prompt,
                            'model': current_base_model,
                            'width': width,
                            'height': height,
                            'steps': steps,
                            'cfg_scale': cfg_scale,
                            'seed': current_seed  # ğŸš¨ ä¿®å¤ï¼šæ€»æ˜¯åŒ…å«å…·ä½“çš„ç§å­å€¼
                        })
                        print(f"âœ… å›¾åƒ {i+1}/{num_images} ç”ŸæˆæˆåŠŸ: {filename}")
                    except Exception as upload_error:
                        print(f"âŒ ä¸Šä¼ å›¾åƒ {i+1} å¤±è´¥: {upload_error}")
                        continue
                else:
                    print(f"âš ï¸  å›¾åƒ {i+1} ç”Ÿæˆç»“æœä¸ºç©º")
            else:
                print(f"âš ï¸  å›¾åƒ {i+1} ç®¡é“è¿”å›ç©ºç»“æœæˆ–æ— å›¾åƒ")
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›¾åƒ {i+1} å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            continue
    
    # åˆ é™¤é‡å¤çš„æ—¥å¿—è¾“å‡º - å·²åœ¨generate_images_commonä¸­ç»Ÿä¸€å¤„ç†
    print(f"ğŸ¯ æ€»å…±æˆåŠŸç”Ÿæˆäº† {len(results)} å¼ å›¾åƒ")
    return results

def text_to_image(prompt: str, negative_prompt: str = "", width: int = 1024, height: int = 1024, steps: int = 25, cfg_scale: float = 5.0, seed: int = -1, num_images: int = 1, base_model: str = "realistic") -> list:
    """æ–‡æœ¬ç”Ÿæˆå›¾åƒ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹"""
    global current_base_model, txt2img_pipe
    
    print(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {base_model}, å½“å‰åŠ è½½æ¨¡å‹: {current_base_model}")
    
    # ğŸš¨ ä¿®å¤ï¼šå…ˆæ£€æŸ¥æ¨¡å‹åˆ‡æ¢ï¼Œå†å¤„ç†LoRAé…ç½®
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦åˆ‡æ¢
    if base_model != current_base_model:
        print(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {base_model}, å½“å‰åŠ è½½æ¨¡å‹: {current_base_model}")
        print(f"ğŸ”„ éœ€è¦åˆ‡æ¢æ¨¡å‹: {current_base_model} -> {base_model}")
        
        try:
            load_specific_model(base_model)
            print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ° {base_model} æ¨¡å‹")
        except Exception as switch_error:
            print(f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {switch_error}")
            return {
                'success': False,
                'error': f'Failed to switch to {base_model} model: {str(switch_error)}'
            }
    
    # ğŸš¨ ç¡®ä¿æœ‰æ¨¡å‹åŠ è½½
    if not txt2img_pipe:
        print("âŒ æ²¡æœ‰åŠ è½½ä»»ä½•æ¨¡å‹")
        return {
            'success': False,
            'error': 'No model loaded. Please switch to a valid model first.'
        }
    
    # ğŸš¨ ä¿®å¤ï¼šæ¨¡å‹åˆ‡æ¢å®Œæˆåï¼Œå†å¤„ç†LoRAé…ç½®
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®ï¼ˆåŒ…æ‹¬é¦–æ¬¡åŠ è½½ï¼‰
    if lora_config:
        print(f"ğŸ¨ æ›´æ–°LoRAé…ç½®: {lora_config}")
        
        # æ£€æŸ¥å½“å‰æ¨¡å‹ç±»å‹
        if current_base_model:
            model_config = BASE_MODELS.get(current_base_model, {})
            model_type = model_config.get("model_type", "unknown")
            print(f"ğŸ¯ å½“å‰æ¨¡å‹ç±»å‹: {model_type}")
            
            # æ¸…ç†ç°æœ‰LoRAæƒé‡
            if txt2img_pipe:
                try:
                    print("ğŸ§¹ Clearing existing LoRA weights...")
                    completely_clear_lora_adapters()
                except Exception as clear_error:
                    print(f"âš ï¸  æ¸…ç†LoRAæƒé‡æ—¶å‡ºé”™: {clear_error}")
            
            # å°è¯•åŠ è½½æ–°çš„LoRAé…ç½®
            try:
                if load_multiple_loras(lora_config):
                    print("âœ… LoRAé…ç½®æ›´æ–°æˆåŠŸ")
                else:
                    print("âš ï¸  LoRAé…ç½®æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
            except Exception as lora_load_error:
                print(f"âš ï¸  LoRAåŠ è½½å‡ºé”™: {lora_load_error}")
                print("â„¹ï¸  ç»§ç»­ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ")
    else:
        print("â„¹ï¸  æ²¡æœ‰LoRAé…ç½®ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ")
    
    # ç”Ÿæˆå›¾åƒ
    try:
        print(f"ğŸ¨ ä½¿ç”¨ {current_base_model} æ¨¡å‹ç”Ÿæˆå›¾åƒ...")
        model_config = BASE_MODELS.get(current_base_model, {})
        model_type = model_config.get("model_type", "unknown")
        
        if model_type == "flux":
            print("ğŸ’¡ FLUXæ¨¡å‹æ¨è768x768åˆ†è¾¨ç‡")
            print("ğŸ”§ FLUXæ¨¡å‹ä¼˜åŒ–å‚æ•°(å®˜æ–¹æ¨è): steps=20, cfg_scale=4, size=768x768")
            images = generate_flux_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, current_base_model)
        elif model_type == "diffusers":
            print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹æ¨è1024x1024ä»¥ä¸Šåˆ†è¾¨ç‡")
            print("ğŸ”§ åŠ¨æ¼«æ¨¡å‹ä¼˜åŒ–å‚æ•°(CivitAIæ¨è): steps=20, cfg_scale=6, size=1024x1024")
            images = generate_diffusers_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, current_base_model)
        else:
            print(f"âŒ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
            return {
                'success': False,
                'error': f'Unknown model type: {model_type}'
            }
        
        # ğŸš¨ æ£€æŸ¥ç”Ÿæˆç»“æœæ˜¯å¦ä¸ºç©º
        if not images or len(images) == 0:
            print("âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
            return {
                'success': False,
                'error': 'Image generation failed - no images were created. This may be due to model compatibility issues or parameter problems.'
            }
        
        # åˆ é™¤é‡å¤çš„æ—¥å¿—è¾“å‡º - å·²åœ¨generate_images_commonä¸­ç»Ÿä¸€å¤„ç†
        # print(f"âœ… æˆåŠŸç”Ÿæˆ {len(images)} å¼ å›¾åƒ")
        return {
            'success': True,
            'data': images
        }
        
    except Exception as generation_error:
        print(f"âŒ å›¾åƒç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {generation_error}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return {
            'success': False,
            'error': f'Image generation failed: {str(generation_error)}'
        }

def image_to_image(params: dict) -> list:
    """å›¾ç”Ÿå›¾ç”Ÿæˆ - ä¼˜åŒ–ç‰ˆæœ¬"""
    global img2img_pipe, current_base_model
    
    if img2img_pipe is None:
        raise ValueError("Image-to-image model not loaded")
    
    # æå–å‚æ•°
    prompt = params.get('prompt', '')
    negative_prompt = params.get('negativePrompt', '')
    image_data = params.get('image', '')
    width = params.get('width', 512)
    height = params.get('height', 512)
    steps = params.get('steps', 20)
    cfg_scale = params.get('cfgScale', 7.0)
    seed = params.get('seed', -1)
    num_images = params.get('numImages', 1)
    denoising_strength = params.get('denoisingStrength', 0.7)
    base_model = params.get('baseModel', 'realistic')
    lora_config = params.get('lora_config', {})
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åŸºç¡€æ¨¡å‹
    if base_model != current_base_model:
        print(f"Switching base model for generation: {current_base_model} -> {base_model}")
        switch_base_model(base_model)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
    if lora_config and lora_config != current_lora_config:
        print(f"Updating LoRA config for generation: {lora_config}")
        load_multiple_loras(lora_config)
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    if isinstance(image_data, str):
        source_image = base64_to_image(image_data)
    else:
        raise ValueError("Invalid image data format")
    
    # è°ƒæ•´å›¾åƒå°ºå¯¸
    source_image = source_image.resize((width, height), Image.Resampling.LANCZOS)
    
    # ğŸ¯ é•¿æç¤ºè¯æ”¯æŒ - å…¨æ–°æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨FLUXåŸç”Ÿå¤„ç† (é€šè¿‡ pipeline.encode_prompt)
    print(f"ğŸ“ Processing prompt for Img2Img: {len(prompt)} characters")
    
    generation_kwargs = {
        "image": source_image,
        # "prompt": prompt, # Replaced by embeds
        # "negative_prompt": negative_prompt, # Replaced by embeds
        "width": width, 
        "height": height,
        "strength": denoising_strength, # For Img2Img
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating prompt embeddings for Img2Img using pipeline.encode_prompt()...")
    try:
        device = get_device()
        
        # Clear GPU cache before encoding
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory before img2img encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

        # Only try to move text encoders to CPU if device mapping is NOT enabled
        # Device mapping conflicts with manual component movement
        text_encoder_device = None
        text_encoder_2_device = None
        
        try:
            if not device_mapping_enabled:
                print("ğŸ“¦ Manual memory management mode for img2img (no device mapping)")
                # Store original devices and move text encoders to CPU
                if hasattr(img2img_pipe, 'text_encoder') and img2img_pipe.text_encoder is not None:
                    text_encoder_device = next(img2img_pipe.text_encoder.parameters()).device
                    if str(text_encoder_device) != 'cpu':
                        print("ğŸ“¦ Moving img2img text_encoder to CPU temporarily...")
                        img2img_pipe.text_encoder.to('cpu')
                        torch.cuda.empty_cache()
                        
                if hasattr(img2img_pipe, 'text_encoder_2') and img2img_pipe.text_encoder_2 is not None:
                    text_encoder_2_device = next(img2img_pipe.text_encoder_2.parameters()).device
                    if str(text_encoder_2_device) != 'cpu':
                        print("ğŸ“¦ Moving img2img text_encoder_2 to CPU temporarily...")
                        img2img_pipe.text_encoder_2.to('cpu')
                        torch.cuda.empty_cache()
                        print(f"ğŸ’¾ GPU Memory after moving img2img encoders to CPU: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            else:
                print("âš¡ Device mapping mode for img2img - trusting accelerate for memory management")

            # Encode positive prompt with memory management
            print("ğŸ”¤ Encoding positive prompt for img2img...")
            
            # ğŸ¯ ä¼˜åŒ–é•¿æç¤ºè¯å¤„ç†ï¼šä¸ºFLUXåŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
            clip_prompt, t5_prompt = process_long_prompt(prompt)
            # FLUXä¸éœ€è¦è´Ÿæç¤ºè¯åµŒå…¥ï¼Œåªå¤„ç†æ­£æç¤ºè¯
            
            with torch.cuda.amp.autocast(enabled=False):
                prompt_embeds_obj = img2img_pipe.encode_prompt(
                    prompt=clip_prompt,    # CLIPç¼–ç å™¨ä½¿ç”¨ä¼˜åŒ–åçš„promptï¼ˆæœ€å¤š77 tokensï¼‰
                    prompt_2=t5_prompt,    # T5ç¼–ç å™¨ä½¿ç”¨å®Œæ•´promptï¼ˆæœ€å¤š512 tokensï¼‰
                    device=device,
                    num_images_per_prompt=1
                )
            
            # Force move embeddings to CPU immediately
            if hasattr(prompt_embeds_obj, 'prompt_embeds'):
                prompt_embeds_cpu = prompt_embeds_obj.prompt_embeds.cpu()
                pooled_prompt_embeds_cpu = prompt_embeds_obj.pooled_prompt_embeds.cpu() if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds') else None
            else:
                prompt_embeds_cpu = prompt_embeds_obj[0].cpu() if isinstance(prompt_embeds_obj, tuple) else None
                pooled_prompt_embeds_cpu = prompt_embeds_obj[1].cpu() if isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) > 1 else None
            
            # Clear cache after positive encoding
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory after positive img2img encoding (moved to CPU): {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

            # âŒ è·³è¿‡è´Ÿæç¤ºè¯åµŒå…¥ç¼–ç ï¼ŒFLUXä¸æ”¯æŒ
            print("âš¡ Skipping negative prompt embedding encoding for img2img (FLUX doesn't support negative_prompt_embeds)")
            
        finally:
            # Restore text encoders to original devices (only if we moved them manually)
            if not device_mapping_enabled:
                if text_encoder_device is not None and hasattr(img2img_pipe, 'text_encoder') and img2img_pipe.text_encoder is not None:
                    print(f"ğŸ“¦ Restoring img2img text_encoder to {text_encoder_device}...")
                    img2img_pipe.text_encoder.to(text_encoder_device)
                    
                if text_encoder_2_device is not None and hasattr(img2img_pipe, 'text_encoder_2') and img2img_pipe.text_encoder_2 is not None:
                    print(f"ğŸ“¦ Restoring img2img text_encoder_2 to {text_encoder_2_device}...")
                    img2img_pipe.text_encoder_2.to(text_encoder_2_device)
            else:
                print("âš¡ Skipping img2img text encoder restoration (device mapping handles placement)")
                
            torch.cuda.empty_cache()

        # Now move embeddings back to GPU and assign to generation_kwargs
        print("ğŸš€ Moving img2img embeddings back to GPU for generation...")
        
        # Move embeddings back to GPU when needed
        generation_kwargs["prompt_embeds"] = prompt_embeds_cpu.to(device)
        # âŒ FLUXä¸æ”¯æŒnegative_prompt_embedså‚æ•°ï¼Œç§»é™¤
        # generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_cpu.to(device)
        
        if pooled_prompt_embeds_cpu is not None:
            generation_kwargs["pooled_prompt_embeds"] = pooled_prompt_embeds_cpu.to(device)
        # âŒ FLUXä¸æ”¯æŒnegative_pooled_prompt_embedså‚æ•°ï¼Œç§»é™¤  
        # if negative_pooled_prompt_embeds_cpu is not None:
        #     generation_kwargs["negative_pooled_prompt_embeds"] = negative_pooled_prompt_embeds_cpu.to(device)

        # FLUXä½¿ç”¨ä¼ ç»Ÿçš„guidance_scaleå‚æ•°
        generation_kwargs["guidance_scale"] = cfg_scale
        print(f"ğŸ›ï¸ Using guidance_scale: {cfg_scale}")
            
        print(f"ğŸ’¾ GPU Memory before generation: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

    except torch.cuda.OutOfMemoryError as oom_error:
        print(f"âŒ CUDA Out of Memory during img2img encode_prompt: {oom_error}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ Cleared GPU cache after img2img OOM error")
        
        raise RuntimeError(f"GPU memory insufficient for img2img prompt encoding. Please try with shorter prompts or switch to a GPU with more memory. Original error: {oom_error}")
        
    except Exception as e:
        print(f"âš ï¸ Img2Img pipeline.encode_prompt() failed: {e}. Traceback follows.")
        traceback.print_exc()
        
        # Clear cache on any error
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("Falling back to using raw prompts for Img2Img (this will likely cause the original error).")
        generation_kwargs["prompt"] = prompt
        generation_kwargs["negative_prompt"] = negative_prompt

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    # è·å–å½“å‰æ¨¡å‹ç±»å‹ä»¥ç¡®å®šautocastç­–ç•¥
    model_config = BASE_MODELS.get(current_base_model, {})
    model_type = model_config.get("model_type", "unknown")
    
    # ğŸš¨ åŠ¨æ¼«æ¨¡å‹ç¦ç”¨autocasté¿å…LayerNormç²¾åº¦é—®é¢˜
    use_autocast = model_type == "flux"  # åªæœ‰FLUXæ¨¡å‹ä½¿ç”¨autocast

    results = []
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡
    if num_images > 1 and num_images <= 4:  # é™åˆ¶æ‰¹é‡å¤§å°é¿å…å†…å­˜é—®é¢˜
        try:
            print(f"Batch generating {num_images} images for img2img...")
            # ç”Ÿæˆå›¾åƒ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨autocast
            if use_autocast:
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    result = img2img_pipe(
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        image=source_image,
                        strength=denoising_strength,
                        num_inference_steps=steps,
                        guidance_scale=cfg_scale,
                        generator=generator,
                        num_images_per_prompt=num_images
                    )
            else:
                # åŠ¨æ¼«æ¨¡å‹ä¸ä½¿ç”¨autocast
                print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹img2img: è·³è¿‡autocastä½¿ç”¨float32ç²¾åº¦")
                result = img2img_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=source_image,
                    strength=denoising_strength,
                    num_inference_steps=steps,
                    guidance_scale=cfg_scale,
                    generator=generator,
                    num_images_per_prompt=num_images
                )
            
            # å¤„ç†æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡
            for i, image in enumerate(result.images):
                try:
                    # ä¸Šä¼ åˆ° R2
                    image_id = str(uuid.uuid4())
                    filename = f"generated/{image_id}.png"
                    image_bytes = image_to_bytes(image)
                    image_url = upload_to_r2(image_bytes, filename)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    image_data = {
                        'id': image_id,
                        'url': image_url,
                        'prompt': prompt,
                        'negativePrompt': negative_prompt,
                        'seed': seed + i,
                        'width': width,
                        'height': height,
                        'steps': steps,
                        'cfgScale': cfg_scale,
                        'baseModel': base_model,
                        'createdAt': datetime.utcnow().isoformat(),
                        'type': 'image-to-image',
                        'denoisingStrength': denoising_strength
                    }
                    
                    results.append(image_data)
                    
                except Exception as e:
                    print(f"Error processing batch img2img image {i+1}: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"Batch img2img generation failed, falling back to individual generation: {str(e)}")
            # å¦‚æœæ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ ç”Ÿæˆ
            num_images = min(num_images, 1)
    
    # å•å¼ ç”Ÿæˆæˆ–æ‰¹é‡ç”Ÿæˆå¤±è´¥çš„å›é€€
    if len(results) == 0:
        for i in range(num_images):
            try:
                print(f"Generating img2img image {i+1}/{num_images}...")
                # ç”Ÿæˆå›¾åƒ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨autocast
                if use_autocast:
                    with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                        # ä¼˜åŒ–ï¼šæ¸…ç†GPUç¼“å­˜
                        if torch.cuda.is_available() and i > 0:
                            torch.cuda.empty_cache()
                            
                        result = img2img_pipe(
                            prompt=prompt,
                            negative_prompt=negative_prompt,
                            image=source_image,
                            strength=denoising_strength,
                            num_inference_steps=steps,
                            guidance_scale=cfg_scale,
                            generator=generator,
                            num_images_per_prompt=1
                        )
                else:
                    # åŠ¨æ¼«æ¨¡å‹ä¸ä½¿ç”¨autocast
                    print(f"ğŸ’¡ åŠ¨æ¼«æ¨¡å‹img2img: ç”Ÿæˆå›¾ç‰‡{i+1}ä½¿ç”¨float32ç²¾åº¦")
                    if torch.cuda.is_available() and i > 0:
                        torch.cuda.empty_cache()
                        
                    result = img2img_pipe(
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        image=source_image,
                        strength=denoising_strength,
                        num_inference_steps=steps,
                        guidance_scale=cfg_scale,
                        generator=generator,
                        num_images_per_prompt=1
                    )
                
                image = result.images[0]
                
                # ä¸Šä¼ åˆ° R2
                image_id = str(uuid.uuid4())
                filename = f"generated/{image_id}.png"
                image_bytes = image_to_bytes(image)
                image_url = upload_to_r2(image_bytes, filename)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                image_data = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed,
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'baseModel': base_model,
                    'createdAt': datetime.utcnow().isoformat(),
                    'type': 'image-to-image',
                    'denoisingStrength': denoising_strength
                }
                
                results.append(image_data)
                
                # ä¸ºä¸‹ä¸€å¼ å›¾ç‰‡æ›´æ–°ç§å­
                if i < num_images - 1:
                    seed += 1
                    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
                    
            except Exception as e:
                print(f"Error generating img2img image {i+1}: {str(e)}")
                continue
    
    # ä¼˜åŒ–ï¼šæœ€ç»ˆæ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results

def get_available_loras() -> dict:
    """è·å–å¯ç”¨çš„LoRAæ¨¡å‹åˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬ï¼ˆå‰ç«¯é™æ€æ˜¾ç¤ºï¼‰"""
    # å‰ç«¯å·²ç»æœ‰é™æ€åˆ—è¡¨ï¼Œè¿™é‡Œåªè¿”å›åŸºæœ¬ä¿¡æ¯
    return {
        "message": "å‰ç«¯ä½¿ç”¨é™æ€LoRAåˆ—è¡¨ï¼Œåç«¯åŠ¨æ€æœç´¢æ–‡ä»¶",
        "search_paths": LORA_SEARCH_PATHS,
        "current_selected": current_selected_lora,
        "current_base_model": current_base_model
    }

def get_loras_by_base_model() -> dict:
    """è·å–æŒ‰åŸºç¡€æ¨¡å‹åˆ†ç»„çš„LoRAåˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬"""
    return {
        "realistic": [
            {"id": "flux_nsfw", "name": "FLUX NSFW", "description": "NSFWçœŸäººå†…å®¹ç”Ÿæˆæ¨¡å‹"},
            {"id": "chastity_cage", "name": "Chastity Cage", "description": "è´æ“ç¬¼ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "dynamic_penis", "name": "Dynamic Penis", "description": "åŠ¨æ€ç”·æ€§è§£å‰–ç”Ÿæˆ"},
            {"id": "masturbation", "name": "Masturbation", "description": "è‡ªæ…°ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "puppy_mask", "name": "Puppy Mask", "description": "å°ç‹—é¢å…·ä¸»é¢˜å†…å®¹"},
            {"id": "butt_and_feet", "name": "Butt and Feet", "description": "è‡€éƒ¨å’Œè¶³éƒ¨ä¸»é¢˜å†…å®¹"},
            {"id": "cumshots", "name": "Cumshots", "description": "å°„ç²¾ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "uncutpenis", "name": "Uncut Penis", "description": "æœªå‰²åŒ…çš®ä¸»é¢˜å†…å®¹"},
            {"id": "doggystyle", "name": "Doggystyle", "description": "åå…¥å¼ä¸»é¢˜å†…å®¹"},
            {"id": "fisting", "name": "Fisting", "description": "æ‹³äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "on_off", "name": "On Off", "description": "ç©¿è¡£/è„±è¡£å¯¹æ¯”å†…å®¹"},
            {"id": "blowjob", "name": "Blowjob", "description": "å£äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "cum_on_face", "name": "Cum on Face", "description": "é¢œå°„ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "anal_sex", "name": "Anal Sex", "description": "è‚›äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"}
        ],
        "anime": [
            {"id": "gayporn", "name": "Gayporn", "description": "ç”·åŒåŠ¨æ¼«é£æ ¼å†…å®¹ç”Ÿæˆ"},
            {"id": "blowjob_handjob", "name": "Blowjob Handjob", "description": "å£äº¤å’Œæ‰‹äº¤åŠ¨æ¼«å†…å®¹"},
            {"id": "furry", "name": "Furry", "description": "å…½äººé£æ ¼åŠ¨æ¼«å†…å®¹"},
            {"id": "sex_slave", "name": "Sex Slave", "description": "æ€§å¥´ä¸»é¢˜åŠ¨æ¼«å†…å®¹"},
            {"id": "comic", "name": "Comic", "description": "æ¼«ç”»é£æ ¼å†…å®¹ç”Ÿæˆ"},
            {"id": "glory_wall", "name": "Glory Wall", "description": "è£è€€å¢™ä¸»é¢˜å†…å®¹"},
            {"id": "multiple_views", "name": "Multiple Views", "description": "å¤šè§†è§’åŠ¨æ¼«å†…å®¹"},
            {"id": "pet_play", "name": "Pet Play", "description": "å® ç‰©æ‰®æ¼”ä¸»é¢˜å†…å®¹"}
        ],
        # ğŸš¨ ä¿®å¤ï¼šç§»é™¤å›ºå®šçš„current_selectedï¼Œè®©å‰ç«¯å†³å®šåˆå§‹é€‰æ‹©
        "message": "LoRAåˆ—è¡¨è·å–æˆåŠŸ - é™æ€é…ç½®ç‰ˆæœ¬"
    }

def switch_single_lora(lora_id: str) -> bool:
    """åˆ‡æ¢å•ä¸ªLoRAæ¨¡å‹ï¼ˆä½¿ç”¨åŠ¨æ€æœç´¢ï¼‰"""
    global txt2img_pipe, img2img_pipe, current_lora_config, current_selected_lora
    
    if txt2img_pipe is None:
        raise ValueError("No pipeline loaded, cannot switch LoRA")
    
    # åŠ¨æ€æœç´¢LoRAæ–‡ä»¶
    lora_path = find_lora_file(lora_id, current_base_model)
    
    if not lora_path:
        raise ValueError(f"LoRAæ–‡ä»¶æœªæ‰¾åˆ°: {lora_id}")
    
    # å¦‚æœå·²ç»æ˜¯å½“å‰LoRAï¼Œç›´æ¥è¿”å›
    if lora_id == current_selected_lora:
        print(f"LoRA {lora_id} å·²ç»åŠ è½½ - è·³è¿‡åˆ‡æ¢")
        return True
    
    try:
        print(f"ğŸ”„ åˆ‡æ¢LoRAåˆ°: {lora_id}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {lora_path}")
        
        # å¸è½½å½“å‰LoRA
        if hasattr(txt2img_pipe, 'unload_lora_weights'):
            txt2img_pipe.unload_lora_weights()
            print("ğŸ§¹ å·²å¸è½½ä¹‹å‰çš„LoRA")
        
        # åŠ è½½æ–°çš„LoRA
        txt2img_pipe.load_lora_weights(lora_path)
        print("âœ… æ–°LoRAåŠ è½½æˆåŠŸ")
        
        # åŒæ­¥åˆ°img2imgç®¡é“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if img2img_pipe and hasattr(img2img_pipe, 'load_lora_weights'):
            try:
                if hasattr(img2img_pipe, 'unload_lora_weights'):
                    img2img_pipe.unload_lora_weights()
                img2img_pipe.load_lora_weights(lora_path)
                print("âœ… img2imgç®¡é“LoRAåŒæ­¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  img2imgç®¡é“LoRAåŒæ­¥å¤±è´¥: {e}")
        
        # æ›´æ–°å½“å‰LoRAé…ç½®
        current_lora_config = {lora_id: 1.0}
        current_selected_lora = lora_id
        
        print(f"ğŸ‰ æˆåŠŸåˆ‡æ¢åˆ°LoRA: {lora_id}")
        return True
        
    except Exception as e:
        print(f"âŒ LoRAåˆ‡æ¢å¤±è´¥: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„LoRA
        if current_selected_lora and current_selected_lora != lora_id:
            try:
                previous_lora_path = find_lora_file(current_selected_lora, current_base_model)
                if previous_lora_path:
                    if hasattr(txt2img_pipe, 'unload_lora_weights'):
                        txt2img_pipe.unload_lora_weights()
                    txt2img_pipe.load_lora_weights(previous_lora_path)
                    print(f"ğŸ”„ å·²æ¢å¤åˆ°ä¹‹å‰çš„LoRA: {current_selected_lora}")
            except Exception as recovery_error:
                print(f"âŒ LoRAæ¢å¤å¤±è´¥: {recovery_error}")
        raise RuntimeError(f"LoRAåˆ‡æ¢å¤±è´¥: {str(e)}")

def load_multiple_loras(lora_config: dict) -> bool:
    """åŠ è½½å¤šä¸ªLoRAæ¨¡å‹åˆ°ç®¡é“ä¸­ - ä¿®å¤é€‚é…å™¨åç§°å†²çªé—®é¢˜"""
    global txt2img_pipe, img2img_pipe, current_base_model, current_lora_config
    
    if txt2img_pipe is None:
        print("âŒ No pipeline loaded, cannot load LoRAs")
        return False
    
    if not lora_config:
        print("â„¹ï¸  No LoRA configuration provided")
        return True
    
    # è·å–å½“å‰æ¨¡å‹ç±»å‹
    current_model_type = BASE_MODELS.get(current_base_model, {}).get("model_type", "unknown")
    print(f"ğŸ¯ å½“å‰æ¨¡å‹ç±»å‹: {current_model_type}")
    
    try:
        # ğŸš¨ ä¿®å¤ï¼šä½¿ç”¨æ›´å½»åº•çš„æ¸…ç†æ–¹æ³•
        completely_clear_lora_adapters()
        
        # åŠ¨æ€æœç´¢å¹¶è¿‡æ»¤å…¼å®¹çš„LoRA
        compatible_loras = {}
        for lora_id, weight in lora_config.items():
            if weight <= 0:
                continue
            
            # åŠ¨æ€æœç´¢LoRAæ–‡ä»¶
            lora_path = find_lora_file(lora_id, current_base_model)
            if not lora_path:
                print(f"âš ï¸  LoRAæ–‡ä»¶æœªæ‰¾åˆ°: {lora_id}")
                continue
                
            compatible_loras[lora_id] = {
                "path": lora_path,
                "weight": weight
            }
        
        if not compatible_loras:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°å…¼å®¹çš„LoRAæ¨¡å‹")
            return True
        
        print(f"ğŸ¨ Loading {len(compatible_loras)} compatible LoRA(s): {list(compatible_loras.keys())}")
        
        # åŠ è½½å…¼å®¹çš„LoRA
        lora_paths = []
        lora_weights = []
        
        for lora_id, lora_data in compatible_loras.items():
            lora_paths.append(lora_data["path"])
            lora_weights.append(lora_data["weight"])
            print(f"  ğŸ“¦ {lora_id}: {lora_data['path']} (weight: {lora_data['weight']})")
        
        if current_model_type == "flux":
            # FLUXæ¨¡å‹ä½¿ç”¨æ—§ç‰ˆAPI
            for i, (lora_path, weight) in enumerate(zip(lora_paths, lora_weights)):
                print(f"ğŸ”§ åŠ è½½FLUX LoRA {i+1}/{len(lora_paths)}: {lora_path}")
                
                # å¸è½½ä¹‹å‰çš„LoRAï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(txt2img_pipe, 'unload_lora_weights'):
                    txt2img_pipe.unload_lora_weights()
                
                # åŠ è½½æ–°çš„LoRA
                txt2img_pipe.load_lora_weights(lora_path)
                
                # FLUXçš„æƒé‡é€šè¿‡cross_attention_kwargsè®¾ç½®
                if hasattr(txt2img_pipe, 'set_lora_scale'):
                    txt2img_pipe.set_lora_scale(weight)
                    print(f"âœ… FLUX LoRAæƒé‡è®¾ç½®: {weight}")
                
        elif current_model_type == "diffusers":
            # æ ‡å‡†diffusersæ¨¡å‹ä½¿ç”¨load_lora_weightså’Œset_adapters
            if len(compatible_loras) == 1:
                # å•ä¸ªLoRA
                lora_path = lora_paths[0]
                weight = lora_weights[0]
                lora_id = list(compatible_loras.keys())[0]
                
                # ğŸš¨ ä¿®å¤ï¼šä½¿ç”¨æ›´å¼ºçš„å”¯ä¸€æ€§ä¿è¯
                import time
                import random
                import uuid
                unique_id = str(uuid.uuid4())[:8]  # 8ä½UUID
                timestamp = int(time.time() * 1000)  # æ¯«ç§’çº§æ—¶é—´æˆ³
                unique_adapter_name = f"{lora_id}_{timestamp}_{unique_id}"
                print(f"ğŸ”§ ä½¿ç”¨æ–°ç‰ˆdiffusers LoRA APIåŠ è½½: {lora_id} (é€‚é…å™¨å: {unique_adapter_name})")
                
                # å…ˆæ£€æŸ¥é€‚é…å™¨æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨å°±å¼ºåˆ¶æ¸…ç†
                if hasattr(txt2img_pipe.unet, '_lora_adapters') and unique_adapter_name in txt2img_pipe.unet._lora_adapters:
                    print(f"âš ï¸  æ£€æµ‹åˆ°é€‚é…å™¨åç§°å†²çªï¼Œé‡æ–°ç”Ÿæˆ: {unique_adapter_name}")
                    unique_id = str(uuid.uuid4())[:8]
                    unique_adapter_name = f"{lora_id}_{timestamp}_{unique_id}_retry"
                
                txt2img_pipe.load_lora_weights(lora_path, adapter_name=unique_adapter_name)
                
                # ä½¿ç”¨æ–°çš„set_adaptersæ–¹æ³•è®¾ç½®æƒé‡ï¼Œé¿å…cross_attention_kwargsé”™è¯¯
                txt2img_pipe.set_adapters([unique_adapter_name], adapter_weights=[weight])
                
                # åŒæ­¥åˆ°img2imgç®¡é“
                if img2img_pipe:
                    img2img_pipe.load_lora_weights(lora_path, adapter_name=unique_adapter_name)
                    img2img_pipe.set_adapters([unique_adapter_name], adapter_weights=[weight])
                    
                print(f"âœ… æˆåŠŸè®¾ç½®LoRAæƒé‡: {lora_id} = {weight}")
                
            else:
                # å¤šä¸ªLoRA
                adapter_names = []
                adapter_weights = lora_weights
                
                print(f"ğŸ”§ åŠ è½½å¤šä¸ªLoRA: {list(compatible_loras.keys())}")
                
                # é€ä¸ªåŠ è½½LoRAï¼Œä½¿ç”¨æ›´å¼ºçš„å”¯ä¸€é€‚é…å™¨åç§°
                import time
                import random
                import uuid
                timestamp = int(time.time() * 1000)
                for i, (lora_id, lora_data) in enumerate(compatible_loras.items()):
                    unique_id = str(uuid.uuid4())[:8]
                    unique_adapter_name = f"{lora_id}_{timestamp}_{i}_{unique_id}"
                    adapter_names.append(unique_adapter_name)
                    
                    # æ£€æŸ¥å†²çª
                    if hasattr(txt2img_pipe.unet, '_lora_adapters') and unique_adapter_name in txt2img_pipe.unet._lora_adapters:
                        print(f"âš ï¸  æ£€æµ‹åˆ°å¤šLoRAé€‚é…å™¨åç§°å†²çªï¼Œé‡æ–°ç”Ÿæˆ: {unique_adapter_name}")
                        unique_id = str(uuid.uuid4())[:8]
                        unique_adapter_name = f"{lora_id}_{timestamp}_{i}_{unique_id}_retry"
                        adapter_names[-1] = unique_adapter_name
                    
                    txt2img_pipe.load_lora_weights(lora_data["path"], adapter_name=unique_adapter_name)
                    if img2img_pipe:
                        img2img_pipe.load_lora_weights(lora_data["path"], adapter_name=unique_adapter_name)
                
                # ä¸€æ¬¡æ€§è®¾ç½®æ‰€æœ‰æƒé‡
                txt2img_pipe.set_adapters(adapter_names, adapter_weights=adapter_weights)
                if img2img_pipe:
                    img2img_pipe.set_adapters(adapter_names, adapter_weights=adapter_weights)
                    
                print(f"âœ… æˆåŠŸè®¾ç½®å¤šä¸ªLoRAæƒé‡: {dict(zip(list(compatible_loras.keys()), adapter_weights))}")
        
        # æ›´æ–°å½“å‰é…ç½®
        current_lora_config.update(lora_config)
        print(f"âœ… Successfully loaded {len(compatible_loras)} LoRA(s)")
        return True
        
    except Exception as e:
        print(f"âŒ Error loading multiple LoRAs: {e}")
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # ğŸš¨ ä¿®å¤ï¼šLoRAåŠ è½½å¤±è´¥åçš„æ¸…ç†
        try:
            completely_clear_lora_adapters()
            print("ğŸ§¹ LoRAå¤±è´¥åçŠ¶æ€å·²æ¸…ç†")
        except Exception as cleanup_error:
            print(f"âš ï¸  æ¸…ç†å¤±è´¥åçŠ¶æ€æ—¶å‡ºé”™: {cleanup_error}")
        
        return False

def switch_base_model(base_model_type: str) -> bool:
    """åˆ‡æ¢åŸºç¡€æ¨¡å‹"""
    global current_base_model
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    if current_base_model == base_model_type:
        print(f"Base model {BASE_MODELS[base_model_type]['name']} is already loaded")
        return True
    
    try:
        print(f"Switching base model from {BASE_MODELS[current_base_model]['name']} to {BASE_MODELS[base_model_type]['name']}")
        
        # é‡Šæ”¾å½“å‰æ¨¡å‹å†…å­˜
        global txt2img_pipe, img2img_pipe
        if txt2img_pipe is not None:
            del txt2img_pipe
        if img2img_pipe is not None:
            del img2img_pipe
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # åŠ è½½æ–°çš„åŸºç¡€æ¨¡å‹
        load_specific_model(base_model_type)
        
        print(f"Successfully switched to {BASE_MODELS[base_model_type]['name']}")
        return True
        
    except Exception as e:
        print(f"Failed to switch base model: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„æ¨¡å‹
        try:
            load_specific_model(current_base_model)
            print(f"Recovered to previous model: {BASE_MODELS[current_base_model]['name']}")
        except Exception as recovery_error:
            print(f"Failed to recover base model: {recovery_error}")
        raise RuntimeError(f"Failed to switch base model: {str(e)}")

def handler(job):
    """RunPod å¤„ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        job_input = job['input']
        task_type = job_input.get('task_type')
        
        if task_type == 'get-loras':
            # è·å–å¯ç”¨LoRAåˆ—è¡¨ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            available_loras = get_available_loras()
            return {
                'success': True,
                'data': {
                    'loras': available_loras,
                    'current_config': current_lora_config
                }
            }
            
        elif task_type == 'get-loras-by-model':
            # è·å–æŒ‰åŸºç¡€æ¨¡å‹åˆ†ç»„çš„LoRAåˆ—è¡¨ï¼ˆæ–°çš„å•é€‰UIï¼‰
            loras_by_model = get_loras_by_base_model()
            return {
                'success': True,
                'data': loras_by_model
            }
            
        elif task_type == 'switch-single-lora':
            # åˆ‡æ¢å•ä¸ªLoRAæ¨¡å‹ï¼ˆæ–°çš„å•é€‰æ¨¡å¼ï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            success = switch_single_lora(lora_id)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_selected_lora': current_selected_lora,
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
            
        elif task_type == 'switch-lora':
            # åˆ‡æ¢LoRAæ¨¡å‹ï¼ˆå•ä¸ªLoRAå…¼å®¹æ€§æ”¯æŒï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            # å…¼å®¹å•LoRAåˆ‡æ¢
            single_lora_config = {lora_id: 1.0}
            success = load_multiple_loras(single_lora_config)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
        
        elif task_type == 'load-loras':
            # åŠ è½½å¤šä¸ªLoRAæ¨¡å‹é…ç½®
            lora_config = job_input.get('lora_config', {})
            if not lora_config:
                return {
                    'success': False,
                    'error': 'lora_config is required'
                }
            
            success = load_multiple_loras(lora_config)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Loaded {len([k for k, v in lora_config.items() if v > 0])} LoRA models'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': 'Failed to load LoRA configuration'
                }
        
        elif task_type == 'text-to-image':
            # æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆ
            print("ğŸ“ Processing text-to-image request...")
            
            # æå–å‚æ•°
            prompt = job_input.get('prompt', '')
            negative_prompt = job_input.get('negativePrompt', '') 
            width = job_input.get('width', 1024)
            height = job_input.get('height', 1024)
            steps = job_input.get('steps', 25)
            cfg_scale = job_input.get('cfgScale', 5.0)
            seed = job_input.get('seed', -1)
            num_images = job_input.get('numImages', 1)
            base_model = job_input.get('baseModel', 'realistic')
            lora_config = job_input.get('lora_config', {})
            
            # ğŸš¨ ä¿®å¤ï¼šå…ˆæ£€æŸ¥æ¨¡å‹åˆ‡æ¢ï¼Œå†å¤„ç†LoRAé…ç½®
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦åˆ‡æ¢
            if base_model != current_base_model:
                print(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {base_model}, å½“å‰åŠ è½½æ¨¡å‹: {current_base_model}")
                print(f"ğŸ”„ éœ€è¦åˆ‡æ¢æ¨¡å‹: {current_base_model} -> {base_model}")
                
                try:
                    load_specific_model(base_model)
                    print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ° {base_model} æ¨¡å‹")
                except Exception as switch_error:
                    print(f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {switch_error}")
                    return {
                        'success': False,
                        'error': f'Failed to switch to {base_model} model: {str(switch_error)}'
                    }
            
            # ğŸš¨ ç¡®ä¿æœ‰æ¨¡å‹åŠ è½½
            if not txt2img_pipe:
                print("âŒ æ²¡æœ‰åŠ è½½ä»»ä½•æ¨¡å‹")
                return {
                    'success': False,
                    'error': 'No model loaded. Please switch to a valid model first.'
                }
            
            # ğŸš¨ ä¿®å¤ï¼šæ¨¡å‹åˆ‡æ¢å®Œæˆåï¼Œå†å¤„ç†LoRAé…ç½®
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®ï¼ˆåŒ…æ‹¬é¦–æ¬¡åŠ è½½ï¼‰
            if lora_config:
                print(f"ğŸ¨ æ›´æ–°LoRAé…ç½®: {lora_config}")
                
                # æ£€æŸ¥å½“å‰æ¨¡å‹ç±»å‹
                if current_base_model:
                    model_config = BASE_MODELS.get(current_base_model, {})
                    model_type = model_config.get("model_type", "unknown")
                    print(f"ğŸ¯ å½“å‰æ¨¡å‹ç±»å‹: {model_type}")
                    
                    # æ¸…ç†ç°æœ‰LoRAæƒé‡
                    if txt2img_pipe:
                        try:
                            print("ğŸ§¹ Clearing existing LoRA weights...")
                            completely_clear_lora_adapters()
                        except Exception as clear_error:
                            print(f"âš ï¸  æ¸…ç†LoRAæƒé‡æ—¶å‡ºé”™: {clear_error}")
                    
                    # å°è¯•åŠ è½½æ–°çš„LoRAé…ç½®
                    try:
                        if load_multiple_loras(lora_config):
                            print("âœ… LoRAé…ç½®æ›´æ–°æˆåŠŸ")
                        else:
                            print("âš ï¸  LoRAé…ç½®æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
                    except Exception as lora_load_error:
                        print(f"âš ï¸  LoRAåŠ è½½å‡ºé”™: {lora_load_error}")
                        print("â„¹ï¸  ç»§ç»­ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ")
            else:
                print("â„¹ï¸  æ²¡æœ‰LoRAé…ç½®ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ")
            
            # ç”Ÿæˆå›¾åƒ
            try:
                print(f"ğŸ¨ ä½¿ç”¨ {current_base_model} æ¨¡å‹ç”Ÿæˆå›¾åƒ...")
                model_config = BASE_MODELS.get(current_base_model, {})
                model_type = model_config.get("model_type", "unknown")
                
                if model_type == "flux":
                    print("ğŸ’¡ FLUXæ¨¡å‹æ¨è768x768åˆ†è¾¨ç‡")
                    print("ğŸ”§ FLUXæ¨¡å‹ä¼˜åŒ–å‚æ•°(å®˜æ–¹æ¨è): steps=20, cfg_scale=4, size=768x768")
                    images = generate_flux_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, current_base_model)
                elif model_type == "diffusers":
                    print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹æ¨è1024x1024ä»¥ä¸Šåˆ†è¾¨ç‡")
                    print("ğŸ”§ åŠ¨æ¼«æ¨¡å‹ä¼˜åŒ–å‚æ•°(CivitAIæ¨è): steps=20, cfg_scale=6, size=1024x1024")
                    images = generate_diffusers_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, current_base_model)
                else:
                    print(f"âŒ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
                    return {
                        'success': False,
                        'error': f'Unknown model type: {model_type}'
                    }
                
                # ğŸš¨ æ£€æŸ¥ç”Ÿæˆç»“æœæ˜¯å¦ä¸ºç©º
                if not images or len(images) == 0:
                    print("âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                    return {
                        'success': False,
                        'error': 'Image generation failed - no images were created. This may be due to model compatibility issues or parameter problems.'
                    }
                
                # åˆ é™¤é‡å¤çš„æ—¥å¿—è¾“å‡º - å·²åœ¨generate_images_commonä¸­ç»Ÿä¸€å¤„ç†
                # print(f"âœ… æˆåŠŸç”Ÿæˆ {len(images)} å¼ å›¾åƒ")
                return {
                    'success': True,
                    'data': images
                }
                
            except Exception as generation_error:
                print(f"âŒ å›¾åƒç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {generation_error}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                return {
                    'success': False,
                    'error': f'Image generation failed: {str(generation_error)}'
                }
            
        elif task_type == 'image-to-image':
            # å›¾åƒè½¬å›¾åƒç”Ÿæˆ - ä¿®å¤å‚æ•°æå–
            print("ğŸ“ Processing image-to-image request...")
            
            # ç›´æ¥ä»job_inputæå–å‚æ•°ï¼Œè€Œä¸æ˜¯åµŒå¥—çš„paramså¯¹è±¡
            params = {
                'prompt': job_input.get('prompt', ''),
                'negativePrompt': job_input.get('negativePrompt', ''),
                'image': job_input.get('image', ''),
                'width': job_input.get('width', 512),
                'height': job_input.get('height', 512),
                'steps': job_input.get('steps', 20),
                'cfgScale': job_input.get('cfgScale', 7.0),
                'seed': job_input.get('seed', -1),
                'numImages': job_input.get('numImages', 1),
                'denoisingStrength': job_input.get('denoisingStrength', 0.7),
                'baseModel': job_input.get('baseModel', 'realistic'),
                'lora_config': job_input.get('lora_config', {})
            }
            
            requested_lora_config = params.get('lora_config', current_lora_config)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
            if requested_lora_config != current_lora_config:
                print(f"Auto-loading LoRA config for generation: {requested_lora_config}")
                load_multiple_loras(requested_lora_config)
            
            results = image_to_image(params)
            return {
                'success': True,
                'data': results
            }
            
        elif task_type == 'switch-base-model':
            # åˆ‡æ¢åŸºç¡€æ¨¡å‹
            base_model_type = job_input.get('base_model_type')
            if not base_model_type:
                return {
                    'success': False,
                    'error': 'base_model_type is required'
                }
            
            success = switch_base_model(base_model_type)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_base_model': current_base_model
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {base_model_type}'
                }
        
        else:
            return {
                'success': False,
                'error': f'Unknown task type: {task_type}'
            }
            
    except Exception as e:
        print(f"Handler error: {str(e)}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }

# Note: The serverless worker will be started by start_debug.py
# This allows for better debugging and health checks before startup 
# This allows for better debugging and health checks before startup 

# ç®€åŒ–çš„LoRAé…ç½® - å‰ç«¯é™æ€æ˜¾ç¤ºï¼Œåç«¯åŠ¨æ€æœç´¢æ–‡ä»¶
LORA_SEARCH_PATHS = {
    "realistic": [
        "/runpod-volume/lora",
        "/runpod-volume/lora/flux_nsfw",
        "/runpod-volume/lora/realistic"
    ],
    "anime": [
        "/runpod-volume/cartoon/lora",
        "/runpod-volume/anime/lora",
        "/runpod-volume/cartoon"
    ]
}

# LoRAåç§°åˆ°å¯èƒ½æ–‡ä»¶åçš„æ˜ å°„
LORA_FILE_PATTERNS = {
    # çœŸäººé£æ ¼LoRA
    "flux_nsfw": ["flux_nsfw", "flux_nsfw.safetensors"],
    "chastity_cage": ["Chastity_Cage.safetensors", "chastity_cage.safetensors", "ChastityCase.safetensors"],
    "dynamic_penis": ["DynamicPenis.safetensors", "dynamic_penis.safetensors"],
    "masturbation": ["Masturbation.safetensors", "masturbation.safetensors"],
    "puppy_mask": ["Puppy_mask.safetensors", "puppy_mask.safetensors", "PuppyMask.safetensors"],
    "butt_and_feet": ["butt-and-feet.safetensors", "butt_and_feet.safetensors", "ButtAndFeet.safetensors"],
    "cumshots": ["cumshots.safetensors", "Cumshots.safetensors"],
    "uncutpenis": ["uncutpenis.safetensors", "UncutPenis.safetensors", "uncut_penis.safetensors"],
    "doggystyle": ["Doggystyle.safetensors", "doggystyle.safetensors", "doggy_style.safetensors"],
    "fisting": ["Fisting.safetensors", "fisting.safetensors"],
    "on_off": ["OnOff.safetensors", "on_off.safetensors", "onoff.safetensors"],
    "blowjob": ["blowjob.safetensors", "Blowjob.safetensors", "blow_job.safetensors"],
    "cum_on_face": ["cumonface.safetensors", "cum_on_face.safetensors", "CumOnFace.safetensors"],
    "anal_sex": ["Anal_sex.safetensors", "anal_sex.safetensors", "AnalSex.safetensors", "analsex.safetensors"],
    
    # åŠ¨æ¼«é£æ ¼LoRA - ç§»é™¤anime_nsfwï¼Œå› ä¸ºå®ƒç°åœ¨æ˜¯åº•å±‚æ¨¡å‹
    "gayporn": ["Gayporn.safetensors", "gayporn.safetensors", "GayPorn.safetensors"],
    "blowjob_handjob": ["Blowjob_Handjob.safetensors", "blowjob_handjob.safetensors", "BlowjobHandjob.safetensors"],
    "furry": ["Furry.safetensors", "furry.safetensors", "FURRY.safetensors"],
    "sex_slave": ["Sex_slave.safetensors", "sex_slave.safetensors", "SexSlave.safetensors"],
    "comic": ["comic.safetensors", "Comic.safetensors", "COMIC.safetensors"],
    "glory_wall": ["glory_wall.safetensors", "Glory_wall.safetensors", "GloryWall.safetensors"],
    "multiple_views": ["multiple_views.safetensors", "Multiple_views.safetensors", "MultipleViews.safetensors"],
    "pet_play": ["pet_play.safetensors", "Pet_play.safetensors", "PetPlay.safetensors"]
}

def find_lora_file(lora_id: str, base_model: str) -> str:
    """åŠ¨æ€æœç´¢LoRAæ–‡ä»¶è·¯å¾„ - å¢å¼ºæœç´¢é€»è¾‘"""
    search_paths = LORA_SEARCH_PATHS.get(base_model, [])
    file_patterns = LORA_FILE_PATTERNS.get(lora_id, [lora_id])
    
    print(f"ğŸ” æœç´¢LoRAæ–‡ä»¶: {lora_id} (æ¨¡å‹: {base_model})")
    
    for base_path in search_paths:
        if not os.path.exists(base_path):
            print(f"  âŒ è·¯å¾„ä¸å­˜åœ¨: {base_path}")
            continue
            
        print(f"  ğŸ“ æœç´¢ç›®å½•: {base_path}")
        
        # å°è¯•ç²¾ç¡®åŒ¹é…
        for pattern in file_patterns:
            full_path = os.path.join(base_path, pattern)
            if os.path.exists(full_path):
                print(f"  âœ… æ‰¾åˆ°æ–‡ä»¶: {full_path}")
                return full_path
        
        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å«lora_idï¼‰
        try:
            for filename in os.listdir(base_path):
                if filename.endswith(('.safetensors', '.safetensor', '.ckpt', '.pt')):
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«lora_idçš„å…³é”®è¯
                    name_lower = filename.lower()
                    lora_lower = lora_id.lower().replace('_', '').replace('-', '')
                    
                    if lora_lower in name_lower.replace('_', '').replace('-', ''):
                        full_path = os.path.join(base_path, filename)
                        print(f"  âœ… æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°: {full_path}")
                        return full_path
        except Exception as e:
            print(f"  âŒ æœç´¢é”™è¯¯: {e}")
    
    print(f"  âŒ æœªæ‰¾åˆ°LoRAæ–‡ä»¶: {lora_id}")
    return None

# ç§»é™¤å¤æ‚çš„åŠ¨æ€æ‰«æï¼Œä½¿ç”¨ç®€å•çš„é™æ€é…ç½®
# AVAILABLE_LORAS = None
# LORAS_LAST_SCAN = 0
# LORAS_CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜
# åŠ¨æ¼«æ¨¡å‹æ–°å¢LoRAåˆ—è¡¨
ANIME_ADDITIONAL_LORAS = {
    "blowjob_handjob": "/runpod-volume/cartoon/lora/Blowjob_Handjob.safetensors",
    "furry": "/runpod-volume/cartoon/lora/Furry.safetensors", 
    "sex_slave": "/runpod-volume/cartoon/lora/Sex_slave.safetensors",
    "comic": "/runpod-volume/cartoon/lora/comic.safetensors",
    "glory_wall": "/runpod-volume/cartoon/lora/glory_wall.safetensors",
    "multiple_views": "/runpod-volume/cartoon/lora/multiple_views.safetensors",
    "pet_play": "/runpod-volume/cartoon/lora/pet_play.safetensors"
}

def completely_clear_lora_adapters():
    """å®Œå…¨æ¸…ç†æ‰€æœ‰LoRAé€‚é…å™¨ - æœ€å½»åº•çš„æ¸…ç†æ–¹æ³•"""
    global txt2img_pipe, img2img_pipe
    
    print("ğŸ§¹ å¼€å§‹å®Œå…¨æ¸…ç†LoRAé€‚é…å™¨...")
    
    # æ¸…ç†ç®¡é“åˆ—è¡¨
    pipelines = [txt2img_pipe]
    if img2img_pipe:
        pipelines.append(img2img_pipe)
    
    for i, pipe in enumerate(pipelines):
        if pipe is None:
            continue
        
        pipeline_name = "txt2img" if i == 0 else "img2img"
        
        try:
            # ç¬¬1å±‚ï¼šæ ‡å‡†çš„unload_lora_weightsæ–¹æ³•
            if hasattr(pipe, 'unload_lora_weights'):
                pipe.unload_lora_weights()
                print("âœ… æ ‡å‡†unload_lora_weightså®Œæˆ")
            
            # ç¬¬2å±‚ï¼šæ¸…ç†UNetä¸­çš„ç‰¹å®šLoRAé…ç½®
            if hasattr(pipe, 'unet') and pipe.unet:
                unet = pipe.unet
                
                # æ¸…ç†_hf_peft_config_loaded
                if hasattr(unet, '_hf_peft_config_loaded'):
                    delattr(unet, '_hf_peft_config_loaded')
                    print("ğŸ”§ æ¸…ç†UNet._hf_peft_config_loaded")
                
                # ğŸš¨ æ–°å¢ï¼šæ¸…ç†PEFTç›¸å…³çš„é€‚é…å™¨ç¼“å­˜
                if hasattr(unet, 'peft_config') and unet.peft_config:
                    unet.peft_config.clear()
                    print("ğŸ”§ æ¸…ç†UNet.peft_config")
                
                # ğŸš¨ æ–°å¢ï¼šæ¸…ç†é€‚é…å™¨åç§°ç¼“å­˜
                if hasattr(unet, '_lora_adapters'):
                    unet._lora_adapters.clear()
                    print("ğŸ”§ æ¸…ç†UNet._lora_adapters")
                
                # ğŸš¨ æ–°å¢ï¼šå¼ºåˆ¶æ¸…ç†æ‰€æœ‰å¯èƒ½çš„é€‚é…å™¨æ®‹ç•™
                adapter_attrs = ['_lora_adapters', 'peft_config', '_adapter_names', '_active_adapters']
                for attr in adapter_attrs:
                    if hasattr(unet, attr):
                        try:
                            attr_obj = getattr(unet, attr)
                            if hasattr(attr_obj, 'clear'):
                                attr_obj.clear()
                            elif isinstance(attr_obj, dict):
                                attr_obj.clear()
                            elif isinstance(attr_obj, list):
                                attr_obj.clear()
                            print(f"ğŸ”§ æ¸…ç†UNet.{attr}")
                        except Exception as attr_error:
                            print(f"âš ï¸  æ¸…ç†UNet.{attr}æ—¶å‡ºé”™: {attr_error}")
                
                # ğŸš¨ æ–°å¢ï¼šæ¸…ç†Text Encoderé€‚é…å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if hasattr(pipe, 'text_encoder') and pipe.text_encoder:
                    text_encoder = pipe.text_encoder
                    for attr in adapter_attrs:
                        if hasattr(text_encoder, attr):
                            try:
                                attr_obj = getattr(text_encoder, attr)
                                if hasattr(attr_obj, 'clear'):
                                    attr_obj.clear()
                                elif isinstance(attr_obj, dict):
                                    attr_obj.clear()
                                elif isinstance(attr_obj, list):
                                    attr_obj.clear()
                                print(f"ğŸ”§ æ¸…ç†TextEncoder.{attr}")
                            except Exception as attr_error:
                                print(f"âš ï¸  æ¸…ç†TextEncoder.{attr}æ—¶å‡ºé”™: {attr_error}")
                
                # ğŸš¨ æ–°å¢ï¼šå¦‚æœæœ‰ç¬¬äºŒä¸ªText Encoderï¼ˆSDXLï¼‰
                if hasattr(pipe, 'text_encoder_2') and pipe.text_encoder_2:
                    text_encoder_2 = pipe.text_encoder_2
                    for attr in adapter_attrs:
                        if hasattr(text_encoder_2, attr):
                            try:
                                attr_obj = getattr(text_encoder_2, attr)
                                if hasattr(attr_obj, 'clear'):
                                    attr_obj.clear()
                                elif isinstance(attr_obj, dict):
                                    attr_obj.clear()
                                elif isinstance(attr_obj, list):
                                    attr_obj.clear()
                                print(f"ğŸ”§ æ¸…ç†TextEncoder2.{attr}")
                            except Exception as attr_error:
                                print(f"âš ï¸  æ¸…ç†TextEncoder2.{attr}æ—¶å‡ºé”™: {attr_error}")
                
        except Exception as pipeline_error:
            print(f"âš ï¸  æ¸…ç†{pipeline_name}ç®¡é“æ—¶å‡ºé”™: {pipeline_error}")
    
    # ç¬¬3å±‚ï¼šå¼ºåˆ¶æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")
    
    print("âœ… LoRAé€‚é…å™¨å®Œå…¨æ¸…ç†å®Œæˆ")

def compress_prompt_to_77_tokens(prompt: str, max_tokens: int = 75) -> str:
    """
    æ™ºèƒ½å‹ç¼©promptåˆ°æŒ‡å®štokenæ•°é‡ä»¥å†…
    ä¿ç•™æœ€é‡è¦çš„å…³é”®è¯å’Œæè¿°
    """
    import re
    
    # è®¡ç®—å½“å‰tokenæ•°é‡
    token_pattern = r'\w+|[^\w\s]'
    current_tokens = len(re.findall(token_pattern, prompt.lower()))
    
    if current_tokens <= max_tokens:
        return prompt
    
    print(f"ğŸ”§ å‹ç¼©prompt: {current_tokens} tokens -> {max_tokens} tokens")
    
    # å®šä¹‰é‡è¦æ€§æƒé‡
    priority_keywords = {
        # è´¨é‡æ ‡ç­¾ - æœ€é«˜ä¼˜å…ˆçº§
        'quality': ['masterpiece', 'best quality', 'amazing quality', 'high quality', 'ultra quality'],
        # ä¸»ä½“æè¿° - é«˜ä¼˜å…ˆçº§  
        'subject': ['man', 'boy', 'male', 'muscular', 'handsome', 'lean', 'naked', 'nude'],
        # èº«ä½“éƒ¨ä½ - ä¸­é«˜ä¼˜å…ˆçº§
        'anatomy': ['torso', 'chest', 'abs', 'penis', 'erect', 'flaccid', 'body'],
        # åŠ¨ä½œå§¿æ€ - ä¸­ä¼˜å…ˆçº§
        'pose': ['reclining', 'lying', 'sitting', 'standing', 'pose', 'position'],
        # ç¯å¢ƒé“å…· - ä¸­ä¼˜å…ˆçº§
        'environment': ['bed', 'sheets', 'satin', 'luxurious', 'room', 'background'],
        # å…‰å½±æ•ˆæœ - ä½ä¼˜å…ˆçº§
        'lighting': ['lighting', 'illuminated', 'soft', 'moody', 'warm', 'cinematic'],
        # æƒ…æ„Ÿè¡¨è¾¾ - ä½ä¼˜å…ˆçº§
        'emotion': ['serene', 'intense', 'confident', 'contemplation', 'allure']
    }
    
    # ğŸš¨ ä¿®å¤ï¼šä½¿ç”¨setæ¥è·Ÿè¸ªå·²æ·»åŠ çš„è¯ï¼Œé¿å…é‡å¤
    words = prompt.split()
    used_words = set()  # è·Ÿè¸ªå·²ä½¿ç”¨çš„è¯
    compressed_parts = []
    remaining_tokens = max_tokens
    
    # æŒ‰ä¼˜å…ˆçº§å¤„ç†
    priority_order = ['quality', 'subject', 'anatomy', 'pose', 'environment', 'lighting', 'emotion']
    
    for category in priority_order:
        if remaining_tokens <= 5:  # é¢„ç•™ä¸€äº›ç©ºé—´
            break
            
        category_keywords = priority_keywords[category]
        
        # æ‰¾åˆ°å±äºè¿™ä¸ªç±»åˆ«çš„è¯
        for word in words:
            if remaining_tokens <= 0:
                break
                
            word_clean = word.lower().strip('.,!?;:')
            
            # æ£€æŸ¥æ˜¯å¦å±äºå½“å‰ç±»åˆ« ä¸” æ²¡æœ‰è¢«ä½¿ç”¨è¿‡
            if word_clean not in used_words and any(keyword in word_clean for keyword in category_keywords):
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if word_tokens <= remaining_tokens:
                    compressed_parts.append(word)
                    used_words.add(word_clean)
                    remaining_tokens -= word_tokens
    
    # ğŸš¨ ä¿®å¤ï¼šå¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œæ·»åŠ å…¶ä»–é‡è¦ä½†æœªåˆ†ç±»çš„è¯ï¼ˆé¿å…é‡å¤ï¼‰
    if remaining_tokens > 0:
        for word in words:
            if remaining_tokens <= 0:
                break
                
            word_clean = word.lower().strip('.,!?;:')
            if word_clean not in used_words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if word_tokens <= remaining_tokens:
                    compressed_parts.append(word)
                    used_words.add(word_clean)
                    remaining_tokens -= word_tokens
    
    compressed_prompt = ' '.join(compressed_parts)
    final_tokens = len(re.findall(token_pattern, compressed_prompt.lower()))
    
    print(f"âœ… å‹ç¼©å®Œæˆ: '{compressed_prompt}' ({final_tokens} tokens)")
    return compressed_prompt
