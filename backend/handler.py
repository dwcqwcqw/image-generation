import runpod
import torch
from diffusers import FluxPipeline, FluxImg2ImgPipeline
from PIL import Image
import base64
import io
import json
import os
import uuid
from datetime import datetime
import boto3
from botocore.config import Config
import sys
import traceback

# å¯¼å…¥compelç”¨äºå¤„ç†é•¿æç¤ºè¯
try:
    from compel import Compel
    COMPEL_AVAILABLE = True
    print("âœ“ Compel library loaded for long prompt support")
except ImportError:
    COMPEL_AVAILABLE = False
    print("âš ï¸  Compel library not available - long prompt support limited")

# å…¼å®¹æ€§ä¿®å¤ï¼šä¸ºæ—§ç‰ˆæœ¬PyTorchæ·»åŠ get_default_deviceå‡½æ•°
if not hasattr(torch, 'get_default_device'):
    def get_default_device():
        """Fallback implementation for torch.get_default_device()"""
        if torch.cuda.is_available():
            return torch.device('cuda')
        else:
            return torch.device('cpu')
    
    # å°†å‡½æ•°æ·»åŠ åˆ°torchæ¨¡å—ä¸­
    torch.get_default_device = get_default_device
    print("âœ“ Added fallback torch.get_default_device() function")

# æ·»åŠ å¯åŠ¨æ—¥å¿—
print("=== Starting AI Image Generation Backend ===")
print(f"Python version: {sys.version}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

# éªŒè¯å…³é”®ç¯å¢ƒå˜é‡
required_env_vars = [
    "CLOUDFLARE_R2_ACCESS_KEY",
    "CLOUDFLARE_R2_SECRET_KEY", 
    "CLOUDFLARE_R2_BUCKET",
    "CLOUDFLARE_R2_ENDPOINT"
]

missing_vars = []
for var in required_env_vars:
    if not os.getenv(var):
        missing_vars.append(var)

if missing_vars:
    print(f"WARNING: Missing environment variables: {missing_vars}")
    print("Container will start but R2 upload may fail")

# ç¯å¢ƒå˜é‡
CLOUDFLARE_R2_ACCESS_KEY = os.getenv("CLOUDFLARE_R2_ACCESS_KEY")
CLOUDFLARE_R2_SECRET_KEY = os.getenv("CLOUDFLARE_R2_SECRET_KEY") 
CLOUDFLARE_R2_BUCKET = os.getenv("CLOUDFLARE_R2_BUCKET")
CLOUDFLARE_R2_ENDPOINT = os.getenv("CLOUDFLARE_R2_ENDPOINT")
CLOUDFLARE_R2_PUBLIC_DOMAIN = os.getenv("CLOUDFLARE_R2_PUBLIC_DOMAIN")  # å¯é€‰ï¼šè‡ªå®šä¹‰å…¬å…±åŸŸå

# æ¨¡å‹è·¯å¾„
FLUX_BASE_PATH = "/runpod-volume/flux_base"
FLUX_LORA_BASE_PATH = "/runpod-volume/lora"

# åŸºç¡€æ¨¡å‹é…ç½®
BASE_MODELS = {
    "realistic": {
        "name": "çœŸäººé£æ ¼",
        "base_path": "/runpod-volume/flux_base",
        "lora_path": "/runpod-volume/lora/flux_nsfw",
        "lora_id": "flux_nsfw"
    },
    "anime": {
        "name": "åŠ¨æ¼«é£æ ¼",
        "base_path": "/runpod-volume/cartoon/waiNSFWIllustrious_v130.safetensors",
        "lora_path": "/runpod-volume/cartoon/lora/Gayporn.safetensor",
        "lora_id": "gayporn"
    }
}

# æ”¯æŒçš„LoRAæ¨¡å‹åˆ—è¡¨ - æ›´æ–°ä¸ºæ”¯æŒä¸åŒåŸºç¡€æ¨¡å‹
AVAILABLE_LORAS = {
    "flux_nsfw": {
        "name": "FLUX NSFW",
        "path": "/runpod-volume/lora/flux_nsfw",
        "description": "NSFWçœŸäººå†…å®¹ç”Ÿæˆæ¨¡å‹",
        "default_weight": 1.0,
        "base_model": "realistic"
    },
    "gayporn": {
        "name": "Gayporn",
        "path": "/runpod-volume/cartoon/lora/Gayporn.safetensor",
        "description": "NSFWåŠ¨æ¼«å†…å®¹ç”Ÿæˆæ¨¡å‹",
        "default_weight": 1.0,
        "base_model": "anime"
    },
    # ä¿ç•™å…¶ä»–LoRAä»¥å¤‡æ‰©å±•ä½¿ç”¨
    "UltraRealPhoto": {
        "name": "Ultra Real Photo",
        "path": "/runpod-volume/lora/UltraRealPhoto.safetensors",
        "description": "Ultra realistic photo generation",
        "default_weight": 1.0,
        "base_model": "realistic"
    },
    "Chastity_Cage": {
        "name": "Chastity Cage",
        "path": "/runpod-volume/lora/Chastity_Cage.safetensors",
        "description": "Chastity device focused generation",
        "default_weight": 0.5,
        "base_model": "realistic"
    },
    "DynamicPenis": {
        "name": "Dynamic Penis",
        "path": "/runpod-volume/lora/DynamicPenis.safetensors",
        "description": "Dynamic male anatomy generation",
        "default_weight": 0.5,
        "base_model": "realistic"
    },
    "OnOff": {
        "name": "On Off",
        "path": "/runpod-volume/lora/OnOff.safetensors",
        "description": "Clothing on/off variations",
        "default_weight": 0.5,
        "base_model": "realistic"
    },
    "Puppy_mask": {
        "name": "Puppy Mask",
        "path": "/runpod-volume/lora/Puppy_mask.safetensors",
        "description": "Puppy mask and pet play content",
        "default_weight": 0.5,
        "base_model": "realistic"
    },
    "asianman": {
        "name": "Asian Man",
        "path": "/runpod-volume/lora/asianman.safetensors",
        "description": "Asian male character generation",
        "default_weight": 0.5,
        "base_model": "realistic"
    },
    "butt-and-feet": {
        "name": "Butt and Feet",
        "path": "/runpod-volume/lora/butt-and-feet.safetensors",
        "description": "Focus on lower body parts",
        "default_weight": 0.5,
        "base_model": "realistic"
    },
    "cumshots": {
        "name": "Cumshots",
        "path": "/runpod-volume/lora/cumshots.safetensors",
        "description": "Adult climax content generation",
        "default_weight": 0.5,
        "base_model": "realistic"
    }
}

# é»˜è®¤LoRAé…ç½® - æ ¹æ®åŸºç¡€æ¨¡å‹
DEFAULT_LORA_CONFIG = {
    "flux_nsfw": 1.0
}

# åˆå§‹åŒ– Cloudflare R2 å®¢æˆ·ç«¯
r2_client = None
if all([CLOUDFLARE_R2_ACCESS_KEY, CLOUDFLARE_R2_SECRET_KEY, CLOUDFLARE_R2_BUCKET, CLOUDFLARE_R2_ENDPOINT]):
    try:
        r2_client = boto3.client(
            's3',
            endpoint_url=CLOUDFLARE_R2_ENDPOINT,
            aws_access_key_id=CLOUDFLARE_R2_ACCESS_KEY,
            aws_secret_access_key=CLOUDFLARE_R2_SECRET_KEY,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        print("âœ“ R2 client initialized successfully")
    except Exception as e:
        print(f"âœ— Failed to initialize R2 client: {e}")
        r2_client = None
else:
    print("âœ— R2 configuration incomplete - R2 upload will be disabled")

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
txt2img_pipe = None
img2img_pipe = None
current_lora_config = DEFAULT_LORA_CONFIG.copy()
current_base_model = "realistic"  # å½“å‰åŠ è½½çš„åŸºç¡€æ¨¡å‹

# å…¨å±€å˜é‡å­˜å‚¨compelå¤„ç†å™¨
compel_proc = None
compel_proc_neg = None

def get_device():
    """è·å–è®¾å¤‡ï¼Œå…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬"""
    if torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def load_models():
    """åŠ è½½ FLUX æ¨¡å‹ - å¤§å¹…æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
    global txt2img_pipe, img2img_pipe, current_base_model
    
    print("ğŸš€ Loading FLUX models with optimizations...")
    start_time = datetime.now()
    
    # é»˜è®¤åŠ è½½çœŸäººé£æ ¼æ¨¡å‹
    base_model_type = "realistic"
    load_specific_model(base_model_type)

def load_specific_model(base_model_type: str):
    """åŠ è½½æŒ‡å®šçš„åŸºç¡€æ¨¡å‹"""
    global txt2img_pipe, img2img_pipe, current_base_model
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    model_config = BASE_MODELS[base_model_type]
    base_path = model_config["base_path"]
    
    print(f"ğŸ¨ Loading {model_config['name']} model from {base_path}")
    start_time = datetime.now()
    
    # CUDAå…¼å®¹æ€§æ£€æŸ¥å’Œä¿®å¤
    if torch.cuda.is_available():
        try:
            # æµ‹è¯•CUDAæ˜¯å¦å¯ç”¨
            test_tensor = torch.tensor([1.0]).cuda()
            print("âœ“ CUDA test successful")
            del test_tensor
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"âš ï¸  CUDA error detected: {e}")
            print("âš ï¸  Falling back to CPU mode")
            # å¼ºåˆ¶ä½¿ç”¨CPU
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    device = get_device()
    print(f"ğŸ“± Using device: {device}")
    
    # GPUå†…å­˜ä¼˜åŒ–
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"ğŸ’¾ GPU Memory before loading: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
    
    try:
        # ğŸ¯ ä¼˜åŒ–1: ä½¿ç”¨ä½å†…å­˜æ¨¡å¼å’Œä¼˜åŒ–é…ç½®
        print("âš¡ Loading text-to-image pipeline with optimizations...")
        
        # å†…å­˜ä¼˜åŒ–é…ç½®
        model_kwargs = {
            "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            "use_safetensors": True,
            "low_cpu_mem_usage": True,  # ä½CPUå†…å­˜ä½¿ç”¨
        }
        
        # å°è¯•ä½¿ç”¨è®¾å¤‡æ˜ å°„ä¼˜åŒ– - ä¿®å¤å…¼å®¹æ€§é—®é¢˜
        device_mapping_used = False  # æ ‡å¿—è·Ÿè¸ªæ˜¯å¦ä½¿ç”¨è®¾å¤‡æ˜ å°„
        if device == "cuda":
            try:
                # å…ˆå°è¯• "balanced" ç­–ç•¥
                model_kwargs_with_device_map = model_kwargs.copy()
                model_kwargs_with_device_map["device_map"] = "balanced"
                
                txt2img_pipe = FluxPipeline.from_pretrained(
                    base_path,
                    **model_kwargs_with_device_map
                )
                print("âœ… Device mapping enabled with 'balanced' strategy")
                device_mapping_used = True
                
            except Exception as device_map_error:
                print(f"âš ï¸  Device mapping failed ({device_map_error}), loading without device mapping")
                # å›é€€åˆ°ä¸ä½¿ç”¨è®¾å¤‡æ˜ å°„
                txt2img_pipe = FluxPipeline.from_pretrained(
                    base_path,
                    **model_kwargs
                )
                device_mapping_used = False
        else:
            # CPUæ¨¡å¼ç›´æ¥åŠ è½½
            txt2img_pipe = FluxPipeline.from_pretrained(
                base_path,
                **model_kwargs
            )
            device_mapping_used = False
        
        loading_time = (datetime.now() - start_time).total_seconds()
        print(f"â±ï¸  Base model loaded in {loading_time:.2f}s")
        
        # ğŸ¯ ä¼˜åŒ–2: å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
        try:
            txt2img_pipe.enable_attention_slicing()
            print("âœ… Attention slicing enabled")
        except Exception as e:
            print(f"âš ï¸  Attention slicing not available: {e}")
            
        try:
            txt2img_pipe.enable_model_cpu_offload()
            print("âœ… CPU offload enabled")
        except Exception as e:
            print(f"âš ï¸  CPU offload not available: {e}")
        
        # ğŸ¯ ä¼˜åŒ–3: VAEå†…å­˜ä¼˜åŒ–
        try:
            txt2img_pipe.enable_vae_slicing()
            txt2img_pipe.enable_vae_tiling()
            print("âœ… VAE optimizations enabled")
        except Exception as e:
            print(f"âš ï¸  VAE optimizations not available: {e}")
        
        # åŠ è½½å¯¹åº”çš„é»˜è®¤ LoRA æƒé‡
        lora_start_time = datetime.now()
        default_lora_path = model_config["lora_path"]
        if os.path.exists(default_lora_path):
            print(f"ğŸ¨ Loading default LoRA for {model_config['name']}: {default_lora_path}")
            try:
                txt2img_pipe.load_lora_weights(default_lora_path)
                lora_time = (datetime.now() - lora_start_time).total_seconds()
                print(f"âœ… LoRA loaded in {lora_time:.2f}s")
                
                # æ›´æ–°å½“å‰LoRAé…ç½®
                global current_lora_config
                current_lora_config = {model_config["lora_id"]: 1.0}
                
            except ValueError as e:
                if "PEFT backend is required" in str(e):
                    print("âŒ ERROR: PEFT backend is required for LoRA support")
                    print("   Please install: pip install peft>=0.8.0")
                    raise RuntimeError("PEFT library is required but not installed")
                else:
                    print(f"âŒ ERROR: Failed to load LoRA weights: {e}")
                    raise RuntimeError(f"Failed to load required LoRA model: {e}")
            except Exception as e:
                print(f"âŒ ERROR: Failed to load LoRA weights: {e}")
                raise RuntimeError(f"Failed to load required LoRA model: {e}")
        else:
            print(f"âŒ ERROR: Default LoRA weights not found at {default_lora_path}")
            raise RuntimeError(f"Required LoRA model not found for {model_config['name']}")
        
        # ğŸ¯ ä¼˜åŒ–4: æ™ºèƒ½è®¾å¤‡ç§»åŠ¨ï¼ˆä»…åœ¨æœªä½¿ç”¨è®¾å¤‡æ˜ å°„æ—¶ï¼‰
        if not device_mapping_used:
            device_start_time = datetime.now()
            print("ğŸšš Moving pipeline to device...")
            
            if device == "cuda":
                # æ¸è¿›å¼ç§»åŠ¨åˆ°GPUï¼Œé¿å…å†…å­˜å³°å€¼
                txt2img_pipe = txt2img_pipe.to(device)
            else:
                txt2img_pipe = txt2img_pipe.to(device)
            
            device_time = (datetime.now() - device_start_time).total_seconds()
            print(f"âœ… Device transfer completed in {device_time:.2f}s")
        else:
            print("âš¡ Skipping manual device transfer (using device mapping)")
        
        # ğŸ¯ ä¼˜åŒ–5: å›¾ç”Ÿå›¾æ¨¡å‹ä½¿ç”¨å…±äº«ç»„ä»¶ (é›¶æ‹·è´)
        print("ğŸ”— Creating image-to-image pipeline (sharing components)...")
        img_start_time = datetime.now()
        
        img2img_pipe = FluxImg2ImgPipeline(
            vae=txt2img_pipe.vae,
            text_encoder=txt2img_pipe.text_encoder,
            text_encoder_2=txt2img_pipe.text_encoder_2,
            tokenizer=txt2img_pipe.tokenizer,
            tokenizer_2=txt2img_pipe.tokenizer_2,
            transformer=txt2img_pipe.transformer,
            scheduler=txt2img_pipe.scheduler,
        )
        
        # ä¸éœ€è¦å†æ¬¡ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œå› ä¸ºå…±äº«ç»„ä»¶å·²ç»åœ¨è®¾å¤‡ä¸Š
        img_time = (datetime.now() - img_start_time).total_seconds()
        print(f"âœ… Image-to-image pipeline created in {img_time:.2f}s")
        
        # æ›´æ–°å½“å‰åŸºç¡€æ¨¡å‹
        current_base_model = base_model_type
        
        # æœ€ç»ˆå†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            print(f"ğŸ’¾ GPU Memory after loading: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            print(f"ğŸ’¾ GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f}GB")
        
        total_time = (datetime.now() - start_time).total_seconds()
        print(f"ğŸ‰ {model_config['name']} model loaded successfully in {total_time:.2f}s!")
        
        # ğŸ¯ ä¼˜åŒ–6: é¢„çƒ­æ¨ç† (å¯é€‰)
        try:
            print("ğŸ”¥ Warming up models with test inference...")
            warmup_start = datetime.now()
            with torch.no_grad():
                # å°å°ºå¯¸é¢„çƒ­æ¨ç†
                test_result = txt2img_pipe(
                    prompt="test",
                    width=512,
                    height=512,
                    num_inference_steps=1,
                    guidance_scale=1.0
                )
            warmup_time = (datetime.now() - warmup_start).total_seconds()
            print(f"âœ… Model warmup completed in {warmup_time:.2f}s")
        except Exception as e:
            print(f"âš ï¸  Model warmup failed (ä¸å½±å“æ­£å¸¸ä½¿ç”¨): {e}")
        
        print(f"ğŸš€ {model_config['name']} system ready for image generation!")
        
        # compel_proc and advanced long prompt support is handled by pipeline.encode_prompt directly
        # No need for separate Compel instances here for basic embedding generation

    except Exception as e:
        print(f"âŒ Error loading {model_config['name']} model: {str(e)}")
        traceback.print_exc()
        raise e

def upload_to_r2(image_data: bytes, filename: str) -> str:
    """ä¸Šä¼ å›¾ç‰‡åˆ° Cloudflare R2"""
    try:
        # æ£€æŸ¥R2å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if r2_client is None:
            raise RuntimeError("R2 client not available - check environment variables")
            
        # ç¡®ä¿ image_data æ˜¯ bytes ç±»å‹
        if not isinstance(image_data, bytes):
            raise TypeError(f"Expected bytes, got {type(image_data)}")
            
        # éªŒè¯æ–‡ä»¶å¤§å°
        if len(image_data) == 0:
            raise ValueError("Image data is empty")
            
        if len(image_data) > 50 * 1024 * 1024:  # 50MB limit
            raise ValueError(f"Image too large: {len(image_data)} bytes")
            
        print(f"Uploading {len(image_data)} bytes to R2 as {filename}")
        
        r2_client.put_object(
            Bucket=CLOUDFLARE_R2_BUCKET,
            Key=filename,
            Body=image_data,
            ContentType='image/png',
            ACL='public-read'
        )
        
        # æ„å»ºæ­£ç¡®çš„å…¬å…± URL æ ¼å¼ - Cloudflare R2 public URL format
        # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰å…¬å…±åŸŸåï¼ˆæ¨èï¼Œå¯é¿å…CORSé—®é¢˜ï¼‰
        if CLOUDFLARE_R2_PUBLIC_DOMAIN:
            public_url = f"{CLOUDFLARE_R2_PUBLIC_DOMAIN.rstrip('/')}/{filename}"
            print(f"âœ“ Successfully uploaded to (custom domain): {public_url}")
        else:
            # å›é€€åˆ°æ ‡å‡†R2æ ¼å¼
            # æ­£ç¡®æ ¼å¼: https://{bucket}.{account_id}.r2.cloudflarestorage.com/{filename}
            # ä»endpoint URLä¸­æå–account ID
            account_id = CLOUDFLARE_R2_ENDPOINT.split('//')[1].split('.')[0]
            public_url = f"https://{CLOUDFLARE_R2_BUCKET}.{account_id}.r2.cloudflarestorage.com/{filename}"
            print(f"âœ“ Successfully uploaded to (standard R2): {public_url}")
        
        return public_url
        
    except Exception as e:
        print(f"âœ— Error uploading to R2: {str(e)}")
        print(f"Image data type: {type(image_data)}, size: {len(image_data) if hasattr(image_data, '__len__') else 'unknown'}")
        
        # å¯¹äºæ¼”ç¤ºç›®çš„ï¼Œè¿”å›ä¸€ä¸ªå ä½ç¬¦URLè€Œä¸æ˜¯å¤±è´¥
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æŠ›å‡ºå¼‚å¸¸
        placeholder_url = f"https://via.placeholder.com/512x512/cccccc/666666?text=Upload+Failed"
        print(f"Returning placeholder URL: {placeholder_url}")
        return placeholder_url

def image_to_bytes(image: Image.Image) -> bytes:
    """å°† PIL Image è½¬æ¢ä¸ºå­—èŠ‚"""
    try:
        buffer = io.BytesIO()
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image.save(buffer, format='PNG', quality=95, optimize=True)
        buffer.seek(0)  # é‡ç½®bufferä½ç½®
        return buffer.getvalue()
    except Exception as e:
        print(f"Error converting image to bytes: {str(e)}")
        raise e

def base64_to_image(base64_str: str) -> Image.Image:
    """å°† base64 å­—ç¬¦ä¸²è½¬æ¢ä¸º PIL Image"""
    if base64_str.startswith('data:image'):
        base64_str = base64_str.split(',')[1]
    
    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image.convert('RGB')

def text_to_image(params: dict) -> list:
    """æ–‡ç”Ÿå›¾ç”Ÿæˆ - ä¼˜åŒ–ç‰ˆæœ¬ with long prompt support"""
    global txt2img_pipe, current_base_model
    
    if txt2img_pipe is None:
        raise ValueError("Text-to-image model not loaded")
    
    # æå–å‚æ•°
    prompt = params.get('prompt', '')
    negative_prompt = params.get('negativePrompt', '')
    width = params.get('width', 512)
    height = params.get('height', 512)
    steps = params.get('steps', 20)
    cfg_scale = params.get('cfgScale', 7.0)
    seed = params.get('seed', -1)
    num_images = params.get('numImages', 1)
    base_model = params.get('baseModel', 'realistic')
    lora_config = params.get('lora_config', {})
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åŸºç¡€æ¨¡å‹
    if base_model != current_base_model:
        print(f"Switching base model for generation: {current_base_model} -> {base_model}")
        switch_base_model(base_model)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
    if lora_config and lora_config != current_lora_config:
        print(f"Updating LoRA config for generation: {lora_config}")
        load_multiple_loras(lora_config)
    
    # FLUXæ¨¡å‹åŸç”Ÿæ”¯æŒé•¿æç¤ºè¯ï¼Œä¸éœ€è¦å¤æ‚çš„embeddingå¤„ç†
    generation_kwargs = {
        "width": width,
        "height": height,
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating prompt embeddings using pipeline.encode_prompt()...")
    try:
        device = get_device()

        # Encode positive prompt
        prompt_embeds_obj = txt2img_pipe.encode_prompt(
            prompt=prompt,
            device=device,
            num_images_per_prompt=1, # Encode for a single image initially
            do_classifier_free_guidance=cfg_scale > 1.0, # Still relevant for how embeds might be used/conditioned
        )

        # Encode negative prompt
        # Note: Some encode_prompt versions might not need do_classifier_free_guidance for negative prompts
        # or expect it to be False. For safety, keeping it consistent or specific to positive.
        negative_prompt_embeds_obj = txt2img_pipe.encode_prompt(
            prompt=negative_prompt if negative_prompt else "", # Pass empty string if no negative prompt
            device=device,
            num_images_per_prompt=1,
            do_classifier_free_guidance=cfg_scale > 1.0, # Or False, depending on pipeline requirements
        )

        # Unpack positive embeddings
        if hasattr(prompt_embeds_obj, 'prompt_embeds'):
            generation_kwargs["prompt_embeds"] = prompt_embeds_obj.prompt_embeds
            if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds'): # FLUX uses pooled
                 generation_kwargs["pooled_prompt_embeds"] = prompt_embeds_obj.pooled_prompt_embeds
        elif isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) >= 1: # Basic embed, possibly pooled as second element
            generation_kwargs["prompt_embeds"] = prompt_embeds_obj[0]
            if len(prompt_embeds_obj) > 1 and prompt_embeds_obj[1] is not None: # Check for pooled
                generation_kwargs["pooled_prompt_embeds"] = prompt_embeds_obj[1]
        elif isinstance(prompt_embeds_obj, dict):
            generation_kwargs["prompt_embeds"] = prompt_embeds_obj.get("prompt_embeds")
            if "pooled_prompt_embeds" in prompt_embeds_obj:
                generation_kwargs["pooled_prompt_embeds"] = prompt_embeds_obj.get("pooled_prompt_embeds")
        else:
            raise ValueError("Unsupported output format from pipeline.encode_prompt() for positive prompt")

        # Unpack negative embeddings
        if hasattr(negative_prompt_embeds_obj, 'prompt_embeds'):
            generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_obj.prompt_embeds
            if hasattr(negative_prompt_embeds_obj, 'pooled_prompt_embeds'): # FLUX uses pooled
                 generation_kwargs["negative_pooled_prompt_embeds"] = negative_prompt_embeds_obj.pooled_prompt_embeds
        elif isinstance(negative_prompt_embeds_obj, tuple) and len(negative_prompt_embeds_obj) >=1:
            generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_obj[0]
            if len(negative_prompt_embeds_obj) > 1 and negative_prompt_embeds_obj[1] is not None:
                generation_kwargs["negative_pooled_prompt_embeds"] = negative_prompt_embeds_obj[1]
        elif isinstance(negative_prompt_embeds_obj, dict):
            generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_obj.get("prompt_embeds")
            if "pooled_prompt_embeds" in negative_prompt_embeds_obj: # Note: key might be just 'pooled_prompt_embeds' from encode
                generation_kwargs["negative_pooled_prompt_embeds"] = negative_prompt_embeds_obj.get("pooled_prompt_embeds")
        else:
            raise ValueError("Unsupported output format from pipeline.encode_prompt() for negative prompt")

        print("âœ… Embeddings successfully generated and assigned.")

    except Exception as e:
        print(f"âš ï¸ pipeline.encode_prompt() failed: {e}. Traceback follows.")
        traceback.print_exc()
        print("Falling back to using raw prompts (this will likely cause the original error with FluxPipeline).")
        generation_kwargs["prompt"] = prompt
        generation_kwargs["negative_prompt"] = negative_prompt

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    results = []
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡ï¼Œè€Œä¸æ˜¯å¾ªç¯
    if num_images > 1 and num_images <= 4:  # é™åˆ¶æ‰¹é‡å¤§å°é¿å…å†…å­˜é—®é¢˜
        try:
            print(f"Batch generating {num_images} images...")
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                batch_kwargs = generation_kwargs.copy()
                batch_kwargs["num_images_per_prompt"] = num_images
                result = txt2img_pipe(**batch_kwargs)
            
            # å¤„ç†æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡
            for i, image in enumerate(result.images):
                try:
                    # ä¸Šä¼ åˆ° R2
                    image_id = str(uuid.uuid4())
                    filename = f"generated/{image_id}.png"
                    image_bytes = image_to_bytes(image)
                    image_url = upload_to_r2(image_bytes, filename)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    image_data = {
                        'id': image_id,
                        'url': image_url,
                        'prompt': prompt,
                        'negativePrompt': negative_prompt,
                        'seed': seed + i,  # æ¯å¼ å›¾ç‰‡ä¸åŒçš„ç§å­
                        'width': width,
                        'height': height,
                        'steps': steps,
                        'cfgScale': cfg_scale,
                        'baseModel': base_model,
                        'createdAt': datetime.utcnow().isoformat(),
                        'type': 'text-to-image'
                    }
                    
                    results.append(image_data)
                    
                except Exception as e:
                    print(f"Error processing batch image {i+1}: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"Batch generation failed, falling back to individual generation: {str(e)}")
            # å¦‚æœæ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ ç”Ÿæˆ
            num_images = min(num_images, 1)
    
    # å•å¼ ç”Ÿæˆæˆ–æ‰¹é‡ç”Ÿæˆå¤±è´¥çš„å›é€€
    if len(results) == 0:
        for i in range(num_images):
            try:
                print(f"Generating image {i+1}/{num_images}...")
                # ç”Ÿæˆå›¾åƒ
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    # ä¼˜åŒ–ï¼šæ¸…ç†GPUç¼“å­˜
                    if torch.cuda.is_available() and i > 0:
                        torch.cuda.empty_cache()
                        
                    single_kwargs = generation_kwargs.copy()
                    single_kwargs["num_images_per_prompt"] = 1
                    result = txt2img_pipe(**single_kwargs)
                
                image = result.images[0]
                
                # ä¸Šä¼ åˆ° R2
                image_id = str(uuid.uuid4())
                filename = f"generated/{image_id}.png"
                image_bytes = image_to_bytes(image)
                image_url = upload_to_r2(image_bytes, filename)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                image_data = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed,
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'baseModel': base_model,
                    'createdAt': datetime.utcnow().isoformat(),
                    'type': 'text-to-image'
                }
                
                results.append(image_data)
                
                # ä¸ºä¸‹ä¸€å¼ å›¾ç‰‡æ›´æ–°ç§å­
                if i < num_images - 1:
                    seed += 1
                    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
                    generation_kwargs["generator"] = generator
                    
            except Exception as e:
                print(f"Error generating image {i+1}: {str(e)}")
                continue
    
    # ä¼˜åŒ–ï¼šæœ€ç»ˆæ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results

def image_to_image(params: dict) -> list:
    """å›¾ç”Ÿå›¾ç”Ÿæˆ - ä¼˜åŒ–ç‰ˆæœ¬"""
    global img2img_pipe, current_base_model
    
    if img2img_pipe is None:
        raise ValueError("Image-to-image model not loaded")
    
    # æå–å‚æ•°
    prompt = params.get('prompt', '')
    negative_prompt = params.get('negativePrompt', '')
    image_data = params.get('image', '')
    width = params.get('width', 512)
    height = params.get('height', 512)
    steps = params.get('steps', 20)
    cfg_scale = params.get('cfgScale', 7.0)
    seed = params.get('seed', -1)
    num_images = params.get('numImages', 1)
    denoising_strength = params.get('denoisingStrength', 0.7)
    base_model = params.get('baseModel', 'realistic')
    lora_config = params.get('lora_config', {})
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åŸºç¡€æ¨¡å‹
    if base_model != current_base_model:
        print(f"Switching base model for generation: {current_base_model} -> {base_model}")
        switch_base_model(base_model)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
    if lora_config and lora_config != current_lora_config:
        print(f"Updating LoRA config for generation: {lora_config}")
        load_multiple_loras(lora_config)
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    if isinstance(image_data, str):
        source_image = base64_to_image(image_data)
    else:
        raise ValueError("Invalid image data format")
    
    # è°ƒæ•´å›¾åƒå°ºå¯¸
    source_image = source_image.resize((width, height), Image.Resampling.LANCZOS)
    
    # ğŸ¯ é•¿æç¤ºè¯æ”¯æŒ - å…¨æ–°æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨FLUXåŸç”Ÿå¤„ç† (é€šè¿‡ pipeline.encode_prompt)
    print(f"ğŸ“ Processing prompt for Img2Img: {len(prompt)} characters")
    
    generation_kwargs = {
        "image": source_image,
        # "prompt": prompt, # Replaced by embeds
        # "negative_prompt": negative_prompt, # Replaced by embeds
        "width": width, 
        "height": height,
        "strength": denoising_strength, # For Img2Img
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating prompt embeddings for Img2Img using pipeline.encode_prompt()...")
    try:
        device = get_device()

        # Encode positive prompt
        prompt_embeds_obj = img2img_pipe.encode_prompt(
            prompt=prompt,
            device=device,
            num_images_per_prompt=1,
            do_classifier_free_guidance=cfg_scale > 1.0,
        )

        # Encode negative prompt
        negative_prompt_embeds_obj = img2img_pipe.encode_prompt(
            prompt=negative_prompt if negative_prompt else "",
            device=device,
            num_images_per_prompt=1,
            do_classifier_free_guidance=cfg_scale > 1.0, # Or False
        )

        # Unpack positive embeddings
        if hasattr(prompt_embeds_obj, 'prompt_embeds'):
            generation_kwargs["prompt_embeds"] = prompt_embeds_obj.prompt_embeds
            if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds'):
                 generation_kwargs["pooled_prompt_embeds"] = prompt_embeds_obj.pooled_prompt_embeds
        elif isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) >= 1:
            generation_kwargs["prompt_embeds"] = prompt_embeds_obj[0]
            if len(prompt_embeds_obj) > 1 and prompt_embeds_obj[1] is not None:
                generation_kwargs["pooled_prompt_embeds"] = prompt_embeds_obj[1]
        elif isinstance(prompt_embeds_obj, dict):
            generation_kwargs["prompt_embeds"] = prompt_embeds_obj.get("prompt_embeds")
            if "pooled_prompt_embeds" in prompt_embeds_obj:
                generation_kwargs["pooled_prompt_embeds"] = prompt_embeds_obj.get("pooled_prompt_embeds")
        else:
            raise ValueError("Unsupported output format from Img2Img pipeline.encode_prompt() for positive prompt")

        # Unpack negative embeddings
        if hasattr(negative_prompt_embeds_obj, 'prompt_embeds'):
            generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_obj.prompt_embeds
            if hasattr(negative_prompt_embeds_obj, 'pooled_prompt_embeds'):
                 generation_kwargs["negative_pooled_prompt_embeds"] = negative_prompt_embeds_obj.pooled_prompt_embeds
        elif isinstance(negative_prompt_embeds_obj, tuple) and len(negative_prompt_embeds_obj) >= 1:
            generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_obj[0]
            if len(negative_prompt_embeds_obj) > 1 and negative_prompt_embeds_obj[1] is not None:
                generation_kwargs["negative_pooled_prompt_embeds"] = negative_prompt_embeds_obj[1]
        elif isinstance(negative_prompt_embeds_obj, dict):
            generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_obj.get("prompt_embeds")
            if "pooled_prompt_embeds" in negative_prompt_embeds_obj:
                generation_kwargs["negative_pooled_prompt_embeds"] = negative_prompt_embeds_obj.get("pooled_prompt_embeds")
        else:
            raise ValueError("Unsupported output format from Img2Img pipeline.encode_prompt() for negative prompt")

        print("âœ… Img2Img Embeddings successfully generated and assigned.")

    except Exception as e:
        print(f"âš ï¸ Img2Img pipeline.encode_prompt() failed: {e}. Traceback follows.")
        traceback.print_exc()
        print("Falling back to using raw prompts for Img2Img (this will likely cause the original error).")
        generation_kwargs["prompt"] = prompt
        generation_kwargs["negative_prompt"] = negative_prompt

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    results = []
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡
    if num_images > 1 and num_images <= 4:  # é™åˆ¶æ‰¹é‡å¤§å°é¿å…å†…å­˜é—®é¢˜
        try:
            print(f"Batch generating {num_images} images for img2img...")
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                result = img2img_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=source_image,
                    strength=denoising_strength,
                    num_inference_steps=steps,
                    guidance_scale=cfg_scale,
                    generator=generator,
                    num_images_per_prompt=num_images
                )
            
            # å¤„ç†æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡
            for i, image in enumerate(result.images):
                try:
                    # ä¸Šä¼ åˆ° R2
                    image_id = str(uuid.uuid4())
                    filename = f"generated/{image_id}.png"
                    image_bytes = image_to_bytes(image)
                    image_url = upload_to_r2(image_bytes, filename)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    image_data = {
                        'id': image_id,
                        'url': image_url,
                        'prompt': prompt,
                        'negativePrompt': negative_prompt,
                        'seed': seed + i,
                        'width': width,
                        'height': height,
                        'steps': steps,
                        'cfgScale': cfg_scale,
                        'baseModel': base_model,
                        'createdAt': datetime.utcnow().isoformat(),
                        'type': 'image-to-image',
                        'denoisingStrength': denoising_strength
                    }
                    
                    results.append(image_data)
                    
                except Exception as e:
                    print(f"Error processing batch img2img image {i+1}: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"Batch img2img generation failed, falling back to individual generation: {str(e)}")
            # å¦‚æœæ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ ç”Ÿæˆ
            num_images = min(num_images, 1)
    
    # å•å¼ ç”Ÿæˆæˆ–æ‰¹é‡ç”Ÿæˆå¤±è´¥çš„å›é€€
    if len(results) == 0:
        for i in range(num_images):
            try:
                print(f"Generating img2img image {i+1}/{num_images}...")
                # ç”Ÿæˆå›¾åƒ
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    # ä¼˜åŒ–ï¼šæ¸…ç†GPUç¼“å­˜
                    if torch.cuda.is_available() and i > 0:
                        torch.cuda.empty_cache()
                        
                    result = img2img_pipe(
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        image=source_image,
                        strength=denoising_strength,
                        num_inference_steps=steps,
                        guidance_scale=cfg_scale,
                        generator=generator,
                        num_images_per_prompt=1
                    )
                
                image = result.images[0]
                
                # ä¸Šä¼ åˆ° R2
                image_id = str(uuid.uuid4())
                filename = f"generated/{image_id}.png"
                image_bytes = image_to_bytes(image)
                image_url = upload_to_r2(image_bytes, filename)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                image_data = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed,
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'baseModel': base_model,
                    'createdAt': datetime.utcnow().isoformat(),
                    'type': 'image-to-image',
                    'denoisingStrength': denoising_strength
                }
                
                results.append(image_data)
                
                # ä¸ºä¸‹ä¸€å¼ å›¾ç‰‡æ›´æ–°ç§å­
                if i < num_images - 1:
                    seed += 1
                    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
                    
            except Exception as e:
                print(f"Error generating img2img image {i+1}: {str(e)}")
                continue
    
    # ä¼˜åŒ–ï¼šæœ€ç»ˆæ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results

def get_available_loras() -> dict:
    """è·å–å¯ç”¨çš„LoRAæ¨¡å‹åˆ—è¡¨"""
    available = {}
    for lora_id, lora_info in AVAILABLE_LORAS.items():
        if os.path.exists(lora_info["path"]):
            available[lora_id] = {
                "name": lora_info["name"],
                "description": lora_info["description"],
                "default_weight": lora_info["default_weight"],
                "current_weight": current_lora_config.get(lora_id, 0.0)
            }
    return available

def load_multiple_loras(lora_config: dict) -> bool:
    """åŠ è½½å¤šä¸ªLoRAæ¨¡å‹ï¼Œæ¯ä¸ªéƒ½æœ‰è‡ªå·±çš„æƒé‡"""
    global txt2img_pipe, current_lora_config
    
    if not lora_config:
        print("No LoRA configuration provided")
        return False
    
    try:
        print(f"Loading multiple LoRAs with config: {lora_config}")
        
        # å…ˆå¸è½½æ‰€æœ‰ç°æœ‰çš„LoRA
        txt2img_pipe.unload_lora_weights()
        
        # å‡†å¤‡LoRAæƒé‡å’Œé€‚é…å™¨åç§°
        adapter_names = []
        adapter_weights = []
        
        for lora_id, weight in lora_config.items():
            if weight > 0 and lora_id in AVAILABLE_LORAS:
                lora_path = AVAILABLE_LORAS[lora_id]["path"]
                if os.path.exists(lora_path):
                    # åŠ è½½LoRAé€‚é…å™¨
                    adapter_name = f"lora_{lora_id}"
                    txt2img_pipe.load_lora_weights(lora_path, adapter_name=adapter_name)
                    adapter_names.append(adapter_name)
                    adapter_weights.append(weight)
                    print(f"âœ… Loaded LoRA {AVAILABLE_LORAS[lora_id]['name']} with weight {weight}")
                else:
                    print(f"âš ï¸ LoRA path not found: {lora_path}")
        
        if adapter_names:
            # è®¾ç½®æ··åˆæƒé‡
            txt2img_pipe.set_adapters(adapter_names, adapter_weights)
            current_lora_config = lora_config.copy()
            print(f"âœ… Successfully loaded {len(adapter_names)} LoRA adapters")
            return True
        else:
            print("âŒ No valid LoRA adapters could be loaded")
            return False
            
    except Exception as e:
        print(f"âŒ Error loading multiple LoRAs: {str(e)}")
        # å°è¯•æ¢å¤åˆ°é»˜è®¤é…ç½®
        try:
            txt2img_pipe.unload_lora_weights()
            default_lora_path = AVAILABLE_LORAS["flux_nsfw"]["path"]
            txt2img_pipe.load_lora_weights(default_lora_path)
            current_lora_config = {"flux_nsfw": 1.0}
            print("Recovered to default LoRA configuration")
        except Exception as recovery_error:
            print(f"Failed to recover to default LoRA: {recovery_error}")
        return False

def switch_lora(lora_id: str) -> bool:
    """åˆ‡æ¢LoRAæ¨¡å‹ - ä¼˜åŒ–ç‰ˆæœ¬"""
    global txt2img_pipe, img2img_pipe, current_lora_config
    
    if lora_id not in AVAILABLE_LORAS:
        raise ValueError(f"Unknown LoRA model: {lora_id}")
    
    lora_info = AVAILABLE_LORAS[lora_id]
    lora_path = lora_info["path"]
    
    if not os.path.exists(lora_path):
        raise ValueError(f"LoRA model not found: {lora_info['name']} at {lora_path}")
    
    # ä¼˜åŒ–ï¼šå¦‚æœå·²ç»æ˜¯å½“å‰LoRAï¼Œç›´æ¥è¿”å›ï¼Œé¿å…ä¸å¿…è¦çš„é‡æ–°åŠ è½½
    if lora_id == current_lora_config["flux_nsfw"]:
        print(f"LoRA {lora_info['name']} is already loaded - skipping switch")
        return True
    
    try:
        print(f"Switching LoRA from {AVAILABLE_LORAS[current_lora_config['flux_nsfw']]['name']} to {lora_info['name']}")
        
        # å¸è½½å½“å‰LoRA
        txt2img_pipe.unload_lora_weights()
        
        # åŠ è½½æ–°çš„LoRA
        txt2img_pipe.load_lora_weights(lora_path)
        
        # æ›´æ–°å½“å‰LoRA
        current_lora_config["flux_nsfw"] = lora_id
        
        print(f"Successfully switched to LoRA: {lora_info['name']}")
        return True
        
    except Exception as e:
        print(f"Failed to switch LoRA: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„LoRA
        try:
            previous_lora_path = AVAILABLE_LORAS[current_lora_config["flux_nsfw"]]["path"]
            txt2img_pipe.unload_lora_weights()
            txt2img_pipe.load_lora_weights(previous_lora_path)
            print(f"Recovered to previous LoRA: {AVAILABLE_LORAS[current_lora_config['flux_nsfw']]['name']}")
        except Exception as recovery_error:
            print(f"Failed to recover LoRA: {recovery_error}")
        raise RuntimeError(f"Failed to switch LoRA model: {str(e)}")

def switch_base_model(base_model_type: str) -> bool:
    """åˆ‡æ¢åŸºç¡€æ¨¡å‹"""
    global current_base_model
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    if current_base_model == base_model_type:
        print(f"Base model {BASE_MODELS[base_model_type]['name']} is already loaded")
        return True
    
    try:
        print(f"Switching base model from {BASE_MODELS[current_base_model]['name']} to {BASE_MODELS[base_model_type]['name']}")
        
        # é‡Šæ”¾å½“å‰æ¨¡å‹å†…å­˜
        global txt2img_pipe, img2img_pipe
        if txt2img_pipe is not None:
            del txt2img_pipe
        if img2img_pipe is not None:
            del img2img_pipe
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # åŠ è½½æ–°çš„åŸºç¡€æ¨¡å‹
        load_specific_model(base_model_type)
        
        print(f"Successfully switched to {BASE_MODELS[base_model_type]['name']}")
        return True
        
    except Exception as e:
        print(f"Failed to switch base model: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„æ¨¡å‹
        try:
            load_specific_model(current_base_model)
            print(f"Recovered to previous model: {BASE_MODELS[current_base_model]['name']}")
        except Exception as recovery_error:
            print(f"Failed to recover base model: {recovery_error}")
        raise RuntimeError(f"Failed to switch base model: {str(e)}")

def handler(job):
    """RunPod å¤„ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        job_input = job['input']
        task_type = job_input.get('task_type')
        
        if task_type == 'get-loras':
            # è·å–å¯ç”¨LoRAåˆ—è¡¨
            available_loras = get_available_loras()
            return {
                'success': True,
                'data': {
                    'loras': available_loras,
                    'current_config': current_lora_config
                }
            }
            
        elif task_type == 'switch-lora':
            # åˆ‡æ¢LoRAæ¨¡å‹ï¼ˆå•ä¸ªLoRAå…¼å®¹æ€§æ”¯æŒï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            # å…¼å®¹å•LoRAåˆ‡æ¢
            single_lora_config = {lora_id: 1.0}
            success = load_multiple_loras(single_lora_config)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
        
        elif task_type == 'load-loras':
            # åŠ è½½å¤šä¸ªLoRAæ¨¡å‹é…ç½®
            lora_config = job_input.get('lora_config', {})
            if not lora_config:
                return {
                    'success': False,
                    'error': 'lora_config is required'
                }
            
            success = load_multiple_loras(lora_config)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Loaded {len([k for k, v in lora_config.items() if v > 0])} LoRA models'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': 'Failed to load LoRA configuration'
                }
        
        elif task_type == 'text-to-image':
            # ä¼˜åŒ–ï¼šæ”¯æŒå¤šLoRAé…ç½®
            params = job_input.get('params', {})
            requested_lora_config = params.get('lora_config', current_lora_config)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
            if requested_lora_config != current_lora_config:
                print(f"Auto-loading LoRA config for generation: {requested_lora_config}")
                load_multiple_loras(requested_lora_config)
            
            results = text_to_image(params)
            return {
                'success': True,
                'data': results
            }
            
        elif task_type == 'image-to-image':
            # ä¼˜åŒ–ï¼šæ”¯æŒå¤šLoRAé…ç½®
            params = job_input.get('params', {})
            requested_lora_config = params.get('lora_config', current_lora_config)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
            if requested_lora_config != current_lora_config:
                print(f"Auto-loading LoRA config for generation: {requested_lora_config}")
                load_multiple_loras(requested_lora_config)
            
            results = image_to_image(params)
            return {
                'success': True,
                'data': results
            }
            
        elif task_type == 'switch-base-model':
            # åˆ‡æ¢åŸºç¡€æ¨¡å‹
            base_model_type = job_input.get('base_model_type')
            if not base_model_type:
                return {
                    'success': False,
                    'error': 'base_model_type is required'
                }
            
            success = switch_base_model(base_model_type)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_base_model': current_base_model
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {base_model_type}'
                }
        
        else:
            return {
                'success': False,
                'error': f'Unknown task type: {task_type}'
            }
            
    except Exception as e:
        print(f"Handler error: {str(e)}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }

# Note: The serverless worker will be started by start_debug.py
# This allows for better debugging and health checks before startup 