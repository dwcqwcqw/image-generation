import os
import base64
import io
import time
import traceback
import uuid
import sys  # æ·»åŠ ç¼ºå¤±çš„syså¯¼å…¥
import re  # æ·»åŠ regexæ¨¡å—ç”¨äºé•¿promptå¤„ç†
from datetime import datetime
from typing import Optional, Dict, List, Tuple

import torch
import numpy as np
from PIL import Image
import runpod

# AIå’Œå›¾åƒå¤„ç†åº“
from diffusers import (
    FluxPipeline, 
    FluxImg2ImgPipeline,  # <-- Add this import
    StableDiffusionPipeline, 
    StableDiffusionImg2ImgPipeline,
    StableDiffusionXLPipeline,  # <-- Add import
    StableDiffusionXLImg2ImgPipeline,  # <-- Add import
    DPMSolverMultistepScheduler,
    EulerDiscreteScheduler
)

from transformers import T5EncoderModel, CLIPTextModel, CLIPTokenizer
import boto3
from botocore.exceptions import ClientError
from botocore.client import Config # æ·»åŠ Configå¯¼å…¥

# ğŸ”§ å…¼å®¹æ€§ä¿®å¤ï¼šæ·»åŠ å›é€€çš„torch.get_default_deviceå‡½æ•°
if not hasattr(torch, 'get_default_device'):
    def get_default_device():
        if torch.cuda.is_available():
            return torch.device('cuda')
        else:
            return torch.device('cpu')
    torch.get_default_device = get_default_device
    print("âœ“ Added fallback torch.get_default_device() function")

# å¯¼å…¥compelç”¨äºå¤„ç†é•¿æç¤ºè¯
try:
    from compel import Compel
    COMPEL_AVAILABLE = True
    print("âœ“ Compel library loaded for long prompt support")
except ImportError:
    COMPEL_AVAILABLE = False
    print("âš ï¸  Compel library not available - long prompt support limited")

# å¯¼å…¥æ¢è„¸é›†æˆæ¨¡å—
try:
    # ç¡®ä¿å½“å‰ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    
    # å¤šä¸ªå¯èƒ½çš„æ–‡ä»¶ä½ç½®
    possible_locations = [
        os.path.join(current_dir, 'face_swap_integration.py'),  # åŒç›®å½•
        os.path.join(os.getcwd(), 'face_swap_integration.py'),  # å·¥ä½œç›®å½•
        '/app/face_swap_integration.py',  # å®¹å™¨ç»å¯¹è·¯å¾„
        './face_swap_integration.py'  # ç›¸å¯¹è·¯å¾„
    ]
    
    face_swap_file = None
    for location in possible_locations:
        if os.path.exists(location):
            face_swap_file = location
            break
    
    if not face_swap_file:
        raise ImportError(f"face_swap_integration.py not found in any of these locations: {possible_locations}")
    
    print(f"ğŸ” Loading face swap integration from: {face_swap_file}")
    from face_swap_integration import process_face_swap_pipeline, is_face_swap_available
    
    FACE_SWAP_AVAILABLE = is_face_swap_available()
    if FACE_SWAP_AVAILABLE:
        print("âœ“ Face swap integration loaded successfully")
    else:
        print("âš ï¸ Face swap models not available - face swap will be disabled")
except ImportError as e:
    FACE_SWAP_AVAILABLE = False
    print(f"âš ï¸ Face swap integration not available: {e}")
    print(f"ğŸ“ Current directory: {os.path.dirname(os.path.abspath(__file__))}")
    print(f"ğŸ“ Working directory: {os.getcwd()}")
    try:
        print(f"ğŸ“ Files in current directory: {os.listdir(os.path.dirname(os.path.abspath(__file__)))}")
        print(f"ğŸ“ Files in working directory: {os.listdir(os.getcwd())}")
    except Exception as list_error:
        print(f"ğŸ“ Could not list directory contents: {list_error}")
except Exception as e:
    FACE_SWAP_AVAILABLE = False
    print(f"âš ï¸ Face swap integration error: {e}")
    import traceback
    print(f"ğŸ“ Error traceback: {traceback.format_exc()}")

# æ·»åŠ å¯åŠ¨æ—¥å¿—
print("=== Starting AI Image Generation Backend ===")
print(f"Python version: {sys.version}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

# éªŒè¯å…³é”®ç¯å¢ƒå˜é‡
required_env_vars = [
    "CLOUDFLARE_R2_ACCESS_KEY",
    "CLOUDFLARE_R2_SECRET_KEY", 
    "CLOUDFLARE_R2_BUCKET",
    "CLOUDFLARE_R2_ENDPOINT"
]

missing_vars = []
for var in required_env_vars:
    if not os.getenv(var):
        missing_vars.append(var)

if missing_vars:
    print(f"WARNING: Missing environment variables: {missing_vars}")
    print("Container will start but R2 upload may fail")

# ç¯å¢ƒå˜é‡
CLOUDFLARE_R2_ACCESS_KEY = os.getenv("CLOUDFLARE_R2_ACCESS_KEY")
CLOUDFLARE_R2_SECRET_KEY = os.getenv("CLOUDFLARE_R2_SECRET_KEY") 
CLOUDFLARE_R2_BUCKET = os.getenv("CLOUDFLARE_R2_BUCKET")
CLOUDFLARE_R2_ENDPOINT = os.getenv("CLOUDFLARE_R2_ENDPOINT")
CLOUDFLARE_R2_PUBLIC_DOMAIN = os.getenv("CLOUDFLARE_R2_PUBLIC_DOMAIN")  # å¯é€‰ï¼šè‡ªå®šä¹‰å…¬å…±åŸŸå

# æ¨¡å‹è·¯å¾„
FLUX_BASE_PATH = "/runpod-volume/flux_base"
FLUX_LORA_BASE_PATH = "/runpod-volume/lora"

# é…ç½®åŸºç¡€æ¨¡å‹ç±»å‹å’Œè·¯å¾„
BASE_MODELS = {
    "realistic": {
        "name": "çœŸäººé£æ ¼",
        "model_path": "/runpod-volume/flux_base",
        "model_type": "flux", 
        "lora_path": None,  # ğŸš¨ ä¿®å¤ï¼šä¸è‡ªåŠ¨åŠ è½½é»˜è®¤LoRA
        "lora_id": None     # ğŸš¨ ä¿®å¤ï¼šè®©ç”¨æˆ·é€‰æ‹©å†³å®šLoRA
    },
    "anime": {
        "name": "åŠ¨æ¼«é£æ ¼", 
        "model_path": "/runpod-volume/cartoon/Anime_NSFW.safetensors",
        "model_type": "diffusers",
        "lora_path": None,  # ğŸš¨ ä¿®å¤ï¼šä¸è‡ªåŠ¨åŠ è½½é»˜è®¤LoRA
        "lora_id": None     # ğŸš¨ ä¿®å¤ï¼šè®©ç”¨æˆ·é€‰æ‹©å†³å®šLoRA
    }
}

# ä¿®å¤ï¼šç§»é™¤é»˜è®¤LoRAé…ç½®ï¼Œè®©ç”¨æˆ·é€‰æ‹©å†³å®š
DEFAULT_LORA_CONFIG = {}

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
txt2img_pipe = None
img2img_pipe = None
current_lora_config = {}  # ä¿®å¤ï¼šåˆå§‹åŒ–ä¸ºç©º
current_base_model = None
device_mapping_enabled = False
current_selected_lora = None  # ä¿®å¤ï¼šåˆå§‹åŒ–ä¸ºNone

# å…¨å±€å˜é‡å­˜å‚¨compelå¤„ç†å™¨
compel_proc = None
compel_proc_neg = None

# æ”¯æŒçš„LoRAæ¨¡å‹åˆ—è¡¨ - æ›´æ–°ä¸ºæ”¯æŒä¸åŒåŸºç¡€æ¨¡å‹
AVAILABLE_LORAS = None
LORAS_LAST_SCAN = 0
LORAS_CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜

# åˆå§‹åŒ– Cloudflare R2 å®¢æˆ·ç«¯
r2_client = None
if all([CLOUDFLARE_R2_ACCESS_KEY, CLOUDFLARE_R2_SECRET_KEY, CLOUDFLARE_R2_BUCKET, CLOUDFLARE_R2_ENDPOINT]):
    try:
        r2_client = boto3.client(
            's3',
            endpoint_url=CLOUDFLARE_R2_ENDPOINT,
            aws_access_key_id=CLOUDFLARE_R2_ACCESS_KEY,
            aws_secret_access_key=CLOUDFLARE_R2_SECRET_KEY,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        print("âœ“ R2 client initialized successfully")
    except Exception as e:
        print(f"âœ— Failed to initialize R2 client: {e}")
        r2_client = None
else:
    print("âœ— R2 configuration incomplete - R2 upload will be disabled")

def get_device():
    """è·å–è®¾å¤‡ï¼Œå…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬"""
    if torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def load_models():
    """æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œä¸é¢„çƒ­"""
    global txt2img_pipe, img2img_pipe, current_base_model
    
    print("âœ“ æ¨¡å‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå°†æŒ‰éœ€åŠ è½½æ¨¡å‹")
    print(f"ğŸ“ æ”¯æŒçš„æ¨¡å‹ç±»å‹: {list(BASE_MODELS.keys())}")
    
    # ä¸é¢„çƒ­ä»»ä½•æ¨¡å‹ï¼Œç­‰å¾…ç”¨æˆ·è¯·æ±‚æ—¶åŠ è½½
    current_base_model = None
    print("ğŸ¯ ç³»ç»Ÿå°±ç»ªï¼Œç­‰å¾…æ¨¡å‹åŠ è½½è¯·æ±‚...")

def load_flux_model(base_path: str, device: str) -> tuple:
    """åŠ è½½FLUXæ¨¡å‹"""
    global device_mapping_enabled
    
    # å†…å­˜ä¼˜åŒ–é…ç½®
    model_kwargs = {
        "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
        "use_safetensors": True,
        "low_cpu_mem_usage": True,
    }
    
    # ğŸš¨ ç¦ç”¨device mappingä»¥é¿å…æ¨¡å‹åˆ‡æ¢æ—¶çš„device conflicts
    print("âš ï¸  ç¦ç”¨FLUX device mappingä»¥é¿å…æ¨¡å‹åˆ‡æ¢å†²çª")
    device_mapping_enabled = False
    
    # ç›´æ¥åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡ï¼Œä¸ä½¿ç”¨device mapping
    txt2img_pipe = FluxPipeline.from_pretrained(
        base_path,
        **model_kwargs
    ).to(device)
    
    # å¯ç”¨ä¼˜åŒ–
    try:
        txt2img_pipe.enable_attention_slicing()
        print("âœ… Attention slicing enabled")
    except Exception as e:
        print(f"âš ï¸  Attention slicing not available: {e}")
        
    # ğŸš¨ è·³è¿‡CPU offloadä»¥é¿å…device conflicts
    print("âš ï¸  è·³è¿‡FLUX CPU offloadä»¥é¿å…deviceå†²çª")
    
    try:
        txt2img_pipe.enable_vae_slicing()
        txt2img_pipe.enable_vae_tiling()
        print("âœ… VAE optimizations enabled")
    except Exception as e:
        print(f"âš ï¸  VAE optimizations not available: {e}")
    
    # åˆ›å»ºå›¾ç”Ÿå›¾ç®¡é“
    print("ğŸ”— Creating FLUX image-to-image pipeline (sharing components)...")
    img2img_pipe = FluxImg2ImgPipeline(
        vae=txt2img_pipe.vae,
        text_encoder=txt2img_pipe.text_encoder,
        text_encoder_2=txt2img_pipe.text_encoder_2,
        tokenizer=txt2img_pipe.tokenizer,
        tokenizer_2=txt2img_pipe.tokenizer_2,
        transformer=txt2img_pipe.transformer,
        scheduler=txt2img_pipe.scheduler,
    ).to(device)
    
    return txt2img_pipe, img2img_pipe

def load_diffusers_model(base_path: str, device: str) -> tuple:
    """åŠ è½½æ ‡å‡†diffusersæ¨¡å‹ - æ”¯æŒSDXLç›®å½•åŠ è½½"""
    print(f"ğŸ¨ Loading diffusers model from {base_path}")
    
    model_filename = os.path.basename(base_path)
    is_anime_nsfw_model = model_filename == "Anime_NSFW.safetensors"

    if is_anime_nsfw_model:
        print(f"ğŸ’¡ ç‰¹å®šé…ç½®: ä¸º {model_filename} ä½¿ç”¨ StableDiffusionXLPipeline å’Œ float16")
        torch_dtype = torch.float16
        variant = "fp16"
        pipeline_class = StableDiffusionXLPipeline
        img2img_pipeline_class = StableDiffusionXLImg2ImgPipeline
        # æš‚æ—¶ç¦ç”¨offloadä»¥åŒ¹é…notebookè¡Œä¸ºï¼Œåç»­å¯æ ¹æ®å†…å­˜æƒ…å†µè°ƒæ•´
        enable_offload = False 
    else:
        print(f"ğŸ’¡ æ ‡å‡†é…ç½®: ä¸º {model_filename} ä½¿ç”¨ StableDiffusionPipeline å’Œ float32 (å…¼å®¹æ€§ä¼˜å…ˆ)")
        torch_dtype = torch.float32
        variant = None # variantä¸ç”¨äºé€šç”¨SDPipelineæˆ–ç›®å½•åŠ è½½
        pipeline_class = StableDiffusionPipeline
        img2img_pipeline_class = StableDiffusionImg2ImgPipeline
        enable_offload = True # å¯¹å…¶ä»–æ¨¡å‹ä¿æŒoffload

    print(f"ğŸ’¡ ä½¿ç”¨ {torch_dtype} ç²¾åº¦åŠ è½½æ¨¡å‹")
    
    try:
        if os.path.isdir(base_path):
            print(f"ğŸ“ æ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨from_pretrainedåŠ è½½æ¨¡å‹ ({pipeline_class.__name__})")
            if pipeline_class == StableDiffusionXLPipeline:
                txt2img_pipeline = pipeline_class.from_pretrained(
                    base_path,
                    torch_dtype=torch_dtype,
                    variant=variant if variant else None,
                    use_safetensors=True,
                    # safety_checker, requires_safety_checker not valid for SDXL from_pretrained
                ).to(device)
            else:
                txt2img_pipeline = pipeline_class.from_pretrained(
                    base_path,
                    torch_dtype=torch_dtype,
                    variant=variant if variant else None, 
                    use_safetensors=True,
                    safety_checker=None,
                    requires_safety_checker=False
                ).to(device)
        else:
            print(f"ğŸ“„ æ£€æµ‹åˆ°å•æ–‡ä»¶ï¼Œä½¿ç”¨from_single_fileåŠ è½½ ({pipeline_class.__name__})")
            if pipeline_class == StableDiffusionXLPipeline:
                txt2img_pipeline = pipeline_class.from_single_file(
                    base_path,
                    torch_dtype=torch_dtype,
                    variant=variant if variant else None,
                    use_safetensors=True,
                    # safety_checker, requires_safety_checker, load_safety_checker not valid for SDXL from_single_file
                ).to(device)
            else:
                txt2img_pipeline = pipeline_class.from_single_file(
                    base_path,
                    torch_dtype=torch_dtype,
                    variant=variant if variant else None, 
                    use_safetensors=True,
                    safety_checker=None,
                    requires_safety_checker=False,
                    load_safety_checker=False 
                ).to(device)
        
        # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        txt2img_pipeline.enable_attention_slicing()
        if enable_offload:
            print("ğŸ“¦ å¯ç”¨æ¨¡å‹CPU Offload")
            txt2img_pipeline.enable_model_cpu_offload()
        else:
            print("ğŸš« æ¨¡å‹CPU Offloadå·²ç¦ç”¨ (ç‰¹å®šäºAnime_NSFW.safetensorsæµ‹è¯•)")

        if img2img_pipeline_class == StableDiffusionXLImg2ImgPipeline:
            # SDXL img2imgç®¡é“ä¸æ¥å—safety_checkerå‚æ•°
            img2img_pipeline = img2img_pipeline_class(
                vae=txt2img_pipeline.vae,
                text_encoder=txt2img_pipeline.text_encoder,
                text_encoder_2=txt2img_pipeline.text_encoder_2,
                tokenizer=txt2img_pipeline.tokenizer,
                tokenizer_2=txt2img_pipeline.tokenizer_2,
                unet=txt2img_pipeline.unet,
                scheduler=txt2img_pipeline.scheduler,
                feature_extractor=getattr(txt2img_pipeline, 'feature_extractor', None),
            ).to(device)
        else:
            # æ ‡å‡†SD img2imgç®¡é“æ¥å—safety_checkerå‚æ•°
            img2img_pipeline = img2img_pipeline_class(
                vae=txt2img_pipeline.vae,
                text_encoder=getattr(txt2img_pipeline, 'text_encoder', None),
                text_encoder_2=getattr(txt2img_pipeline, 'text_encoder_2', None),
                tokenizer=getattr(txt2img_pipeline, 'tokenizer', None),
                tokenizer_2=getattr(txt2img_pipeline, 'tokenizer_2', None),
                unet=txt2img_pipeline.unet,
                scheduler=txt2img_pipeline.scheduler,
                safety_checker=None,
                feature_extractor=getattr(txt2img_pipeline, 'feature_extractor', None),
                requires_safety_checker=False
            ).to(device)
        
        txt2img_pipeline.safety_checker = None
        txt2img_pipeline.requires_safety_checker = False
        img2img_pipeline.safety_checker = None
        img2img_pipeline.requires_safety_checker = False
        
        img2img_pipeline.enable_attention_slicing()
        if enable_offload: # Apply to img2img pipe as well
             print("ğŸ“¦ ä¸ºimg2imgç®¡é“å¯ç”¨æ¨¡å‹CPU Offload")
             img2img_pipeline.enable_model_cpu_offload()
        else:
            print("ğŸš« img2imgç®¡é“æ¨¡å‹CPU Offloadå·²ç¦ç”¨")
        
        print(f"âœ… {pipeline_class.__name__} æ¨¡å‹åŠ è½½æˆåŠŸ: {base_path}")
        return txt2img_pipeline, img2img_pipeline  # ğŸš¨ ä¿®å¤ï¼šè¿”å›æ­£ç¡®çš„img2img_pipelineè€Œä¸æ˜¯img2img_pipe
        
    except Exception as e:
        print(f"âŒ Error loading diffusers model ({pipeline_class.__name__}): {str(e)}")
        raise e

def load_specific_model(base_model_type: str):
    """åŠ è½½æŒ‡å®šçš„åŸºç¡€æ¨¡å‹ - ä¿®å¤ï¼šä¸è‡ªåŠ¨åŠ è½½LoRA"""
    global txt2img_pipe, img2img_pipe, current_base_model, current_lora_config, current_selected_lora
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    # ğŸš¨ å½»åº•æ¸…ç†ä¹‹å‰çš„æ¨¡å‹ï¼Œé¿å…device conflicts
    if txt2img_pipe is not None:
        print("ğŸ§¹ æ¸…ç†ä¹‹å‰çš„txt2imgæ¨¡å‹...")
        try:
            del txt2img_pipe
        except:
            pass
        txt2img_pipe = None
    
    if img2img_pipe is not None:
        print("ğŸ§¹ æ¸…ç†ä¹‹å‰çš„img2imgæ¨¡å‹...")
        try:
            del img2img_pipe
        except:
            pass
        img2img_pipe = None
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")
    
    model_config = BASE_MODELS[base_model_type]
    device = get_device()
    
    print(f"ğŸ¯ Loading {model_config['name']} model...")
    
    try:
        model_start_time = datetime.now()
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        if model_config["model_type"] == "flux":
            txt2img_pipe, img2img_pipe = load_flux_model(model_config["model_path"], device)
        elif model_config["model_type"] == "diffusers":
            txt2img_pipe, img2img_pipe = load_diffusers_model(model_config["model_path"], device)
        
        current_base_model = base_model_type
        
        # ğŸš¨ ä¿®å¤ï¼šä¸è‡ªåŠ¨åŠ è½½é»˜è®¤LoRAï¼Œä¿æŒæ¸…æ´çŠ¶æ€ï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©
        print("â„¹ï¸  åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆï¼Œæ— é»˜è®¤LoRAï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©LoRA")
        current_lora_config = {}
        current_selected_lora = None
        
        model_time = (datetime.now() - model_start_time).total_seconds()
        print(f"ğŸ‰ {model_config['name']} model loaded successfully in {model_time:.2f}s!")
        
        # ğŸš¨ è·³è¿‡åŠ¨æ¼«æ¨¡å‹çš„é¢„çƒ­æ¨ç†ï¼Œé¿å…ç²¾åº¦é—®é¢˜
        if model_config["model_type"] == "diffusers":
            print("âš¡ è·³è¿‡åŠ¨æ¼«æ¨¡å‹é¢„çƒ­æ¨ç†(é¿å…ç²¾åº¦å…¼å®¹æ€§é—®é¢˜)")
            print("âœ… åŠ¨æ¼«æ¨¡å‹ready for generation (no warmup needed)")
        else:
            # å¯¹FLUXæ¨¡å‹è¿›è¡Œé¢„çƒ­
            try:
                print("ğŸ”¥ Warming up model with test inference...")
                warmup_start = datetime.now()
                with torch.no_grad():
                    test_result = txt2img_pipe(
                        prompt="test",
                        width=512,
                        height=512,
                        num_inference_steps=1,
                        guidance_scale=1.0
                    )
                warmup_time = (datetime.now() - warmup_start).total_seconds()
                print(f"âœ… Model warmup completed in {warmup_time:.2f}s")
            except Exception as warmup_error:
                print(f"âš ï¸  Model warmup failed, but model should still work: {warmup_error}")
        
        print(f"ğŸš€ {model_config['name']} system ready for image generation!")
        
    except Exception as e:
        print(f"âŒ Failed to load {model_config['name']} model: {e}")
        # é‡ç½®å…¨å±€å˜é‡
        txt2img_pipe = None
        img2img_pipe = None
        current_base_model = None
        current_lora_config = {}
        current_selected_lora = None
        raise

def upload_to_r2(image_data: bytes, filename: str) -> str:
    """ä¸Šä¼ å›¾ç‰‡åˆ° Cloudflare R2"""
    try:
        # æ£€æŸ¥R2å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if r2_client is None:
            raise RuntimeError("R2 client not available - check environment variables")
            
        # ç¡®ä¿ image_data æ˜¯ bytes ç±»å‹
        if not isinstance(image_data, bytes):
            raise TypeError(f"Expected bytes, got {type(image_data)}")
            
        # éªŒè¯æ–‡ä»¶å¤§å°
        if len(image_data) == 0:
            raise ValueError("Image data is empty")
            
        if len(image_data) > 50 * 1024 * 1024:  # 50MB limit
            raise ValueError(f"Image too large: {len(image_data)} bytes")
            
        print(f"Uploading {len(image_data)} bytes to R2 as {filename}")
        
        r2_client.put_object(
            Bucket=CLOUDFLARE_R2_BUCKET,
            Key=filename,
            Body=image_data,
            ContentType='image/png',
            ACL='public-read'
        )
        
        # æ„å»ºæ­£ç¡®çš„å…¬å…± URL æ ¼å¼ - ä¼˜å…ˆä½¿ç”¨ R2 Public Domain
        # ä½¿ç”¨æä¾›çš„ R2 public domain (æœ€ç¨³å®šçš„è§£å†³æ–¹æ¡ˆ)
        r2_public_domain = os.getenv("CLOUDFLARE_R2_PUBLIC_BUCKET_DOMAIN")
        
        if r2_public_domain:
            # ä½¿ç”¨ R2 public domain (.r2.dev æ ¼å¼)
            public_url = f"https://{r2_public_domain}/{filename}"
            print(f"âœ“ Successfully uploaded to (R2 public domain): {public_url}")
        elif CLOUDFLARE_R2_PUBLIC_DOMAIN:
            # å¤‡é€‰ï¼šè‡ªå®šä¹‰åŸŸå
            public_url = f"{CLOUDFLARE_R2_PUBLIC_DOMAIN.rstrip('/')}/{filename}"
            print(f"âœ“ Successfully uploaded to (custom domain): {public_url}")
        else:
            # æœ€åå›é€€åˆ°æ ‡å‡†R2æ ¼å¼
            # æ­£ç¡®æ ¼å¼: https://{bucket}.{account_id}.r2.cloudflarestorage.com/{filename}
            # ä»endpoint URLä¸­æå–account ID
            account_id = CLOUDFLARE_R2_ENDPOINT.split('//')[1].split('.')[0]
            public_url = f"https://{CLOUDFLARE_R2_BUCKET}.{account_id}.r2.cloudflarestorage.com/{filename}"
            print(f"âœ“ Successfully uploaded to (standard R2): {public_url}")
            print(f"âš ï¸  æ³¨æ„ï¼šå¦‚æœå‡ºç°CORSé”™è¯¯ï¼Œå»ºè®®è®¾ç½® CLOUDFLARE_R2_PUBLIC_BUCKET_DOMAIN ç¯å¢ƒå˜é‡")
        
        return public_url
        
    except Exception as e:
        print(f"âœ— Error uploading to R2: {str(e)}")
        print(f"Image data type: {type(image_data)}, size: {len(image_data) if hasattr(image_data, '__len__') else 'unknown'}")
        
        # å¯¹äºæ¼”ç¤ºç›®çš„ï¼Œè¿”å›ä¸€ä¸ªå ä½ç¬¦URLè€Œä¸æ˜¯å¤±è´¥
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æŠ›å‡ºå¼‚å¸¸
        placeholder_url = f"https://via.placeholder.com/512x512/cccccc/666666?text=Upload+Failed"
        print(f"Returning placeholder URL: {placeholder_url}")
        return placeholder_url

def image_to_bytes(image: Image.Image) -> bytes:
    """å°† PIL Image è½¬æ¢ä¸ºå­—èŠ‚"""
    try:
        buffer = io.BytesIO()
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image.save(buffer, format='PNG', quality=95, optimize=True)
        buffer.seek(0)  # é‡ç½®bufferä½ç½®
        return buffer.getvalue()
    except Exception as e:
        print(f"Error converting image to bytes: {str(e)}")
        raise e

def base64_to_image(base64_str: str) -> Image.Image:
    """å°† base64 å­—ç¬¦ä¸²è½¬æ¢ä¸º PIL Image"""
    if base64_str.startswith('data:image'):
        base64_str = base64_str.split(',')[1]
    
    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image.convert('RGB')

def process_long_prompt(prompt: str, max_clip_tokens: int = 75, max_t5_tokens: int = 500) -> tuple:
    """
    å¤„ç†é•¿æç¤ºè¯ï¼Œä¸ºFLUXçš„åŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
    
    Args:
        prompt: è¾“å…¥æç¤ºè¯
        max_clip_tokens: CLIPç¼–ç å™¨æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤75ï¼Œç•™2ä¸ªç‰¹æ®Štokenç©ºé—´ï¼‰
        max_t5_tokens: T5ç¼–ç å™¨æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤500ï¼Œç•™ç©ºé—´ç»™ç‰¹æ®Štokenï¼‰
    
    Returns:
        tuple: (clip_prompt, t5_prompt)
    """
    if not prompt:
        return "", ""
    
    # ğŸ¯ æ›´å‡†ç¡®çš„tokenä¼°ç®—ï¼šè€ƒè™‘æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    # ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼ã€é€—å·ã€æ ‡ç‚¹ç¬¦å·åˆ†å‰²
    token_pattern = r'\w+|[^\w\s]'  # æå–regexæ¨¡å¼é¿å…f-stringä¸­çš„åæ–œæ 
    tokens = re.findall(token_pattern, prompt.lower())
    estimated_tokens = len(tokens)
    
    print(f"ğŸ“ Prompt analysis: {len(prompt)} chars, ~{estimated_tokens} tokens (improved estimation)")
    
    if estimated_tokens <= max_clip_tokens:
        # çŸ­promptï¼šä¸¤ä¸ªç¼–ç å™¨éƒ½ä½¿ç”¨å®Œæ•´prompt
        print("âœ… Short prompt: using full prompt for both CLIP and T5")
        return prompt, prompt
    else:
        # é•¿promptï¼šCLIPä½¿ç”¨æˆªæ–­ç‰ˆæœ¬ï¼ŒT5ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬
        if estimated_tokens <= max_t5_tokens:
            # ğŸ¯ æ›´æ™ºèƒ½çš„CLIPæˆªæ–­ï¼šä¿æŒå®Œæ•´çš„è¯­ä¹‰å•å…ƒ
            words = prompt.split()
            
            # ä»å‰å¾€åç´¯ç§¯tokenï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
            clip_words = []
            current_tokens = 0
            
            for word in words:
                # ä¼°ç®—å½“å‰å•è¯çš„tokenæ•°ï¼ˆè€ƒè™‘æ ‡ç‚¹ç¬¦å·ï¼‰
                word_tokens = len(re.findall(token_pattern, word.lower()))
                
                if current_tokens + word_tokens <= max_clip_tokens:
                    clip_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # å¦‚æœæˆªæ–­ç‚¹ä¸ç†æƒ³ï¼Œå°è¯•åœ¨å¥å·æˆ–é€—å·å¤„æˆªæ–­
            if len(clip_words) > 10:  # åªåœ¨æœ‰è¶³å¤Ÿè¯æ±‡æ—¶ä¼˜åŒ–æˆªæ–­ç‚¹
                for i in range(len(clip_words) - 1, max(0, len(clip_words) - 5), -1):
                    if clip_words[i].endswith(('.', ',', ';', '!')):
                        clip_words = clip_words[:i+1]
                        break
            
            clip_prompt = ' '.join(clip_words)
            clip_token_count = len(re.findall(token_pattern, clip_prompt.lower()))
            
            print(f"ğŸ“ Long prompt optimization:")
            print(f"   CLIP prompt: ~{len(clip_words)} words â†’ {clip_token_count} tokens (safe truncation)")
            print(f"   T5 prompt: ~{estimated_tokens} tokens (full prompt)")
            return clip_prompt, prompt
        else:
            # è¶…é•¿promptï¼šä¸¤ä¸ªç¼–ç å™¨éƒ½éœ€è¦æˆªæ–­
            words = prompt.split()
            
            # CLIPæˆªæ–­
            clip_words = []
            current_tokens = 0
            for word in words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if current_tokens + word_tokens <= max_clip_tokens:
                    clip_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # T5æˆªæ–­
            t5_words = []
            current_tokens = 0
            for word in words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if current_tokens + word_tokens <= max_t5_tokens:
                    t5_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # ä¼˜åŒ–æˆªæ–­ç‚¹
            if len(clip_words) > 10:
                for i in range(len(clip_words) - 1, max(0, len(clip_words) - 5), -1):
                    if clip_words[i].endswith(('.', ',', ';')):
                        clip_words = clip_words[:i+1]
                        break
                        
            if len(t5_words) > 20:
                for i in range(len(t5_words) - 1, max(0, len(t5_words) - 10), -1):
                    if t5_words[i].endswith(('.', ',', ';')):
                        t5_words = t5_words[:i+1]
                        break
            
            clip_prompt = ' '.join(clip_words)
            t5_prompt = ' '.join(t5_words)
            
            clip_token_count = len(re.findall(token_pattern, clip_prompt.lower()))
            t5_token_count = len(re.findall(token_pattern, t5_prompt.lower()))
            
            print(f"âš ï¸  Ultra-long prompt: both encoders truncated intelligently")
            print(f"   CLIP prompt: ~{len(clip_words)} words â†’ {clip_token_count} tokens")
            print(f"   T5 prompt: ~{len(t5_words)} words â†’ {t5_token_count} tokens")
            return clip_prompt, t5_prompt

def generate_flux_images(prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str) -> list:
    """FLUXæ¨¡å‹å›¾åƒç”Ÿæˆ"""
    global txt2img_pipe, device_mapping_enabled
    
    # FLUXæ¨¡å‹åŸç”Ÿæ”¯æŒé•¿æç¤ºè¯ï¼Œä½¿ç”¨ä¼˜åŒ–çš„embeddingå¤„ç†
    generation_kwargs = {
        "width": width,
        "height": height,
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating FLUX prompt embeddings using pipeline.encode_prompt()...")
    try:
        device = get_device()
        
        # ğŸš¨ ä¿®å¤ï¼šç®€åŒ–FLUXé•¿promptå¤„ç†ï¼Œé¿å…deviceå†²çª
        print(f"ğŸ’¾ GPU Memory before encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
        
        # ğŸ¯ ä¼˜åŒ–é•¿æç¤ºè¯å¤„ç†ï¼šä¸ºFLUXåŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
        clip_prompt, t5_prompt = process_long_prompt(prompt)
        print(f"ğŸ“ FLUX prompt processing:")
        print(f"   CLIP prompt: {len(clip_prompt)} chars")
        print(f"   T5 prompt: {len(t5_prompt)} chars")
        
        # ğŸš¨ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨pipeline encode_promptï¼Œä¸è¿›è¡ŒCPU/GPUåˆ‡æ¢
        with torch.cuda.amp.autocast(enabled=False):
            prompt_embeds_obj = txt2img_pipe.encode_prompt(
                prompt=clip_prompt,    # CLIPç¼–ç å™¨ä½¿ç”¨ä¼˜åŒ–åçš„prompt
                prompt_2=t5_prompt,    # T5ç¼–ç å™¨ä½¿ç”¨å®Œæ•´prompt
                device=device,
                num_images_per_prompt=1 
            )
        
        # å¤„ç†embeddings
        if hasattr(prompt_embeds_obj, 'prompt_embeds'):
            prompt_embeds = prompt_embeds_obj.prompt_embeds
            pooled_prompt_embeds = prompt_embeds_obj.pooled_prompt_embeds if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds') else None
        else:
            # Handle tuple case
            prompt_embeds = prompt_embeds_obj[0] if isinstance(prompt_embeds_obj, tuple) else None
            pooled_prompt_embeds = prompt_embeds_obj[1] if isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) > 1 else None
        
        # è®¾ç½®embeddingsåˆ°generation_kwargs
        if prompt_embeds is not None:
            generation_kwargs["prompt_embeds"] = prompt_embeds
            print("âœ… FLUX prompt embeddingsç”ŸæˆæˆåŠŸ")
        
        if pooled_prompt_embeds is not None:
            generation_kwargs["pooled_prompt_embeds"] = pooled_prompt_embeds
            print("âœ… FLUX pooled embeddingsç”ŸæˆæˆåŠŸ")

        # FLUXä½¿ç”¨ä¼ ç»Ÿçš„guidance_scaleå‚æ•°
        generation_kwargs["guidance_scale"] = cfg_scale
        print(f"ğŸ›ï¸ Using guidance_scale: {cfg_scale}")
        print(f"ğŸ’¾ GPU Memory after encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

    except Exception as e:
        print(f"âš ï¸ FLUX pipeline.encode_prompt() failed: {e}. Using raw prompts.")
        generation_kwargs["prompt"] = prompt
        # ğŸš¨ FLUXæ¨¡å‹ä¸æ”¯æŒnegative_promptï¼Œç§»é™¤æ­¤å‚æ•°
        # generation_kwargs["negative_prompt"] = negative_prompt  # <-- æ³¨é‡Šæ‰è¿™è¡Œ

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text-to-image")

def generate_diffusers_images(prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str) -> list:
    """ä½¿ç”¨æ ‡å‡†diffusersç®¡é“ç”Ÿæˆå›¾åƒ - æ”¯æŒé•¿æç¤ºè¯å¤„ç†å’ŒWAI-NSFW-illustrious-SDXLä¼˜åŒ–å‚æ•°"""
    global txt2img_pipe
    import traceback  # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿tracebackå·²å¯¼å…¥
    
    if txt2img_pipe is None:
        raise RuntimeError("Diffusers pipeline not loaded")
    
    print(f"ğŸ“ Processing anime model generation...")
    
    # ğŸš¨ å…¨é¢çš„å‚æ•°å®‰å…¨æ£€æŸ¥å’Œä¿®å¤
    if not prompt or prompt is None:
        prompt = "masterpiece, best quality, amazing quality, 1boy, handsome man, anime style"
        print(f"âš ï¸  ä¿®å¤ç©ºprompt: {prompt}")
    
    if negative_prompt is None:
        negative_prompt = ""
        print(f"âš ï¸  ä¿®å¤None negative_prompt")
    
    # ç¡®ä¿promptå’Œnegative_promptéƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
    prompt = str(prompt).strip()
    negative_prompt = str(negative_prompt).strip()
    
    # ğŸš¨ æ ¹æ®CivitAI WAI-NSFW-illustrious-SDXLæ¨èè®¾ç½®
    # å¼ºåˆ¶ä½¿ç”¨1024x1024æˆ–æ›´å¤§å°ºå¯¸
    if width < 1024 or height < 1024:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹éœ€è¦1024x1024æˆ–æ›´å¤§ ({width}x{height})ï¼Œè°ƒæ•´ä¸º1024x1024")
        width = max(1024, width)
        height = max(1024, height)
    
    # CFG Scale: 5-7 (CivitAIæ¨è)
    if cfg_scale < 5.0:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹CFGè¿‡ä½ ({cfg_scale})ï¼Œè°ƒæ•´ä¸º6.0 (æ¨è5-7)")
        cfg_scale = 6.0
    elif cfg_scale > 7.0:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹CFGè¿‡é«˜ ({cfg_scale})ï¼Œè°ƒæ•´ä¸º6.5 (æ¨è5-7)")
        cfg_scale = 6.5
    
    # Steps: 15-30 (v14), 25-40 (older versions) - æˆ‘ä»¬ä½¿ç”¨25-30
    if steps < 15:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹stepsè¿‡ä½ ({steps})ï¼Œè°ƒæ•´ä¸º20 (æ¨è15-30)")
        steps = 20
    elif steps > 35:
        print(f"âš ï¸  WAI-NSFW-illustrious-SDXLæ¨¡å‹stepsè¿‡é«˜ ({steps})ï¼Œè°ƒæ•´ä¸º30 (æ¨è15-30)")
        steps = 30
    
    # ğŸš¨ ä¿®å¤ï¼šæ·»åŠ WAI-NSFW-illustrious-SDXLæ¨èçš„è´¨é‡æ ‡ç­¾
    # ğŸš¨ ä¿®å¤ï¼šæ·»åŠ æ¨èçš„è´Ÿé¢æç¤º
    recommended_negative = "bad quality, worst quality, worst detail, sketch, censor"
    # ğŸš¨ ä¿®å¤ï¼šé˜²æ­¢é‡å¤æ·»åŠ æ¨ènegative prompt
    # ğŸ”§ å¯é€‰ï¼šå¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥ä»»ä½•è´Ÿé¢æç¤ºè¯ï¼Œå¯ä»¥è·³è¿‡è‡ªåŠ¨æ·»åŠ 
    auto_add_negative = False  # è®¾ä¸ºFalseå¯å®Œå…¨ç¦ç”¨è‡ªåŠ¨æ·»åŠ 
    
    if auto_add_negative and recommended_negative not in negative_prompt:
        if negative_prompt and negative_prompt.strip():
            # å¦‚æœç”¨æˆ·æœ‰è‡ªå®šä¹‰è´Ÿé¢æç¤ºï¼Œæ·»åŠ åˆ°æ¨èè´Ÿé¢æç¤ºä¹‹å
            negative_prompt = recommended_negative + ", " + negative_prompt
        else:
            # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰è´Ÿé¢æç¤ºï¼Œä½¿ç”¨æ¨èçš„
            negative_prompt = recommended_negative
        print(f"ğŸ›¡ï¸ æ·»åŠ WAI-NSFW-illustrious-SDXLæ¨èè´Ÿé¢æç¤º")
    else:
        if auto_add_negative:
            print(f"ğŸ›¡ï¸ å·²åŒ…å«æ¨èè´Ÿé¢æç¤ºï¼Œè·³è¿‡æ·»åŠ ")
        else:
            print(f"ğŸ”§ è‡ªåŠ¨æ·»åŠ è´Ÿé¢æç¤ºå·²ç¦ç”¨ï¼Œä¿æŒç”¨æˆ·åŸå§‹è¾“å…¥")
    
    print(f"ğŸ” æœ€ç»ˆå‚æ•°æ£€æŸ¥:")
    print(f"  prompt: {repr(prompt)} (type: {type(prompt)})")
    print(f"  negative_prompt: {repr(negative_prompt)} (type: {type(negative_prompt)})")
    print(f"  dimensions: {width}x{height}")
    print(f"  steps: {steps}, cfg_scale: {cfg_scale}")
    
    # ğŸ¯ SDXLé•¿æç¤ºè¯å¤„ç† - åŠ¨æ¼«æ¨¡å‹é¿å…Compelï¼Œä½¿ç”¨æ™ºèƒ½å‹ç¼©
    processed_prompt = prompt
    processed_negative_prompt = negative_prompt
    
    try:
        # ğŸš¨ ä¿®å¤ï¼šä½¿ç”¨æ›´å‡†ç¡®çš„tokenä¼°ç®—æ–¹æ³•
        import re
        token_pattern = r'\w+|[^\w\s]'
        estimated_tokens = len(re.findall(token_pattern, prompt.lower()))
        
        # ğŸš¨ ä¿®å¤ï¼šåŠ¨æ¼«æ¨¡å‹å§‹ç»ˆä½¿ç”¨æ™ºèƒ½å‹ç¼©ï¼Œé¿å…Compelå¯¼è‡´çš„é»‘å›¾é—®é¢˜
        print(f"ğŸ’¡ åŠ¨æ¼«æ¨¡å‹å§‹ç»ˆä½¿ç”¨æ™ºèƒ½å‹ç¼©æ¨¡å¼ (ä¼°è®¡token: {estimated_tokens})")
        
        # å‹ç¼©æ­£å‘prompt
        if estimated_tokens > 75:
            print(f"ğŸ“ å‹ç¼©é•¿prompt: {estimated_tokens} tokens -> 75 tokens")
            processed_prompt = compress_prompt_to_77_tokens(processed_prompt, max_tokens=75)
            print(f"âœ… promptå‹ç¼©å®Œæˆ")
        else:
            print("âœ… promptå·²åœ¨75 tokené™åˆ¶å†…ï¼Œæ— éœ€å‹ç¼©")
        
        # å‹ç¼©negative prompt
        negative_tokens = len(re.findall(r'\w+|[^\w\s]', processed_negative_prompt.lower()))
        if negative_tokens > 75:
            print(f"ğŸ”§ å‹ç¼©negative prompt: {negative_tokens} tokens -> 75 tokens")
            processed_negative_prompt = compress_prompt_to_77_tokens(processed_negative_prompt, max_tokens=75)
            print(f"âœ… negative promptå‹ç¼©å®Œæˆ")
        
        # ä½¿ç”¨æ ‡å‡†å¤„ç†æ–¹å¼ï¼Œé¿å…Compel
        generation_kwargs = {
            'prompt': processed_prompt,
            'negative_prompt': processed_negative_prompt,
            'height': height,
            'width': width,
            'num_inference_steps': steps,
            'guidance_scale': cfg_scale,
            'num_images_per_prompt': 1,
            'output_type': 'pil',
            'return_dict': True
        }
        
        # ğŸš¨ ä¿®å¤é€’å½’è°ƒç”¨ - ç›´æ¥ä½¿ç”¨generate_images_commonç»Ÿä¸€å¤„ç†
        print(f"ğŸ¨ ä½¿ç”¨ {base_model} diffusersæ¨¡å‹ç”Ÿæˆå›¾åƒ...")
        print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹æ¨è1024x1024ä»¥ä¸Šåˆ†è¾¨ç‡")
        print("ğŸ”§ åŠ¨æ¼«æ¨¡å‹ä¼˜åŒ–å‚æ•°(CivitAIæ¨è): steps=20, cfg_scale=6, size=1024x1024")
        
        return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text-to-image")
        
    except Exception as long_prompt_error:
        print(f"âš ï¸  æ™ºèƒ½å‹ç¼©å¤„ç†å¤±è´¥: {long_prompt_error}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        print("ğŸ“ å›é€€åˆ°æ ‡å‡†å¤„ç†æ¨¡å¼")
        
        generation_kwargs = {
            "prompt": processed_prompt,
            "negative_prompt": processed_negative_prompt,
            "height": int(height),
            "width": int(width),
            "num_inference_steps": int(steps),
            "guidance_scale": float(cfg_scale),
            "num_images_per_prompt": 1,
            "output_type": "pil",
            "return_dict": True
        }
        print("âœ… å›é€€åˆ°æ ‡å‡†å¤„ç†")
        
        return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text-to-image")

def generate_images_common(generation_kwargs: dict, prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str, task_type: str) -> list:
    """é€šç”¨å›¾åƒç”Ÿæˆé€»è¾‘ - æ”¯æŒçœŸæ­£çš„å¤šå¼ ç”Ÿæˆ"""
    global txt2img_pipe, current_base_model
    
    # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸ºNoneï¼Œé¿å…NoneTypeé”™è¯¯
    if prompt is None or prompt == "":
        prompt = "masterpiece, best quality, 1boy"
        print(f"âš ï¸  ç©ºpromptï¼Œä½¿ç”¨é»˜è®¤: {prompt}")
    if negative_prompt is None:
        negative_prompt = ""
        print(f"âš ï¸  negative_promptä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²")
    
    print(f"ğŸ” Debug - prompt: {repr(prompt)}, negative_prompt: {repr(negative_prompt)}")
    
    results = []
    
    # è·å–å½“å‰æ¨¡å‹ç±»å‹ä»¥ç¡®å®šautocastç­–ç•¥
    model_config = BASE_MODELS.get(current_base_model, {})
    model_type = model_config.get("model_type", "unknown")
    
    # ğŸš¨ åŠ¨æ¼«æ¨¡å‹ç¦ç”¨autocasté¿å…LayerNormç²¾åº¦é—®é¢˜
    use_autocast = model_type == "flux"  # åªæœ‰FLUXæ¨¡å‹ä½¿ç”¨autocast
    
    print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆ {num_images} å¼ å›¾åƒ (æ¨¡å‹: {model_type})")
    
    # ğŸ¯ ä¿®å¤ï¼šå¾ªç¯ç”ŸæˆçœŸæ­£çš„å¤šå¼ å›¾ç‰‡
    for i in range(num_images):
        try:
            # ä¸ºæ¯å¼ å›¾ç‰‡è®¾ç½®ä¸åŒçš„éšæœºç§å­
            current_generation_kwargs = generation_kwargs.copy()
            
            if seed != -1:
                # åŸºäºåŸå§‹ç§å­ç”Ÿæˆä¸åŒçš„ç§å­
                current_seed = seed + i
                import torch
                generator = torch.Generator(device=txt2img_pipe.device).manual_seed(int(current_seed))
                current_generation_kwargs["generator"] = generator
                print(f"ğŸ² å›¾åƒ {i+1} ç§å­: {current_seed}")
            else:
                # ğŸš¨ ä¿®å¤ï¼šä¸ºéšæœºç§å­ç”Ÿæˆå…·ä½“çš„ç§å­å€¼å¹¶æ˜¾ç¤º
                import random
                current_seed = random.randint(0, 2147483647)  # ä½¿ç”¨32ä½æ•´æ•°èŒƒå›´
                import torch
                generator = torch.Generator(device=txt2img_pipe.device).manual_seed(int(current_seed))
                current_generation_kwargs["generator"] = generator
                print(f"ğŸ² å›¾åƒ {i+1} ç§å­: {current_seed} (éšæœºç”Ÿæˆ)")
            
            # ç”Ÿæˆå›¾åƒ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨autocast
            if use_autocast:
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    result = txt2img_pipe(**current_generation_kwargs)
            else:
                print(f"ğŸ’¡ åŠ¨æ¼«æ¨¡å‹: è·³è¿‡autocastä½¿ç”¨float32ç²¾åº¦ (å›¾åƒ {i+1})")
                result = txt2img_pipe(**current_generation_kwargs)
            
            # å¤„ç†ç»“æœ
            if hasattr(result, 'images') and result.images and len(result.images) > 0:
                image = result.images[0]  # å–ç¬¬ä¸€å¼ å›¾ç‰‡
                if image is not None:
                    try:
                        # ä¸Šä¼ åˆ°R2
                        filename = f"txt2img_{current_base_model}_{int(time.time())}_{i}.png"
                        image_url = upload_to_r2(image_to_bytes(image), filename)
                        
                        results.append({
                            'url': image_url,
                            'filename': filename,
                            'prompt': prompt,
                            'model': current_base_model,
                            'width': width,
                            'height': height,
                            'steps': steps,
                            'cfg_scale': cfg_scale,
                            'seed': current_seed  # ğŸš¨ ä¿®å¤ï¼šæ€»æ˜¯åŒ…å«å…·ä½“çš„ç§å­å€¼
                        })
                        print(f"âœ… å›¾åƒ {i+1}/{num_images} ç”ŸæˆæˆåŠŸ: {filename}")
                    except Exception as upload_error:
                        print(f"âŒ ä¸Šä¼ å›¾åƒ {i+1} å¤±è´¥: {upload_error}")
                        continue
                else:
                    print(f"âš ï¸  å›¾åƒ {i+1} ç”Ÿæˆç»“æœä¸ºç©º")
            else:
                print(f"âš ï¸  å›¾åƒ {i+1} ç®¡é“è¿”å›ç©ºç»“æœæˆ–æ— å›¾åƒ")
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›¾åƒ {i+1} å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            continue
    
    # åˆ é™¤é‡å¤çš„æ—¥å¿—è¾“å‡º - å·²åœ¨generate_images_commonä¸­ç»Ÿä¸€å¤„ç†
    print(f"ğŸ¯ æ€»å…±æˆåŠŸç”Ÿæˆäº† {len(results)} å¼ å›¾åƒ")
    return results

def text_to_image(prompt: str, negative_prompt: str = "", width: int = 1024, height: int = 1024, steps: int = 25, cfg_scale: float = 5.0, seed: int = -1, num_images: int = 1, base_model: str = "realistic", lora_config: dict = None) -> list:
    """æ–‡æœ¬ç”Ÿæˆå›¾åƒ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹"""
    global current_base_model, txt2img_pipe
    
    print(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {base_model}, å½“å‰åŠ è½½æ¨¡å‹: {current_base_model}")
    
    # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿lora_configæœ‰é»˜è®¤å€¼
    if lora_config is None:
        lora_config = {}
    
    # ğŸš¨ ä¿®å¤ï¼šå…ˆæ£€æŸ¥æ¨¡å‹åˆ‡æ¢ï¼Œå†å¤„ç†LoRAé…ç½®
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦åˆ‡æ¢
    if base_model != current_base_model:
        print(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {base_model}, å½“å‰åŠ è½½æ¨¡å‹: {current_base_model}")
        print(f"ğŸ”„ éœ€è¦åˆ‡æ¢æ¨¡å‹: {current_base_model} -> {base_model}")
        try:
            load_specific_model(base_model)
            print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ° {base_model} æ¨¡å‹")
        except Exception as switch_error:
            print(f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {switch_error}")
            return {
                'success': False,
                'error': f'Failed to switch to {base_model} model: {str(switch_error)}'
            }
    # å†åˆ‡æ¢LoRA
    if lora_config and isinstance(lora_config, dict) and len(lora_config) > 0:
        lora_id = next(iter(lora_config.keys()))
        print(f"ğŸ¨ åˆ‡æ¢LoRA: {lora_id}")
        switch_single_lora(lora_id)
    else:
        print("â„¹ï¸  æ²¡æœ‰LoRAé…ç½®ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ")
    
    # ç”Ÿæˆå›¾åƒ
    try:
        print(f"ğŸ¨ ä½¿ç”¨ {current_base_model} æ¨¡å‹ç”Ÿæˆå›¾åƒ...")
        model_config = BASE_MODELS.get(current_base_model, {})
        model_type = model_config.get("model_type", "unknown")
        
        # ğŸš¨ ä¿®å¤ï¼šç›´æ¥è°ƒç”¨å¯¹åº”çš„ç”Ÿæˆå‡½æ•°ï¼Œé¿å…é€’å½’è°ƒç”¨
        if model_type == "flux":
            print("ğŸ¯ è°ƒç”¨generate_flux_imageså‡½æ•° (FLUX)")
            return generate_flux_images(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                steps=steps,
                cfg_scale=cfg_scale,
                seed=seed,
                num_images=num_images,
                base_model=current_base_model
            )
        elif model_type == "diffusers":
            print("ğŸ¯ è°ƒç”¨generate_diffusers_imageså‡½æ•° (diffusers)")
            return generate_diffusers_images(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                steps=steps,
                cfg_scale=cfg_scale,
                seed=seed,
                num_images=num_images,
                base_model=current_base_model
            )
        else:
            print(f"âŒ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
            return {
                'success': False,
                'error': f'Unknown model type: {model_type}'
            }
        
    except Exception as generation_error:
        print(f"âŒ å›¾åƒç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {generation_error}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return {
            'success': False,
            'error': f'Image generation failed: {str(generation_error)}'
        }

def image_to_image(params: dict) -> list:
    """
    å›¾ç”Ÿå›¾ç”Ÿæˆ - æ–°ç‰ˆæœ¬ï¼Œæ”¯æŒæ¢è„¸åŠŸèƒ½
    
    å¯¹äºçœŸäººæ¨¡å‹ï¼š
    1. å…ˆè¿›è¡Œæ–‡ç”Ÿå›¾
    2. åˆ†æç”¨æˆ·ä¸Šä¼ å›¾ç‰‡çš„äººè„¸
    3. å°†ç”¨æˆ·äººè„¸ç½®æ¢åˆ°ç”Ÿæˆå›¾ç‰‡ä¸­
    4. å¦‚æœæ¢è„¸å¤±è´¥ï¼Œè¿”å›åŸå§‹ç”Ÿæˆå›¾ç‰‡
    
    å¯¹äºåŠ¨æ¼«æ¨¡å‹ï¼š
    ä¿æŒåŸæœ‰çš„å›¾ç”Ÿå›¾é€»è¾‘
    """
    global txt2img_pipe, img2img_pipe, current_base_model
    
    # æ£€æŸ¥å¹¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    base_model = params.get('baseModel', 'realistic')
    
    if txt2img_pipe is None or current_base_model != base_model:
        print(f"ğŸ“ æ¨¡å‹æœªåŠ è½½æˆ–éœ€è¦åˆ‡æ¢ï¼Œå½“å‰: {current_base_model} -> è¯·æ±‚: {base_model}")
        try:
            load_specific_model(base_model)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {base_model}")
        except Exception as model_error:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {model_error}")
            raise ValueError(f"Failed to load model '{base_model}': {str(model_error)}")
    
    # ç¡®ä¿æ¨¡å‹åŠ è½½æˆåŠŸ
    if txt2img_pipe is None:
        raise ValueError("Model failed to load properly")
    
    print(f"âœ… æ¨¡å‹å·²å°±ç»ª: {current_base_model}")
    
    # æå–å‚æ•°
    prompt = params.get('prompt', '')
    negative_prompt = params.get('negativePrompt', '')
    image_data = params.get('image', '')
    width = params.get('width', 512)
    height = params.get('height', 512)
    steps = params.get('steps', 20)
    cfg_scale = params.get('cfgScale', 7.0)
    seed = params.get('seed', -1)
    num_images = params.get('numImages', 1)
    denoising_strength = params.get('denoisingStrength', 0.7)
    lora_config = params.get('lora_config', {})
    
    # ç¡®ä¿promptå’Œnegative_promptä¸ä¸ºNone
    if prompt is None:
        prompt = ""
    if negative_prompt is None:
        negative_prompt = ""
    
    print(f"ğŸ“ å›¾ç”Ÿå›¾å¤„ç† - æç¤ºè¯: {len(prompt)} å­—ç¬¦")
    print(f"ğŸ“ å›¾åƒå°ºå¯¸: {width}x{height}, æ­¥æ•°: {steps}, CFG: {cfg_scale}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
    if lora_config and isinstance(lora_config, dict) and len(lora_config) > 0:
        lora_id = next(iter(lora_config.keys()))
        print(f"ğŸ¨ åˆ‡æ¢LoRA: {lora_id}")
        switch_single_lora(lora_id)
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    try:
        if isinstance(image_data, str):
            source_image = base64_to_image(image_data)
        else:
            raise ValueError("Invalid image data format")
    except Exception as e:
        print(f"âŒ å›¾åƒè§£ç å¤±è´¥: {e}")
        raise ValueError(f"Failed to decode input image: {str(e)}")
    
    # è·å–å½“å‰æ¨¡å‹ç±»å‹
    model_config = BASE_MODELS.get(current_base_model, {})
    model_type = model_config.get("model_type", "unknown")
    
    print(f"ğŸ¯ å½“å‰æ¨¡å‹ç±»å‹: {model_type}")
    
    # ğŸš€ æ–°é€»è¾‘ï¼šçœŸäººæ¨¡å‹ä½¿ç”¨"æ–‡ç”Ÿå›¾+æ¢è„¸"ï¼ŒåŠ¨æ¼«æ¨¡å‹ä½¿ç”¨ä¼ ç»Ÿå›¾ç”Ÿå›¾
    if current_base_model == "realistic":
        print("ğŸ­ ä½¿ç”¨çœŸäººæ¨¡å‹æ¢è„¸æµç¨‹ï¼šæ–‡ç”Ÿå›¾ + æ¢è„¸")
        if not FACE_SWAP_AVAILABLE:
            print("âš ï¸ æ¢è„¸åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œå°†å›é€€åˆ°æ–‡ç”Ÿå›¾+åŸºç¡€å›¾åƒå¤„ç†")
        return _process_realistic_with_face_swap(
            prompt, negative_prompt, source_image, width, height, 
            steps, cfg_scale, seed, num_images, base_model
        )
    else:
        print("ğŸ¨ ä½¿ç”¨ä¼ ç»Ÿå›¾ç”Ÿå›¾æµç¨‹")
        return _process_traditional_img2img(
            prompt, negative_prompt, source_image, width, height, 
            steps, cfg_scale, seed, num_images, denoising_strength, base_model
        )

def _process_realistic_with_face_swap(prompt: str, negative_prompt: str, source_image: Image.Image, 
                                    width: int, height: int, steps: int, cfg_scale: float, 
                                    seed: int, num_images: int, base_model: str) -> list:
    """çœŸäººæ¨¡å‹ï¼šæ–‡ç”Ÿå›¾ + æ¢è„¸æµç¨‹"""
    try:
        print("ğŸ¯ ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨çœŸäººæ¨¡å‹è¿›è¡Œæ–‡ç”Ÿå›¾...")
        
        # ä½¿ç”¨æ–‡ç”Ÿå›¾ç”Ÿæˆåˆå§‹å›¾åƒ
        txt2img_results = text_to_image(
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            steps=steps,
            cfg_scale=cfg_scale,
            seed=seed,
            num_images=num_images,
            base_model=base_model
        )
        
        if not txt2img_results or len(txt2img_results) == 0:
            raise ValueError("Text-to-image generation failed")
        
        print(f"âœ… æ–‡ç”Ÿå›¾å®Œæˆï¼Œç”Ÿæˆäº† {len(txt2img_results)} å¼ å›¾åƒ")
        
        # ç¬¬äºŒæ­¥ï¼šå¯¹æ¯å¼ ç”Ÿæˆçš„å›¾åƒè¿›è¡Œæ¢è„¸å¤„ç†
        print("ğŸ­ ç¬¬äºŒæ­¥ï¼šè¿›è¡Œæ¢è„¸å¤„ç†...")
        
        final_results = []
        
        for i, result_item in enumerate(txt2img_results):
            try:
                print(f"ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(txt2img_results)} å¼ å›¾åƒ...")
                
                # ä»ç»“æœä¸­è·å–å›¾åƒ
                if isinstance(result_item, dict) and 'url' in result_item:
                    # å¦‚æœç»“æœåŒ…å«URLï¼Œéœ€è¦ä¸‹è½½å›¾åƒ
                    import requests
                    response = requests.get(result_item['url'])
                    generated_image = Image.open(io.BytesIO(response.content))
                elif hasattr(result_item, 'images') and len(result_item.images) > 0:
                    generated_image = result_item.images[0]
                else:
                    print(f"âš ï¸ æ— æ³•ä»ç»“æœä¸­æå–å›¾åƒï¼Œè·³è¿‡ç¬¬ {i+1} å¼ ")
                    continue
                
                # æ‰§è¡Œæ¢è„¸ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if FACE_SWAP_AVAILABLE:
                    face_swapped_image, swap_success = process_face_swap_pipeline(
                        generated_image, source_image
                    )
                    
                    if swap_success:
                        print(f"âœ… ç¬¬ {i+1} å¼ å›¾åƒæ¢è„¸æˆåŠŸ")
                    else:
                        print(f"âš ï¸ ç¬¬ {i+1} å¼ å›¾åƒæ¢è„¸å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç”Ÿæˆå›¾åƒ")
                        face_swapped_image = generated_image
                        swap_success = False
                else:
                    print(f"âš ï¸ æ¢è„¸åŠŸèƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹ç”Ÿæˆå›¾åƒ")
                    face_swapped_image = generated_image
                    swap_success = False
                
                # ä¸Šä¼ å¤„ç†åçš„å›¾åƒ
                image_id = str(uuid.uuid4())
                image_bytes = image_to_bytes(face_swapped_image)
                image_url = upload_to_r2(image_bytes, f"{image_id}.jpg")
                
                # æ„å»ºç»“æœ
                result_dict = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed + i if seed != -1 else torch.randint(0, 2**32 - 1, (1,)).item(),
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'createdAt': time.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),
                    'type': 'text-to-image-with-faceswap',  # æ›´æ–°ç±»å‹åç§°
                    'baseModel': base_model,
                    'faceSwapAvailable': FACE_SWAP_AVAILABLE,
                    'faceSwapSuccess': swap_success
                }
                
                final_results.append(result_dict)
                print(f"âœ… ç¬¬ {i+1} å¼ å›¾åƒå¤„ç†å®Œæˆ: {image_url}")
                
            except Exception as e:
                print(f"âŒ ç¬¬ {i+1} å¼ å›¾åƒå¤„ç†å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ‰ æ¢è„¸æµç¨‹å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(final_results)} å¼ å›¾åƒ")
        return final_results
        
    except Exception as e:
        print(f"âŒ æ¢è„¸æµç¨‹å¤±è´¥: {e}")
        # å¦‚æœæ¢è„¸æµç¨‹å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿå›¾ç”Ÿå›¾
        print("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿå›¾ç”Ÿå›¾æµç¨‹...")
        return _process_traditional_img2img(
            prompt, negative_prompt, source_image, width, height, 
            steps, cfg_scale, seed, num_images, 0.7, base_model
        )

def _process_traditional_img2img(prompt: str, negative_prompt: str, source_image: Image.Image,
                               width: int, height: int, steps: int, cfg_scale: float,
                               seed: int, num_images: int, denoising_strength: float, 
                               base_model: str) -> list:
    """ä¼ ç»Ÿå›¾ç”Ÿå›¾æµç¨‹"""
    global img2img_pipe, current_base_model
    
    # ç¡®ä¿img2img_pipeå·²åŠ è½½
    if img2img_pipe is None:
        print("ğŸ”„ img2img_pipeæœªåŠ è½½ï¼Œé‡æ–°åŠ è½½...")
        load_specific_model(current_base_model)
    
    if img2img_pipe is None:
        raise ValueError("Image-to-image pipeline failed to load")
    
    # è°ƒæ•´å›¾åƒå°ºå¯¸
    try:
        source_image = source_image.resize((width, height), Image.Resampling.LANCZOS)
        print(f"âœ… å›¾åƒå°ºå¯¸è°ƒæ•´ä¸º: {width}x{height}")
    except Exception as e:
        print(f"âŒ å›¾åƒå°ºå¯¸è°ƒæ•´å¤±è´¥: {e}")
        raise ValueError(f"Failed to resize image: {str(e)}")
    
    # è·å–å½“å‰æ¨¡å‹ç±»å‹
    model_config = BASE_MODELS.get(current_base_model, {})
    model_type = model_config.get("model_type", "unknown")
    
    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆé€»è¾‘
    results = []
    
    try:
        if model_type == "flux":
            # FLUXæ¨¡å‹ - æ”¯æŒé•¿æç¤ºè¯ï¼Œä¸æ”¯æŒnegative_prompt
            print("ğŸ¯ ä½¿ç”¨FLUXå›¾ç”Ÿå›¾ç®¡é“")
            
            # FLUXæ¨¡å‹å‹ç¼©æç¤ºè¯
            if len(prompt) > 400:  # FLUXå¯ä»¥å¤„ç†æ›´é•¿çš„æç¤ºè¯
                compressed_prompt = compress_prompt_to_77_tokens(prompt, max_tokens=75)
                print(f"ğŸ“ FLUXå›¾ç”Ÿå›¾æç¤ºè¯å‹ç¼©: {len(prompt)} -> {len(compressed_prompt)} å­—ç¬¦")
                prompt = compressed_prompt
            
            for i in range(num_images):
                try:
                    current_seed = seed + i if seed != -1 else torch.randint(0, 2**32 - 1, (1,)).item()
                    current_generator = torch.Generator(device=img2img_pipe.device).manual_seed(current_seed)
                    
                    print(f"ğŸ–¼ï¸ ç”ŸæˆFLUXå›¾ç”Ÿå›¾ {i+1}/{num_images} (ç§å­: {current_seed})")
                    
                    result = img2img_pipe(
                        prompt=prompt,
                        # æ³¨æ„ï¼šFLUXä¸æ”¯æŒnegative_promptå‚æ•°
                        image=source_image,
                        strength=denoising_strength,
                        width=width,
                        height=height,
                        num_inference_steps=steps,
                        guidance_scale=cfg_scale,
                        generator=current_generator,
                        num_images_per_prompt=1
                    )
                    
                    if hasattr(result, 'images') and len(result.images) > 0:
                        image = result.images[0]
                        # ä¸Šä¼ åˆ°R2
                        image_id = str(uuid.uuid4())
                        image_bytes = image_to_bytes(image)
                        image_url = upload_to_r2(image_bytes, f"{image_id}.jpg")
                        
                        # ğŸš¨ ä¿®å¤ï¼šè¿”å›æ ¼å¼ä¸å‰ç«¯æœŸæœ›ä¸€è‡´
                        results.append({
                            'id': image_id,  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
                            'url': image_url,  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
                            'prompt': prompt,
                            'negativePrompt': negative_prompt,
                            'seed': current_seed,
                            'width': width,
                            'height': height,
                            'steps': steps,
                            'cfgScale': cfg_scale,
                            'denoisingStrength': denoising_strength,
                            'createdAt': time.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),
                            'type': 'image-to-image',
                            'baseModel': base_model
                        })
                        print(f"âœ… FLUXå›¾ç”Ÿå›¾ {i+1} ç”ŸæˆæˆåŠŸ: {image_url}")
                    else:
                        print(f"âŒ FLUXå›¾ç”Ÿå›¾ {i+1} ç”Ÿæˆå¤±è´¥ï¼šæ— å›¾åƒç»“æœ")
                        
                except Exception as e:
                    print(f"âŒ FLUXå›¾ç”Ÿå›¾ {i+1} ç”Ÿæˆå¤±è´¥: {e}")
                    continue
                    
        elif model_type == "diffusers":
            # SDXL/æ ‡å‡†diffusersæ¨¡å‹
            print("ğŸ¯ ä½¿ç”¨æ ‡å‡†Diffuserså›¾ç”Ÿå›¾ç®¡é“")
            
            # å‹ç¼©æç¤ºè¯
            if len(prompt) > 200:
                compressed_prompt = compress_prompt_to_77_tokens(prompt, max_tokens=75)
                print(f"ğŸ“ Diffuserså›¾ç”Ÿå›¾æç¤ºè¯å‹ç¼©: {len(prompt)} -> {len(compressed_prompt)} å­—ç¬¦")
                prompt = compressed_prompt
                
            if len(negative_prompt) > 200:
                compressed_negative = compress_prompt_to_77_tokens(negative_prompt, max_tokens=75)
                print(f"ğŸ“ Diffuserså›¾ç”Ÿå›¾è´Ÿé¢æç¤ºè¯å‹ç¼©: {len(negative_prompt)} -> {len(compressed_negative)} å­—ç¬¦")
                negative_prompt = compressed_negative
            
            # ğŸš¨ åŠ¨æ¼«æ¨¡å‹ç¦ç”¨autocasté¿å…LayerNormç²¾åº¦é—®é¢˜
            use_autocast = model_type == "flux"  # åªæœ‰FLUXæ¨¡å‹ä½¿ç”¨autocast
            
            for i in range(num_images):
                try:
                    current_seed = seed + i if seed != -1 else torch.randint(0, 2**32 - 1, (1,)).item()
                    current_generator = torch.Generator(device=img2img_pipe.device).manual_seed(current_seed)
                    
                    print(f"ğŸ–¼ï¸ ç”ŸæˆDiffuserså›¾ç”Ÿå›¾ {i+1}/{num_images} (ç§å­: {current_seed})")
                    
                    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨autocast
                    if use_autocast:
                        with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                            result = img2img_pipe(
                                prompt=prompt,
                                negative_prompt=negative_prompt,
                                image=source_image,
                                strength=denoising_strength,
                                width=width,
                                height=height,
                                num_inference_steps=steps,
                                guidance_scale=cfg_scale,
                                generator=current_generator,
                                num_images_per_prompt=1
                            )
                    else:
                        # åŠ¨æ¼«æ¨¡å‹ä¸ä½¿ç”¨autocast
                        print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹å›¾ç”Ÿå›¾: ä½¿ç”¨float32ç²¾åº¦")
                        result = img2img_pipe(
                            prompt=prompt,
                            negative_prompt=negative_prompt,
                            image=source_image,
                            strength=denoising_strength,
                            width=width,
                            height=height,
                            num_inference_steps=steps,
                            guidance_scale=cfg_scale,
                            generator=current_generator,
                            num_images_per_prompt=1
                        )
                    
                    if hasattr(result, 'images') and len(result.images) > 0:
                        image = result.images[0]
                        # ä¸Šä¼ åˆ°R2
                        image_id = str(uuid.uuid4())
                        image_bytes = image_to_bytes(image)
                        image_url = upload_to_r2(image_bytes, f"{image_id}.jpg")
                        
                        # ğŸš¨ ä¿®å¤ï¼šè¿”å›æ ¼å¼ä¸å‰ç«¯æœŸæœ›ä¸€è‡´
                        results.append({
                            'id': image_id,  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
                            'url': image_url,  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
                            'prompt': prompt,
                            'negativePrompt': negative_prompt,
                            'seed': current_seed,
                            'width': width,
                            'height': height,
                            'steps': steps,
                            'cfgScale': cfg_scale,
                            'denoisingStrength': denoising_strength,
                            'createdAt': time.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),
                            'type': 'image-to-image',
                            'baseModel': base_model
                        })
                        print(f"âœ… Diffuserså›¾ç”Ÿå›¾ {i+1} ç”ŸæˆæˆåŠŸ: {image_url}")
                    else:
                        print(f"âŒ Diffuserså›¾ç”Ÿå›¾ {i+1} ç”Ÿæˆå¤±è´¥ï¼šæ— å›¾åƒç»“æœ")
                        
                except Exception as e:
                    print(f"âŒ Diffuserså›¾ç”Ÿå›¾ {i+1} ç”Ÿæˆå¤±è´¥: {e}")
                    continue
        else:
            raise ValueError(f"Unsupported model type for image-to-image: {model_type}")
            
    except Exception as e:
        print(f"âŒ å›¾ç”Ÿå›¾ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise RuntimeError(f"Image-to-image generation failed: {str(e)}")
    
    if len(results) == 0:
        raise RuntimeError("No images were generated successfully")
    
    print(f"ğŸ‰ å›¾ç”Ÿå›¾å®Œæˆ: æˆåŠŸç”Ÿæˆ {len(results)}/{num_images} å¼ å›¾åƒ")
    return results

def get_available_loras() -> dict:
    """è·å–å¯ç”¨çš„LoRAæ¨¡å‹åˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬ï¼ˆå‰ç«¯é™æ€æ˜¾ç¤ºï¼‰"""
    # å‰ç«¯å·²ç»æœ‰é™æ€åˆ—è¡¨ï¼Œè¿™é‡Œåªè¿”å›åŸºæœ¬ä¿¡æ¯
    return {
        "message": "å‰ç«¯ä½¿ç”¨é™æ€LoRAåˆ—è¡¨ï¼Œåç«¯åŠ¨æ€æœç´¢æ–‡ä»¶",
        "search_paths": LORA_SEARCH_PATHS,
        "current_selected": current_selected_lora,
        "current_base_model": current_base_model
    }

def get_loras_by_base_model() -> dict:
    """è·å–æŒ‰åŸºç¡€æ¨¡å‹åˆ†ç»„çš„LoRAåˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬"""
    return {
        "realistic": [
            {"id": "flux_nsfw", "name": "FLUX NSFW", "description": "NSFWçœŸäººå†…å®¹ç”Ÿæˆæ¨¡å‹"},
            {"id": "chastity_cage", "name": "Chastity Cage", "description": "è´æ“ç¬¼ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "dynamic_penis", "name": "Dynamic Penis", "description": "åŠ¨æ€ç”·æ€§è§£å‰–ç”Ÿæˆ"},
            {"id": "masturbation", "name": "Masturbation", "description": "è‡ªæ…°ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "puppy_mask", "name": "Puppy Mask", "description": "å°ç‹—é¢å…·ä¸»é¢˜å†…å®¹"},
            {"id": "butt_and_feet", "name": "Butt and Feet", "description": "è‡€éƒ¨å’Œè¶³éƒ¨ä¸»é¢˜å†…å®¹"},
            {"id": "cumshots", "name": "Cumshots", "description": "å°„ç²¾ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "uncutpenis", "name": "Uncut Penis", "description": "æœªå‰²åŒ…çš®ä¸»é¢˜å†…å®¹"},
            {"id": "doggystyle", "name": "Doggystyle", "description": "åå…¥å¼ä¸»é¢˜å†…å®¹"},
            {"id": "fisting", "name": "Fisting", "description": "æ‹³äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "on_off", "name": "On Off", "description": "ç©¿è¡£/è„±è¡£å¯¹æ¯”å†…å®¹"},
            {"id": "blowjob", "name": "Blowjob", "description": "å£äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "cum_on_face", "name": "Cum on Face", "description": "é¢œå°„ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "anal_sex", "name": "Anal Sex", "description": "è‚›äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"}
        ],
        "anime": [
            {"id": "gayporn", "name": "Gayporn", "description": "ç”·åŒåŠ¨æ¼«é£æ ¼å†…å®¹ç”Ÿæˆ"},
            {"id": "blowjob_handjob", "name": "Blowjob Handjob", "description": "å£äº¤å’Œæ‰‹äº¤åŠ¨æ¼«å†…å®¹"},
            {"id": "furry", "name": "Furry", "description": "å…½äººé£æ ¼åŠ¨æ¼«å†…å®¹"},
            {"id": "sex_slave", "name": "Sex Slave", "description": "æ€§å¥´ä¸»é¢˜åŠ¨æ¼«å†…å®¹"},
            {"id": "comic", "name": "Comic", "description": "æ¼«ç”»é£æ ¼å†…å®¹ç”Ÿæˆ"},
            {"id": "glory_wall", "name": "Glory Wall", "description": "è£è€€å¢™ä¸»é¢˜å†…å®¹"},
            {"id": "multiple_views", "name": "Multiple Views", "description": "å¤šè§†è§’åŠ¨æ¼«å†…å®¹"},
            {"id": "pet_play", "name": "Pet Play", "description": "å® ç‰©æ‰®æ¼”ä¸»é¢˜å†…å®¹"}
        ],
        # ğŸš¨ ä¿®å¤ï¼šç§»é™¤å›ºå®šçš„current_selectedï¼Œè®©å‰ç«¯å†³å®šåˆå§‹é€‰æ‹©
        "message": "LoRAåˆ—è¡¨è·å–æˆåŠŸ - é™æ€é…ç½®ç‰ˆæœ¬"
    }

def switch_single_lora(lora_id: str) -> bool:
    """åˆ‡æ¢åˆ°å•ä¸ªLoRAï¼ˆå½»åº•æ— å¤šLoRA/adapter_nameæ®‹ç•™ï¼ŒåŠ¨æ¼«æ¨¡å‹å…¼å®¹ï¼‰"""
    global txt2img_pipe, img2img_pipe, current_lora_config, current_selected_lora, current_base_model

    if txt2img_pipe is None:
        raise ValueError("No pipeline loaded, cannot switch LoRA")

    # åŠ¨æ€æœç´¢LoRAæ–‡ä»¶
    lora_path = find_lora_file(lora_id, current_base_model)
    if not lora_path:
        raise ValueError(f"LoRAæ–‡ä»¶æœªæ‰¾åˆ°: {lora_id}")

    # å¦‚æœå·²ç»æ˜¯å½“å‰LoRAï¼Œç›´æ¥è¿”å›
    if lora_id == current_selected_lora:
        print(f"LoRA {lora_id} å·²ç»åŠ è½½ - è·³è¿‡åˆ‡æ¢")
        return True

    try:
        print(f"ğŸ”„ åˆ‡æ¢LoRAåˆ°: {lora_id}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {lora_path}")
        # å¸è½½å½“å‰LoRAï¼ˆå¦‚æœ‰ï¼‰
        if hasattr(txt2img_pipe, 'unload_lora_weights'):
            try:
                txt2img_pipe.unload_lora_weights()
                print("ğŸ§¹ å·²å¸è½½ä¹‹å‰çš„LoRA (txt2img)")
            except Exception as e:
                print(f"âš ï¸  å¸è½½txt2img LoRAæ—¶å‡ºé”™: {e}")
        # åŠ è½½æ–°LoRA
        txt2img_pipe.load_lora_weights(lora_path)
        print("âœ… æ–°LoRAåŠ è½½æˆåŠŸ (txt2img)")
        # img2imgç®¡é“åŒæ­¥
        if img2img_pipe and hasattr(img2img_pipe, 'load_lora_weights'):
            try:
                if hasattr(img2img_pipe, 'unload_lora_weights'):
                    img2img_pipe.unload_lora_weights()
                img2img_pipe.load_lora_weights(lora_path)
                print("âœ… img2imgç®¡é“LoRAåŒæ­¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  img2imgç®¡é“LoRAåŒæ­¥å¤±è´¥: {e}")
        # æ›´æ–°å½“å‰LoRAé…ç½®
        current_lora_config = {lora_id: 1.0}
        current_selected_lora = lora_id
        print(f"ğŸ‰ æˆåŠŸåˆ‡æ¢åˆ°LoRA: {lora_id}")
        return True
    except Exception as e:
        print(f"âŒ LoRAåˆ‡æ¢å¤±è´¥: {str(e)}")
        # å¼ºåˆ¶æ¸…ç†ï¼Œé˜²æ­¢åç»­æ­»é”
        if hasattr(txt2img_pipe, 'unload_lora_weights'):
            try:
                txt2img_pipe.unload_lora_weights()
            except:
                pass
        if img2img_pipe and hasattr(img2img_pipe, 'unload_lora_weights'):
            try:
                img2img_pipe.unload_lora_weights()
            except:
                pass
        raise RuntimeError(f"LoRAåˆ‡æ¢å¤±è´¥: {str(e)}")

def switch_base_model(base_model_type: str) -> bool:
    """åˆ‡æ¢åŸºç¡€æ¨¡å‹"""
    global current_base_model
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    if current_base_model == base_model_type:
        print(f"Base model {BASE_MODELS[base_model_type]['name']} is already loaded")
        return True
    
    try:
        print(f"Switching base model from {BASE_MODELS[current_base_model]['name']} to {BASE_MODELS[base_model_type]['name']}")
        
        # é‡Šæ”¾å½“å‰æ¨¡å‹å†…å­˜
        global txt2img_pipe, img2img_pipe
        if txt2img_pipe is not None:
            del txt2img_pipe
        if img2img_pipe is not None:
            del img2img_pipe
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # åŠ è½½æ–°çš„åŸºç¡€æ¨¡å‹
        load_specific_model(base_model_type)
        
        print(f"Successfully switched to {BASE_MODELS[base_model_type]['name']}")
        return True
        
    except Exception as e:
        print(f"Failed to switch base model: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„æ¨¡å‹
        try:
            load_specific_model(current_base_model)
            print(f"Recovered to previous model: {BASE_MODELS[current_base_model]['name']}")
        except Exception as recovery_error:
            print(f"Failed to recover base model: {recovery_error}")
        raise RuntimeError(f"Failed to switch base model: {str(e)}")

def handler(job):
    """RunPod å¤„ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        job_input = job['input']
        task_type = job_input.get('task_type')
        
        if task_type == 'get-loras':
            # è·å–å¯ç”¨LoRAåˆ—è¡¨ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            available_loras = get_available_loras()
            return {
                'success': True,
                'data': {
                    'loras': available_loras,
                    'current_config': current_lora_config
                }
            }
            
        elif task_type == 'get-loras-by-model':
            # è·å–æŒ‰åŸºç¡€æ¨¡å‹åˆ†ç»„çš„LoRAåˆ—è¡¨ï¼ˆæ–°çš„å•é€‰UIï¼‰
            loras_by_model = get_loras_by_base_model()
            return {
                'success': True,
                'data': loras_by_model
            }
            
        elif task_type == 'switch-single-lora':
            # åˆ‡æ¢å•ä¸ªLoRAæ¨¡å‹ï¼ˆæ–°çš„å•é€‰æ¨¡å¼ï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            success = switch_single_lora(lora_id)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_selected_lora': current_selected_lora,
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
            
        elif task_type == 'switch-lora':
            # åˆ‡æ¢LoRAæ¨¡å‹ï¼ˆå•ä¸ªLoRAå…¼å®¹æ€§æ”¯æŒï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            # å…¼å®¹å•LoRAåˆ‡æ¢
            single_lora_config = {lora_id: 1.0}
            success = switch_single_lora(lora_id)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
        
        elif task_type == 'load-loras':
            # åŠ è½½å¤šä¸ªLoRAæ¨¡å‹é…ç½®
            lora_config = job_input.get('lora_config', {})
            if not lora_config:
                return {
                    'success': False,
                    'error': 'lora_config is required'
                }
            
            success = switch_single_lora(next(iter(lora_config.keys())))
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Loaded {len([k for k, v in lora_config.items() if v > 0])} LoRA models'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': 'Failed to load LoRA configuration'
                }
        
        elif task_type == 'text-to-image':
            # æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆ
            print("ğŸ“ Processing text-to-image request...")
            
            # æå–å‚æ•°
            prompt = job_input.get('prompt', '')
            negative_prompt = job_input.get('negativePrompt', '') 
            width = job_input.get('width', 1024)
            height = job_input.get('height', 1024)
            steps = job_input.get('steps', 25)
            cfg_scale = job_input.get('cfgScale', 5.0)
            seed = job_input.get('seed', -1)
            num_images = job_input.get('numImages', 1)
            base_model = job_input.get('baseModel', 'realistic')
            lora_config = job_input.get('lora_config', {})
            
            # ğŸš¨ ä¿®å¤ï¼šç›´æ¥è°ƒç”¨text_to_imageï¼Œé¿å…é‡å¤å¤„ç†
            print("ğŸ¯ Handlerç›´æ¥è°ƒç”¨text_to_imageå‡½æ•°")
            results = text_to_image(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                steps=steps,
                cfg_scale=cfg_scale,
                seed=seed,
                num_images=num_images,
                base_model=base_model,
                lora_config=lora_config
            )
            
            return {
                'success': True,
                'data': results
            }
            
        elif task_type == 'image-to-image':
            # å›¾åƒè½¬å›¾åƒç”Ÿæˆ - æ”¯æŒå•LoRA
            print("ğŸ“ Processing image-to-image request...")
            params = {
                'prompt': job_input.get('prompt', ''),
                'negativePrompt': job_input.get('negativePrompt', ''),
                'image': job_input.get('image', ''),
                'width': job_input.get('width', 512),
                'height': job_input.get('height', 512),
                'steps': job_input.get('steps', 20),
                'cfgScale': job_input.get('cfgScale', 7.0),
                'seed': job_input.get('seed', -1),
                'numImages': job_input.get('numImages', 1),
                'denoisingStrength': job_input.get('denoisingStrength', 0.7),
                'baseModel': job_input.get('baseModel', 'realistic'),
                'lora_config': job_input.get('lora_config', {})
            }
            # å…ˆåˆ‡æ¢æ¨¡å‹
            base_model = params.get('baseModel', 'realistic')
            if img2img_pipe is None or current_base_model != base_model:
                print(f"ğŸ“ Handlerè‡ªåŠ¨åˆ‡æ¢æ¨¡å‹: {current_base_model} -> {base_model}")
                load_specific_model(base_model)
            # å†åˆ‡æ¢LoRA
            requested_lora_config = params.get('lora_config', current_lora_config)
            if requested_lora_config and isinstance(requested_lora_config, dict) and len(requested_lora_config) > 0:
                lora_id = next(iter(requested_lora_config.keys()))
                print(f"Auto-loading LoRA config for generation: {lora_id}")
                switch_single_lora(lora_id)
            results = image_to_image(params)
            return {
                'success': True,
                'data': results
            }
            
        elif task_type == 'switch-base-model':
            # åˆ‡æ¢åŸºç¡€æ¨¡å‹
            base_model_type = job_input.get('base_model_type')
            if not base_model_type:
                return {
                    'success': False,
                    'error': 'base_model_type is required'
                }
            
            success = switch_base_model(base_model_type)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_base_model': current_base_model
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {base_model_type}'
                }
        
        else:
            return {
                'success': False,
                'error': f'Unknown task type: {task_type}'
            }
            
    except Exception as e:
        print(f"Handler error: {str(e)}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }

# Note: The serverless worker will be started by start_debug.py
# This allows for better debugging and health checks before startup 
# This allows for better debugging and health checks before startup 

# ç®€åŒ–çš„LoRAé…ç½® - å‰ç«¯é™æ€æ˜¾ç¤ºï¼Œåç«¯åŠ¨æ€æœç´¢æ–‡ä»¶
LORA_SEARCH_PATHS = {
    "realistic": [
        "/runpod-volume/lora",
        "/runpod-volume/lora/flux_nsfw",
        "/runpod-volume/lora/realistic"
    ],
    "anime": [
        "/runpod-volume/cartoon/lora",
        "/runpod-volume/anime/lora",
        "/runpod-volume/cartoon"
    ]
}

# LoRAåç§°åˆ°å¯èƒ½æ–‡ä»¶åçš„æ˜ å°„
LORA_FILE_PATTERNS = {
    # çœŸäººé£æ ¼LoRA
    "flux_nsfw": ["flux_nsfw", "flux_nsfw.safetensors"],
    "chastity_cage": ["Chastity_Cage.safetensors", "chastity_cage.safetensors", "ChastityCase.safetensors"],
    "dynamic_penis": ["DynamicPenis.safetensors", "dynamic_penis.safetensors"],
    "masturbation": ["Masturbation.safetensors", "masturbation.safetensors"],
    "puppy_mask": ["Puppy_mask.safetensors", "puppy_mask.safetensors", "PuppyMask.safetensors"],
    "butt_and_feet": ["butt-and-feet.safetensors", "butt_and_feet.safetensors", "ButtAndFeet.safetensors"],
    "cumshots": ["cumshots.safetensors", "Cumshots.safetensors"],
    "uncutpenis": ["uncutpenis.safetensors", "UncutPenis.safetensors", "uncut_penis.safetensors"],
    "doggystyle": ["Doggystyle.safetensors", "doggystyle.safetensors", "doggy_style.safetensors"],
    "fisting": ["Fisting.safetensors", "fisting.safetensors"],
    "on_off": ["OnOff.safetensors", "on_off.safetensors", "onoff.safetensors"],
    "blowjob": ["blowjob.safetensors", "Blowjob.safetensors", "blow_job.safetensors"],
    "cum_on_face": ["cumonface.safetensors", "cum_on_face.safetensors", "CumOnFace.safetensors"],
    "anal_sex": ["Anal_sex.safetensors", "anal_sex.safetensors", "AnalSex.safetensors", "analsex.safetensors"],
    
    # åŠ¨æ¼«é£æ ¼LoRA - ç§»é™¤anime_nsfwï¼Œå› ä¸ºå®ƒç°åœ¨æ˜¯åº•å±‚æ¨¡å‹
    "gayporn": ["Gayporn.safetensors", "gayporn.safetensors", "GayPorn.safetensors"],
    "blowjob_handjob": ["Blowjob_Handjob.safetensors", "blowjob_handjob.safetensors", "BlowjobHandjob.safetensors"],
    "furry": ["Furry.safetensors", "furry.safetensors", "FURRY.safetensors"],
    "sex_slave": ["Sex_slave.safetensors", "sex_slave.safetensors", "SexSlave.safetensors"],
    "comic": ["comic.safetensors", "Comic.safetensors", "COMIC.safetensors"],
    "glory_wall": ["glory_wall.safetensors", "Glory_wall.safetensors", "GloryWall.safetensors"],
    "multiple_views": ["multiple_views.safetensors", "Multiple_views.safetensors", "MultipleViews.safetensors"],
    "pet_play": ["pet_play.safetensors", "Pet_play.safetensors", "PetPlay.safetensors"]
}

def find_lora_file(lora_id: str, base_model: str) -> str:
    """åŠ¨æ€æœç´¢LoRAæ–‡ä»¶è·¯å¾„ - å¢å¼ºæœç´¢é€»è¾‘"""
    search_paths = LORA_SEARCH_PATHS.get(base_model, [])
    file_patterns = LORA_FILE_PATTERNS.get(lora_id, [lora_id])
    
    print(f"ğŸ” æœç´¢LoRAæ–‡ä»¶: {lora_id} (æ¨¡å‹: {base_model})")
    
    for base_path in search_paths:
        if not os.path.exists(base_path):
            print(f"  âŒ è·¯å¾„ä¸å­˜åœ¨: {base_path}")
            continue
            
        print(f"  ğŸ“ æœç´¢ç›®å½•: {base_path}")
        
        # å°è¯•ç²¾ç¡®åŒ¹é…
        for pattern in file_patterns:
            full_path = os.path.join(base_path, pattern)
            if os.path.exists(full_path):
                print(f"  âœ… æ‰¾åˆ°æ–‡ä»¶: {full_path}")
                return full_path
        
        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å«lora_idï¼‰
        try:
            for filename in os.listdir(base_path):
                if filename.endswith(('.safetensors', '.safetensor', '.ckpt', '.pt')):
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«lora_idçš„å…³é”®è¯
                    name_lower = filename.lower()
                    lora_lower = lora_id.lower().replace('_', '').replace('-', '')
                    
                    if lora_lower in name_lower.replace('_', '').replace('-', ''):
                        full_path = os.path.join(base_path, filename)
                        print(f"  âœ… æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°: {full_path}")
                        return full_path
        except Exception as e:
            print(f"  âŒ æœç´¢é”™è¯¯: {e}")
    
    print(f"  âŒ æœªæ‰¾åˆ°LoRAæ–‡ä»¶: {lora_id}")
    return None

# ç§»é™¤å¤æ‚çš„åŠ¨æ€æ‰«æï¼Œä½¿ç”¨ç®€å•çš„é™æ€é…ç½®
# AVAILABLE_LORAS = None
# LORAS_LAST_SCAN = 0
# LORAS_CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜
# åŠ¨æ¼«æ¨¡å‹æ–°å¢LoRAåˆ—è¡¨
ANIME_ADDITIONAL_LORAS = {
    "blowjob_handjob": "/runpod-volume/cartoon/lora/Blowjob_Handjob.safetensors",
    "furry": "/runpod-volume/cartoon/lora/Furry.safetensors", 
    "sex_slave": "/runpod-volume/cartoon/lora/Sex_slave.safetensors",
    "comic": "/runpod-volume/cartoon/lora/comic.safetensors",
    "glory_wall": "/runpod-volume/cartoon/lora/glory_wall.safetensors",
    "multiple_views": "/runpod-volume/cartoon/lora/multiple_views.safetensors",
    "pet_play": "/runpod-volume/cartoon/lora/pet_play.safetensors"
}

def completely_clear_lora_adapters():
    """å®Œå…¨æ¸…ç†æ‰€æœ‰LoRAé€‚é…å™¨ - æœ€å½»åº•çš„æ¸…ç†æ–¹æ³•"""
    global txt2img_pipe, img2img_pipe
    
    print("ğŸ§¹ å¼€å§‹å®Œå…¨æ¸…ç†LoRAé€‚é…å™¨...")
    
    # æ¸…ç†ç®¡é“åˆ—è¡¨
    pipelines = [txt2img_pipe]
    if img2img_pipe:
        pipelines.append(img2img_pipe)
    
    for i, pipe in enumerate(pipelines):
        if pipe is None:
            continue
        
        pipeline_name = "txt2img" if i == 0 else "img2img"
        
        try:
            # ç¬¬1å±‚ï¼šæ ‡å‡†çš„unload_lora_weightsæ–¹æ³•
            if hasattr(pipe, 'unload_lora_weights'):
                pipe.unload_lora_weights()
                print("âœ… æ ‡å‡†unload_lora_weightså®Œæˆ")
            
            # ç¬¬2å±‚ï¼šæ¸…ç†UNetä¸­çš„ç‰¹å®šLoRAé…ç½®
            if hasattr(pipe, 'unet') and pipe.unet:
                unet = pipe.unet
                
                # æ¸…ç†_hf_peft_config_loaded
                if hasattr(unet, '_hf_peft_config_loaded'):
                    delattr(unet, '_hf_peft_config_loaded')
                    print("ğŸ”§ æ¸…ç†UNet._hf_peft_config_loaded")
                
                # ğŸš¨ æ–°å¢ï¼šæ¸…ç†PEFTç›¸å…³çš„é€‚é…å™¨ç¼“å­˜
                if hasattr(unet, 'peft_config') and unet.peft_config:
                    unet.peft_config.clear()
                    print("ğŸ”§ æ¸…ç†UNet.peft_config")
                
                # ğŸš¨ æ–°å¢ï¼šæ¸…ç†é€‚é…å™¨åç§°ç¼“å­˜
                if hasattr(unet, '_lora_adapters'):
                    unet._lora_adapters.clear()
                    print("ğŸ”§ æ¸…ç†UNet._lora_adapters")
                
                # ğŸš¨ æ–°å¢ï¼šå¼ºåˆ¶æ¸…ç†æ‰€æœ‰å¯èƒ½çš„é€‚é…å™¨æ®‹ç•™
                adapter_attrs = ['_lora_adapters', 'peft_config', '_adapter_names', '_active_adapters']
                for attr in adapter_attrs:
                    if hasattr(unet, attr):
                        try:
                            attr_obj = getattr(unet, attr)
                            if hasattr(attr_obj, 'clear'):
                                attr_obj.clear()
                            elif isinstance(attr_obj, dict):
                                attr_obj.clear()
                            elif isinstance(attr_obj, list):
                                attr_obj.clear()
                            print(f"ğŸ”§ æ¸…ç†UNet.{attr}")
                        except Exception as attr_error:
                            print(f"âš ï¸  æ¸…ç†UNet.{attr}æ—¶å‡ºé”™: {attr_error}")
                
                # ğŸš¨ æ–°å¢ï¼šæ¸…ç†Text Encoderé€‚é…å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if hasattr(pipe, 'text_encoder') and pipe.text_encoder:
                    text_encoder = pipe.text_encoder
                    for attr in adapter_attrs:
                        if hasattr(text_encoder, attr):
                            try:
                                attr_obj = getattr(text_encoder, attr)
                                if hasattr(attr_obj, 'clear'):
                                    attr_obj.clear()
                                elif isinstance(attr_obj, dict):
                                    attr_obj.clear()
                                elif isinstance(attr_obj, list):
                                    attr_obj.clear()
                                print(f"ğŸ”§ æ¸…ç†TextEncoder.{attr}")
                            except Exception as attr_error:
                                print(f"âš ï¸  æ¸…ç†TextEncoder.{attr}æ—¶å‡ºé”™: {attr_error}")
                
                # ğŸš¨ æ–°å¢ï¼šå¦‚æœæœ‰ç¬¬äºŒä¸ªText Encoderï¼ˆSDXLï¼‰
                if hasattr(pipe, 'text_encoder_2') and pipe.text_encoder_2:
                    text_encoder_2 = pipe.text_encoder_2
                    for attr in adapter_attrs:
                        if hasattr(text_encoder_2, attr):
                            try:
                                attr_obj = getattr(text_encoder_2, attr)
                                if hasattr(attr_obj, 'clear'):
                                    attr_obj.clear()
                                elif isinstance(attr_obj, dict):
                                    attr_obj.clear()
                                elif isinstance(attr_obj, list):
                                    attr_obj.clear()
                                print(f"ğŸ”§ æ¸…ç†TextEncoder2.{attr}")
                            except Exception as attr_error:
                                print(f"âš ï¸  æ¸…ç†TextEncoder2.{attr}æ—¶å‡ºé”™: {attr_error}")
                
        except Exception as pipeline_error:
            print(f"âš ï¸  æ¸…ç†{pipeline_name}ç®¡é“æ—¶å‡ºé”™: {pipeline_error}")
    
    # ç¬¬3å±‚ï¼šå¼ºåˆ¶æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")
    
    print("âœ… LoRAé€‚é…å™¨å®Œå…¨æ¸…ç†å®Œæˆ")

def compress_prompt_to_77_tokens(prompt: str, max_tokens: int = 75) -> str:
    """
    æ™ºèƒ½å‹ç¼©promptåˆ°æŒ‡å®štokenæ•°é‡ä»¥å†… - ä½¿ç”¨CLIPTokenizerçœŸå®è®¡æ•°
    """
    import re
    # åˆå§‹åŒ–tokenizerï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
    global _clip_tokenizer
    if '_clip_tokenizer' not in globals() or _clip_tokenizer is None:
        _clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
    tokenizer = _clip_tokenizer
    # å…ˆç²—ç•¥åˆ†è¯
    token_pattern = r'\w+|[^\w\s]'
    words = prompt.split()
    # å¾ªç¯æˆªæ–­ï¼Œç›´åˆ°tokenizerè®¡æ•°<=max_tokens
    for end in range(len(words), 0, -1):
        candidate = ' '.join(words[:end])
        token_count = len(tokenizer(candidate, add_special_tokens=False)["input_ids"])
        if token_count <= max_tokens:
            print(f"âœ… æ™ºèƒ½å‹ç¼©å®Œæˆ: {token_count} tokens (ç›®æ ‡: {max_tokens})")
            print(f"   å‹ç¼©å†…å®¹: '{candidate[:100]}{'...' if len(candidate) > 100 else ''}'")
            return candidate
    # å¦‚æœå…¨éƒ¨éƒ½è¶…ï¼Œè¿”å›æœ€çŸ­
    return ' '.join(words[:1])
