import os
import base64
import io
import time
import traceback
import uuid
from datetime import datetime
from typing import Optional, Dict, List, Tuple

import torch
import numpy as np
from PIL import Image
import runpod

# AIå’Œå›¾åƒå¤„ç†åº“
from diffusers import (
    FluxPipeline, 
    StableDiffusionPipeline, 
    StableDiffusionImg2ImgPipeline,
    DPMSolverMultistepScheduler,
    EulerDiscreteScheduler
)

from transformers import T5EncoderModel, CLIPTextModel, CLIPTokenizer
import boto3
from botocore.exceptions import ClientError

# ğŸ”§ å…¼å®¹æ€§ä¿®å¤ï¼šæ·»åŠ å›é€€çš„torch.get_default_deviceå‡½æ•°
if not hasattr(torch, 'get_default_device'):
    def get_default_device():
        if torch.cuda.is_available():
            return torch.device('cuda')
        else:
            return torch.device('cpu')
    torch.get_default_device = get_default_device
    print("âœ“ Added fallback torch.get_default_device() function")

# å¯¼å…¥compelç”¨äºå¤„ç†é•¿æç¤ºè¯
try:
    from compel import Compel
    COMPEL_AVAILABLE = True
    print("âœ“ Compel library loaded for long prompt support")
except ImportError:
    COMPEL_AVAILABLE = False
    print("âš ï¸  Compel library not available - long prompt support limited")

# æ·»åŠ å¯åŠ¨æ—¥å¿—
print("=== Starting AI Image Generation Backend ===")
print(f"Python version: {sys.version}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

# éªŒè¯å…³é”®ç¯å¢ƒå˜é‡
required_env_vars = [
    "CLOUDFLARE_R2_ACCESS_KEY",
    "CLOUDFLARE_R2_SECRET_KEY", 
    "CLOUDFLARE_R2_BUCKET",
    "CLOUDFLARE_R2_ENDPOINT"
]

missing_vars = []
for var in required_env_vars:
    if not os.getenv(var):
        missing_vars.append(var)

if missing_vars:
    print(f"WARNING: Missing environment variables: {missing_vars}")
    print("Container will start but R2 upload may fail")

# ç¯å¢ƒå˜é‡
CLOUDFLARE_R2_ACCESS_KEY = os.getenv("CLOUDFLARE_R2_ACCESS_KEY")
CLOUDFLARE_R2_SECRET_KEY = os.getenv("CLOUDFLARE_R2_SECRET_KEY") 
CLOUDFLARE_R2_BUCKET = os.getenv("CLOUDFLARE_R2_BUCKET")
CLOUDFLARE_R2_ENDPOINT = os.getenv("CLOUDFLARE_R2_ENDPOINT")
CLOUDFLARE_R2_PUBLIC_DOMAIN = os.getenv("CLOUDFLARE_R2_PUBLIC_DOMAIN")  # å¯é€‰ï¼šè‡ªå®šä¹‰å…¬å…±åŸŸå

# æ¨¡å‹è·¯å¾„
FLUX_BASE_PATH = "/runpod-volume/flux_base"
FLUX_LORA_BASE_PATH = "/runpod-volume/lora"

# é…ç½®åŸºç¡€æ¨¡å‹ç±»å‹å’Œè·¯å¾„
BASE_MODELS = {
    "realistic": {
        "name": "çœŸäººé£æ ¼",
        "model_path": "/runpod-volume/flux_base",
        "model_type": "flux", 
        "lora_path": "/runpod-volume/lora/flux_nsfw/flux_lustly-ai_v1.safetensors",
        "lora_id": "flux_nsfw"
    },
    "anime": {
        "name": "åŠ¨æ¼«é£æ ¼", 
        "model_path": "/runpod-volume/cartoon/waiNSFWIllustrious_v130.safetensors",
        "model_type": "diffusers",
        "lora_path": "/runpod-volume/cartoon/lora/Gayporn.safetensor",
        "lora_id": "gayporn"
    }
}

# é»˜è®¤LoRAé…ç½® - æ ¹æ®åŸºç¡€æ¨¡å‹ï¼ˆå•é€‰æ¨¡å¼ï¼‰
DEFAULT_LORA_CONFIG = {
    "flux_nsfw": 1.0  # é»˜è®¤ä½¿ç”¨FLUX NSFW
}

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
txt2img_pipe = None
img2img_pipe = None
current_lora_config = DEFAULT_LORA_CONFIG.copy()
current_base_model = None  # åˆå§‹åŒ–æ—¶ä¸é¢„åŠ è½½ä»»ä½•æ¨¡å‹
device_mapping_enabled = False  # Track if device mapping is used
current_selected_lora = "flux_nsfw"  # å½“å‰é€‰æ‹©çš„å•ä¸ªLoRAï¼ˆç”¨äºçœŸäººé£æ ¼ï¼‰

# å…¨å±€å˜é‡å­˜å‚¨compelå¤„ç†å™¨
compel_proc = None
compel_proc_neg = None

# æ”¯æŒçš„LoRAæ¨¡å‹åˆ—è¡¨ - æ›´æ–°ä¸ºæ”¯æŒä¸åŒåŸºç¡€æ¨¡å‹
AVAILABLE_LORAS = None
LORAS_LAST_SCAN = 0
LORAS_CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜

# åˆå§‹åŒ– Cloudflare R2 å®¢æˆ·ç«¯
r2_client = None
if all([CLOUDFLARE_R2_ACCESS_KEY, CLOUDFLARE_R2_SECRET_KEY, CLOUDFLARE_R2_BUCKET, CLOUDFLARE_R2_ENDPOINT]):
    try:
        r2_client = boto3.client(
            's3',
            endpoint_url=CLOUDFLARE_R2_ENDPOINT,
            aws_access_key_id=CLOUDFLARE_R2_ACCESS_KEY,
            aws_secret_access_key=CLOUDFLARE_R2_SECRET_KEY,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        print("âœ“ R2 client initialized successfully")
    except Exception as e:
        print(f"âœ— Failed to initialize R2 client: {e}")
        r2_client = None
else:
    print("âœ— R2 configuration incomplete - R2 upload will be disabled")

def get_device():
    """è·å–è®¾å¤‡ï¼Œå…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬"""
    if torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def load_models():
    """æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œä¸é¢„çƒ­"""
    global txt2img_pipe, img2img_pipe, current_base_model
    
    print("âœ“ æ¨¡å‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå°†æŒ‰éœ€åŠ è½½æ¨¡å‹")
    print(f"ğŸ“ æ”¯æŒçš„æ¨¡å‹ç±»å‹: {list(BASE_MODELS.keys())}")
    
    # ä¸é¢„çƒ­ä»»ä½•æ¨¡å‹ï¼Œç­‰å¾…ç”¨æˆ·è¯·æ±‚æ—¶åŠ è½½
    current_base_model = None
    print("ğŸ¯ ç³»ç»Ÿå°±ç»ªï¼Œç­‰å¾…æ¨¡å‹åŠ è½½è¯·æ±‚...")

def load_flux_model(base_path: str, device: str) -> tuple:
    """åŠ è½½FLUXæ¨¡å‹"""
    # å†…å­˜ä¼˜åŒ–é…ç½®
    model_kwargs = {
        "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
        "use_safetensors": True,
        "low_cpu_mem_usage": True,
    }
    
    # å°è¯•ä½¿ç”¨è®¾å¤‡æ˜ å°„ä¼˜åŒ–
    if device == "cuda":
        try:
            # å…ˆå°è¯• "balanced" ç­–ç•¥
            model_kwargs_with_device_map = model_kwargs.copy()
            model_kwargs_with_device_map["device_map"] = "balanced"
            
            txt2img_pipe = FluxPipeline.from_pretrained(
                base_path,
                **model_kwargs_with_device_map
            )
            print("âœ… Device mapping enabled with 'balanced' strategy")
            
        except Exception as device_map_error:
            print(f"âš ï¸  Device mapping failed ({device_map_error}), loading without device mapping")
            # å›é€€åˆ°ä¸ä½¿ç”¨è®¾å¤‡æ˜ å°„
            txt2img_pipe = FluxPipeline.from_pretrained(
                base_path,
                **model_kwargs
            )
    else:
        # CPUæ¨¡å¼ç›´æ¥åŠ è½½
        txt2img_pipe = FluxPipeline.from_pretrained(
            base_path,
            **model_kwargs
        )
    
    # å¯ç”¨ä¼˜åŒ–
    try:
        txt2img_pipe.enable_attention_slicing()
        print("âœ… Attention slicing enabled")
    except Exception as e:
        print(f"âš ï¸  Attention slicing not available: {e}")
        
    try:
        txt2img_pipe.enable_model_cpu_offload()
        print("âœ… CPU offload enabled")
    except Exception as e:
        print(f"âš ï¸  CPU offload not available: {e}")
    
    try:
        txt2img_pipe.enable_vae_slicing()
        txt2img_pipe.enable_vae_tiling()
        print("âœ… VAE optimizations enabled")
    except Exception as e:
        print(f"âš ï¸  VAE optimizations not available: {e}")
    
    # åˆ›å»ºå›¾ç”Ÿå›¾ç®¡é“
    print("ğŸ”— Creating FLUX image-to-image pipeline (sharing components)...")
    img2img_pipe = FluxImg2ImgPipeline(
        vae=txt2img_pipe.vae,
        text_encoder=txt2img_pipe.text_encoder,
        text_encoder_2=txt2img_pipe.text_encoder_2,
        tokenizer=txt2img_pipe.tokenizer,
        tokenizer_2=txt2img_pipe.tokenizer_2,
        transformer=txt2img_pipe.transformer,
        scheduler=txt2img_pipe.scheduler,
    )
    
    return txt2img_pipe, img2img_pipe

def load_diffusers_model(base_path: str, device: str) -> tuple:
    """åŠ è½½æ ‡å‡†diffusersæ¨¡å‹ - ä¿®å¤LayerNorm Halfç²¾åº¦å…¼å®¹æ€§"""
    print(f"ğŸ¨ Loading diffusers model from {base_path}")
    
    # ğŸš¨ å¼ºåˆ¶ä½¿ç”¨float32é¿å…LayerNormKernelImplé”™è¯¯
    # WAI-NSFW-illustrious-SDXLæ¨¡å‹åœ¨æŸäº›LayerNormæ“ä½œä¸Šä¸æ”¯æŒHalfç²¾åº¦
    torch_dtype = torch.float32
    print(f"ğŸ’¡ ä½¿ç”¨float32ç²¾åº¦é¿å…LayerNormå…¼å®¹æ€§é—®é¢˜")
    
    try:
        # åŠ è½½ä¸»è¦æ–‡æœ¬åˆ°å›¾åƒç®¡é“
        txt2img_pipeline = StableDiffusionPipeline.from_single_file(
            base_path,
            torch_dtype=torch_dtype,
            use_safetensors=True,
            safety_checker=None,  # ç¦ç”¨å®‰å…¨æ£€æŸ¥å™¨ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
            requires_safety_checker=False,
            load_safety_checker=False
        ).to(device)
        
        # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        txt2img_pipeline.enable_attention_slicing()
        txt2img_pipeline.enable_model_cpu_offload()
        
        # åˆ›å»ºå›¾åƒåˆ°å›¾åƒç®¡é“ï¼ˆå…±äº«ç»„ä»¶ï¼‰
        img2img_pipeline = StableDiffusionImg2ImgPipeline(
            vae=txt2img_pipeline.vae,
            text_encoder=txt2img_pipeline.text_encoder,
            tokenizer=txt2img_pipeline.tokenizer,
            unet=txt2img_pipeline.unet,
            scheduler=txt2img_pipeline.scheduler,
            safety_checker=None,  # ç¦ç”¨å®‰å…¨æ£€æŸ¥å™¨
            feature_extractor=getattr(txt2img_pipeline, 'feature_extractor', None),
            requires_safety_checker=False
        ).to(device)
        
        # ğŸš¨ é¢å¤–ç¡®ä¿å®‰å…¨æ£€æŸ¥å™¨è¢«ç¦ç”¨
        txt2img_pipeline.safety_checker = None
        txt2img_pipeline.requires_safety_checker = False
        img2img_pipeline.safety_checker = None
        img2img_pipeline.requires_safety_checker = False
        
        # åŒæ ·çš„ä¼˜åŒ–
        img2img_pipeline.enable_attention_slicing()
        img2img_pipeline.enable_model_cpu_offload()
        
        return txt2img_pipeline, img2img_pipeline
        
    except Exception as e:
        print(f"âŒ Error loading diffusers model: {str(e)}")
        raise e

def load_specific_model(base_model_type: str):
    """åŠ è½½æŒ‡å®šçš„åŸºç¡€æ¨¡å‹ - æ”¹è¿›çš„LoRAå¤„ç†"""
    global txt2img_pipe, img2img_pipe, current_base_model, current_lora_config, current_selected_lora
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    model_config = BASE_MODELS[base_model_type]
    device = get_device()
    
    print(f"ğŸ¯ Loading {model_config['name']} model...")
    
    try:
        model_start_time = datetime.now()
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        if model_config["model_type"] == "flux":
            txt2img_pipe, img2img_pipe = load_flux_model(model_config["model_path"], device)
        elif model_config["model_type"] == "diffusers":
            txt2img_pipe, img2img_pipe = load_diffusers_model(model_config["model_path"], device)
        
        current_base_model = base_model_type
        
        # ğŸš¨ ä¿å®ˆçš„LoRAåŠ è½½ç­–ç•¥ - å¤±è´¥ä¸å½±å“åŸºç¡€æ¨¡å‹
        default_lora_path = model_config.get("lora_path")
        lora_loaded_successfully = False
        
        if default_lora_path and os.path.exists(default_lora_path):
            try:
                lora_start_time = datetime.now()
                print(f"ğŸ¨ å°è¯•åŠ è½½é»˜è®¤LoRA: {default_lora_path}")
                
                model_type = model_config.get("model_type")
                if model_type == "flux":
                    # FLUXæ¨¡å‹LoRAåŠ è½½
                    try:
                        txt2img_pipe.load_lora_weights(default_lora_path)
                        lora_loaded_successfully = True
                        print("âœ… FLUX LoRAåŠ è½½æˆåŠŸ")
                    except Exception as flux_lora_error:
                        print(f"âš ï¸  FLUX LoRAåŠ è½½å¤±è´¥: {flux_lora_error}")
                        
                elif model_type == "diffusers":
                    # ğŸš¨ åŠ¨æ¼«æ¨¡å‹LoRAå…¼å®¹æ€§æµ‹è¯•
                    print(f"ğŸ§ª æµ‹è¯•åŠ¨æ¼«æ¨¡å‹LoRAå…¼å®¹æ€§...")
                    try:
                        # å°è¯•åŠ è½½LoRAï¼Œä½†å‡†å¤‡æ•è·æ‰€æœ‰å¯èƒ½çš„é”™è¯¯
                        txt2img_pipe.load_lora_weights(default_lora_path)
                        lora_loaded_successfully = True
                        print("âœ… åŠ¨æ¼«æ¨¡å‹LoRAåŠ è½½æˆåŠŸ")
                    except Exception as anime_lora_error:
                        print(f"âš ï¸  åŠ¨æ¼«æ¨¡å‹LoRAä¸å…¼å®¹: {str(anime_lora_error)[:200]}...")
                        print("â„¹ï¸  è¿™æ˜¯é¢„æœŸè¡Œä¸º - è¯¥LoRAä¸å½“å‰åŠ¨æ¼«åŸºç¡€æ¨¡å‹æ¶æ„ä¸åŒ¹é…")
                        print("â„¹ï¸  ç³»ç»Ÿå°†ä½¿ç”¨çº¯åŸºç¡€æ¨¡å‹ç»§ç»­è¿è¡Œ")
                        lora_loaded_successfully = False
                
                if lora_loaded_successfully:
                    lora_time = (datetime.now() - lora_start_time).total_seconds()
                    print(f"âœ… LoRA loaded in {lora_time:.2f}s")
                    current_lora_config = {model_config["lora_id"]: 1.0}
                    current_selected_lora = model_config["lora_id"]
                else:
                    print("ğŸ¯ ç»§ç»­ä½¿ç”¨çº¯åŸºç¡€æ¨¡å‹ï¼ˆæ— LoRAï¼‰")
                    current_lora_config = {}
                    current_selected_lora = None
                    
            except Exception as general_lora_error:
                print(f"âš ï¸  LoRAåŠ è½½è¿‡ç¨‹å‡ºé”™: {general_lora_error}")
                print("â„¹ï¸  å°†ä½¿ç”¨çº¯åŸºç¡€æ¨¡å‹")
                current_lora_config = {}
                current_selected_lora = None
        else:
            print("â„¹ï¸  æ— é»˜è®¤LoRAé…ç½®ï¼Œä½¿ç”¨çº¯åŸºç¡€æ¨¡å‹")
            current_lora_config = {}
            current_selected_lora = None
        
        model_time = (datetime.now() - model_start_time).total_seconds()
        print(f"ğŸ‰ {model_config['name']} model loaded successfully in {model_time:.2f}s!")
        
        # ğŸš¨ è·³è¿‡åŠ¨æ¼«æ¨¡å‹çš„é¢„çƒ­æ¨ç†ï¼Œé¿å…ç²¾åº¦é—®é¢˜
        if model_config["model_type"] == "diffusers":
            print("âš¡ è·³è¿‡åŠ¨æ¼«æ¨¡å‹é¢„çƒ­æ¨ç†(é¿å…ç²¾åº¦å…¼å®¹æ€§é—®é¢˜)")
            print("âœ… åŠ¨æ¼«æ¨¡å‹ready for generation (no warmup needed)")
        else:
            # å¯¹FLUXæ¨¡å‹è¿›è¡Œé¢„çƒ­
            try:
                print("ğŸ”¥ Warming up model with test inference...")
                warmup_start = datetime.now()
                with torch.no_grad():
                    test_result = txt2img_pipe(
                        prompt="test",
                        width=512,
                        height=512,
                        num_inference_steps=1,
                        guidance_scale=1.0
                    )
                warmup_time = (datetime.now() - warmup_start).total_seconds()
                print(f"âœ… Model warmup completed in {warmup_time:.2f}s")
            except Exception as warmup_error:
                print(f"âš ï¸  Model warmup failed, but model should still work: {warmup_error}")
        
        print(f"ğŸš€ {model_config['name']} system ready for image generation!")
        
    except Exception as e:
        print(f"âŒ Failed to load {model_config['name']} model: {e}")
        # é‡ç½®å…¨å±€å˜é‡
        txt2img_pipe = None
        img2img_pipe = None
        current_base_model = None
        current_lora_config = {}
        current_selected_lora = None
        raise

def upload_to_r2(image_data: bytes, filename: str) -> str:
    """ä¸Šä¼ å›¾ç‰‡åˆ° Cloudflare R2"""
    try:
        # æ£€æŸ¥R2å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if r2_client is None:
            raise RuntimeError("R2 client not available - check environment variables")
            
        # ç¡®ä¿ image_data æ˜¯ bytes ç±»å‹
        if not isinstance(image_data, bytes):
            raise TypeError(f"Expected bytes, got {type(image_data)}")
            
        # éªŒè¯æ–‡ä»¶å¤§å°
        if len(image_data) == 0:
            raise ValueError("Image data is empty")
            
        if len(image_data) > 50 * 1024 * 1024:  # 50MB limit
            raise ValueError(f"Image too large: {len(image_data)} bytes")
            
        print(f"Uploading {len(image_data)} bytes to R2 as {filename}")
        
        r2_client.put_object(
            Bucket=CLOUDFLARE_R2_BUCKET,
            Key=filename,
            Body=image_data,
            ContentType='image/png',
            ACL='public-read'
        )
        
        # æ„å»ºæ­£ç¡®çš„å…¬å…± URL æ ¼å¼ - ä¼˜å…ˆä½¿ç”¨ R2 Public Domain
        # ä½¿ç”¨æä¾›çš„ R2 public domain (æœ€ç¨³å®šçš„è§£å†³æ–¹æ¡ˆ)
        r2_public_domain = os.getenv("CLOUDFLARE_R2_PUBLIC_BUCKET_DOMAIN")
        
        if r2_public_domain:
            # ä½¿ç”¨ R2 public domain (.r2.dev æ ¼å¼)
            public_url = f"https://{r2_public_domain}/{filename}"
            print(f"âœ“ Successfully uploaded to (R2 public domain): {public_url}")
        elif CLOUDFLARE_R2_PUBLIC_DOMAIN:
            # å¤‡é€‰ï¼šè‡ªå®šä¹‰åŸŸå
            public_url = f"{CLOUDFLARE_R2_PUBLIC_DOMAIN.rstrip('/')}/{filename}"
            print(f"âœ“ Successfully uploaded to (custom domain): {public_url}")
        else:
            # æœ€åå›é€€åˆ°æ ‡å‡†R2æ ¼å¼
            # æ­£ç¡®æ ¼å¼: https://{bucket}.{account_id}.r2.cloudflarestorage.com/{filename}
            # ä»endpoint URLä¸­æå–account ID
            account_id = CLOUDFLARE_R2_ENDPOINT.split('//')[1].split('.')[0]
            public_url = f"https://{CLOUDFLARE_R2_BUCKET}.{account_id}.r2.cloudflarestorage.com/{filename}"
            print(f"âœ“ Successfully uploaded to (standard R2): {public_url}")
            print(f"âš ï¸  æ³¨æ„ï¼šå¦‚æœå‡ºç°CORSé”™è¯¯ï¼Œå»ºè®®è®¾ç½® CLOUDFLARE_R2_PUBLIC_BUCKET_DOMAIN ç¯å¢ƒå˜é‡")
        
        return public_url
        
    except Exception as e:
        print(f"âœ— Error uploading to R2: {str(e)}")
        print(f"Image data type: {type(image_data)}, size: {len(image_data) if hasattr(image_data, '__len__') else 'unknown'}")
        
        # å¯¹äºæ¼”ç¤ºç›®çš„ï¼Œè¿”å›ä¸€ä¸ªå ä½ç¬¦URLè€Œä¸æ˜¯å¤±è´¥
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æŠ›å‡ºå¼‚å¸¸
        placeholder_url = f"https://via.placeholder.com/512x512/cccccc/666666?text=Upload+Failed"
        print(f"Returning placeholder URL: {placeholder_url}")
        return placeholder_url

def image_to_bytes(image: Image.Image) -> bytes:
    """å°† PIL Image è½¬æ¢ä¸ºå­—èŠ‚"""
    try:
        buffer = io.BytesIO()
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image.save(buffer, format='PNG', quality=95, optimize=True)
        buffer.seek(0)  # é‡ç½®bufferä½ç½®
        return buffer.getvalue()
    except Exception as e:
        print(f"Error converting image to bytes: {str(e)}")
        raise e

def base64_to_image(base64_str: str) -> Image.Image:
    """å°† base64 å­—ç¬¦ä¸²è½¬æ¢ä¸º PIL Image"""
    if base64_str.startswith('data:image'):
        base64_str = base64_str.split(',')[1]
    
    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image.convert('RGB')

def process_long_prompt(prompt: str, max_clip_tokens: int = 75, max_t5_tokens: int = 500) -> tuple:
    """
    å¤„ç†é•¿æç¤ºè¯ï¼Œä¸ºFLUXçš„åŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
    
    Args:
        prompt: è¾“å…¥æç¤ºè¯
        max_clip_tokens: CLIPç¼–ç å™¨æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤75ï¼Œç•™2ä¸ªç‰¹æ®Štokenç©ºé—´ï¼‰
        max_t5_tokens: T5ç¼–ç å™¨æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤500ï¼Œç•™ç©ºé—´ç»™ç‰¹æ®Štokenï¼‰
    
    Returns:
        tuple: (clip_prompt, t5_prompt)
    """
    if not prompt:
        return "", ""
    
    # ğŸ¯ æ›´å‡†ç¡®çš„tokenä¼°ç®—ï¼šè€ƒè™‘æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    # ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼ã€é€—å·ã€æ ‡ç‚¹ç¬¦å·åˆ†å‰²
    token_pattern = r'\w+|[^\w\s]'  # æå–regexæ¨¡å¼é¿å…f-stringä¸­çš„åæ–œæ 
    tokens = re.findall(token_pattern, prompt.lower())
    estimated_tokens = len(tokens)
    
    print(f"ğŸ“ Prompt analysis: {len(prompt)} chars, ~{estimated_tokens} tokens (improved estimation)")
    
    if estimated_tokens <= max_clip_tokens:
        # çŸ­promptï¼šä¸¤ä¸ªç¼–ç å™¨éƒ½ä½¿ç”¨å®Œæ•´prompt
        print("âœ… Short prompt: using full prompt for both CLIP and T5")
        return prompt, prompt
    else:
        # é•¿promptï¼šCLIPä½¿ç”¨æˆªæ–­ç‰ˆæœ¬ï¼ŒT5ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬
        if estimated_tokens <= max_t5_tokens:
            # ğŸ¯ æ›´æ™ºèƒ½çš„CLIPæˆªæ–­ï¼šä¿æŒå®Œæ•´çš„è¯­ä¹‰å•å…ƒ
            words = prompt.split()
            
            # ä»å‰å¾€åç´¯ç§¯tokenï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
            clip_words = []
            current_tokens = 0
            
            for word in words:
                # ä¼°ç®—å½“å‰å•è¯çš„tokenæ•°ï¼ˆè€ƒè™‘æ ‡ç‚¹ç¬¦å·ï¼‰
                word_tokens = len(re.findall(token_pattern, word.lower()))
                
                if current_tokens + word_tokens <= max_clip_tokens:
                    clip_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # å¦‚æœæˆªæ–­ç‚¹ä¸ç†æƒ³ï¼Œå°è¯•åœ¨å¥å·æˆ–é€—å·å¤„æˆªæ–­
            if len(clip_words) > 10:  # åªåœ¨æœ‰è¶³å¤Ÿè¯æ±‡æ—¶ä¼˜åŒ–æˆªæ–­ç‚¹
                for i in range(len(clip_words) - 1, max(0, len(clip_words) - 5), -1):
                    if clip_words[i].endswith(('.', ',', ';', '!')):
                        clip_words = clip_words[:i+1]
                        break
            
            clip_prompt = ' '.join(clip_words)
            clip_token_count = len(re.findall(token_pattern, clip_prompt.lower()))
            
            print(f"ğŸ“ Long prompt optimization:")
            print(f"   CLIP prompt: ~{len(clip_words)} words â†’ {clip_token_count} tokens (safe truncation)")
            print(f"   T5 prompt: ~{estimated_tokens} tokens (full prompt)")
            return clip_prompt, prompt
        else:
            # è¶…é•¿promptï¼šä¸¤ä¸ªç¼–ç å™¨éƒ½éœ€è¦æˆªæ–­
            words = prompt.split()
            
            # CLIPæˆªæ–­
            clip_words = []
            current_tokens = 0
            for word in words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if current_tokens + word_tokens <= max_clip_tokens:
                    clip_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # T5æˆªæ–­
            t5_words = []
            current_tokens = 0
            for word in words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if current_tokens + word_tokens <= max_t5_tokens:
                    t5_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # ä¼˜åŒ–æˆªæ–­ç‚¹
            if len(clip_words) > 10:
                for i in range(len(clip_words) - 1, max(0, len(clip_words) - 5), -1):
                    if clip_words[i].endswith(('.', ',', ';')):
                        clip_words = clip_words[:i+1]
                        break
                        
            if len(t5_words) > 20:
                for i in range(len(t5_words) - 1, max(0, len(t5_words) - 10), -1):
                    if t5_words[i].endswith(('.', ',', ';')):
                        t5_words = t5_words[:i+1]
                        break
            
            clip_prompt = ' '.join(clip_words)
            t5_prompt = ' '.join(t5_words)
            
            clip_token_count = len(re.findall(token_pattern, clip_prompt.lower()))
            t5_token_count = len(re.findall(token_pattern, t5_prompt.lower()))
            
            print(f"âš ï¸  Ultra-long prompt: both encoders truncated intelligently")
            print(f"   CLIP prompt: ~{len(clip_words)} words â†’ {clip_token_count} tokens")
            print(f"   T5 prompt: ~{len(t5_words)} words â†’ {t5_token_count} tokens")
            return clip_prompt, t5_prompt

def generate_flux_images(prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str) -> list:
    """FLUXæ¨¡å‹å›¾åƒç”Ÿæˆ"""
    global txt2img_pipe, device_mapping_enabled
    
    # FLUXæ¨¡å‹åŸç”Ÿæ”¯æŒé•¿æç¤ºè¯ï¼Œä½¿ç”¨ä¼˜åŒ–çš„embeddingå¤„ç†
    generation_kwargs = {
        "width": width,
        "height": height,
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating FLUX prompt embeddings using pipeline.encode_prompt()...")
    try:
        device = get_device()
        
        # Clear GPU cache before encoding
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory before encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            
        # Only try to move text encoders to CPU if device mapping is NOT enabled
        text_encoder_device = None
        text_encoder_2_device = None
        
        try:
            if not device_mapping_enabled:
                print("ğŸ“¦ Manual memory management mode (no device mapping)")
                # Store original devices and move text encoders to CPU
                if hasattr(txt2img_pipe, 'text_encoder') and txt2img_pipe.text_encoder is not None:
                    text_encoder_device = next(txt2img_pipe.text_encoder.parameters()).device
                    if str(text_encoder_device) != 'cpu':
                        print("ğŸ“¦ Moving text_encoder to CPU temporarily to save GPU memory...")
                        txt2img_pipe.text_encoder.to('cpu')
                        torch.cuda.empty_cache()
                        
                if hasattr(txt2img_pipe, 'text_encoder_2') and txt2img_pipe.text_encoder_2 is not None:
                    text_encoder_2_device = next(txt2img_pipe.text_encoder_2.parameters()).device
                    if str(text_encoder_2_device) != 'cpu':
                        print("ğŸ“¦ Moving text_encoder_2 to CPU temporarily to save GPU memory...")
                        txt2img_pipe.text_encoder_2.to('cpu')
                        torch.cuda.empty_cache()
                        print(f"ğŸ’¾ GPU Memory after moving encoders to CPU: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            else:
                print("âš¡ Device mapping mode - trusting accelerate for memory management")

            # Encode positive prompt with memory management
            print("ğŸ”¤ Encoding positive prompt...")
            
            # ğŸ¯ ä¼˜åŒ–é•¿æç¤ºè¯å¤„ç†ï¼šä¸ºFLUXåŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
            clip_prompt, t5_prompt = process_long_prompt(prompt)
            
            with torch.cuda.amp.autocast(enabled=False):  # Disable autocast to reduce memory
                prompt_embeds_obj = txt2img_pipe.encode_prompt(
                    prompt=clip_prompt,    # CLIPç¼–ç å™¨ä½¿ç”¨ä¼˜åŒ–åçš„promptï¼ˆæœ€å¤š77 tokensï¼‰
                    prompt_2=t5_prompt,    # T5ç¼–ç å™¨ä½¿ç”¨å®Œæ•´promptï¼ˆæœ€å¤š512 tokensï¼‰
                    device=device,
                    num_images_per_prompt=1 
                )
            
            # Force move embeddings to CPU immediately
            if hasattr(prompt_embeds_obj, 'prompt_embeds'):
                prompt_embeds_cpu = prompt_embeds_obj.prompt_embeds.cpu()
                pooled_prompt_embeds_cpu = prompt_embeds_obj.pooled_prompt_embeds.cpu() if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds') else None
            else:
                # Handle tuple case
                prompt_embeds_cpu = prompt_embeds_obj[0].cpu() if isinstance(prompt_embeds_obj, tuple) else None
                pooled_prompt_embeds_cpu = prompt_embeds_obj[1].cpu() if isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) > 1 else None
            
            # Clear GPU memory after positive encoding
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory after positive encoding (moved to CPU): {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

            print("âš¡ Skipping negative prompt embedding encoding (FLUX doesn't support negative_prompt_embeds)")
            
        finally:
            # Restore text encoders to original devices (only if we moved them manually)
            if not device_mapping_enabled:
                if text_encoder_device is not None and hasattr(txt2img_pipe, 'text_encoder') and txt2img_pipe.text_encoder is not None:
                    print(f"ğŸ“¦ Restoring text_encoder to {text_encoder_device}...")
                    txt2img_pipe.text_encoder.to(text_encoder_device)
                    
                if text_encoder_2_device is not None and hasattr(txt2img_pipe, 'text_encoder_2') and txt2img_pipe.text_encoder_2 is not None:
                    print(f"ğŸ“¦ Restoring text_encoder_2 to {text_encoder_2_device}...")
                    txt2img_pipe.text_encoder_2.to(text_encoder_2_device)
            else:
                print("âš¡ Skipping text encoder restoration (device mapping handles placement)")
                
            torch.cuda.empty_cache()

        # Now move embeddings back to GPU and assign to generation_kwargs
        print("ğŸš€ Moving embeddings back to GPU for generation...")
        
        # Move embeddings back to GPU when needed  
        generation_kwargs["prompt_embeds"] = prompt_embeds_cpu.to(device)
        
        if pooled_prompt_embeds_cpu is not None:
            generation_kwargs["pooled_prompt_embeds"] = pooled_prompt_embeds_cpu.to(device)

        # FLUXä½¿ç”¨ä¼ ç»Ÿçš„guidance_scaleå‚æ•°
        generation_kwargs["guidance_scale"] = cfg_scale
        print(f"ğŸ›ï¸ Using guidance_scale: {cfg_scale}")
            
        print(f"ğŸ’¾ GPU Memory before generation: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

    except Exception as e:
        print(f"âš ï¸ FLUX pipeline.encode_prompt() failed: {e}. Using raw prompts.")
        generation_kwargs["prompt"] = prompt
        generation_kwargs["negative_prompt"] = negative_prompt

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text-to-image")

def generate_diffusers_images(prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str) -> list:
    """ä½¿ç”¨æ ‡å‡†diffusersç®¡é“ç”Ÿæˆå›¾åƒ - æ·±åº¦ä¿®å¤NoneTypeé”™è¯¯"""
    global txt2img_pipe
    
    if txt2img_pipe is None:
        raise RuntimeError("Diffusers pipeline not loaded")
    
    print(f"ğŸ“ Processing anime model generation...")
    
    # ğŸš¨ å…¨é¢çš„å‚æ•°å®‰å…¨æ£€æŸ¥å’Œä¿®å¤
    if not prompt or prompt is None:
        prompt = "masterpiece, best quality, 1boy, handsome man, anime style"
        print(f"âš ï¸  ä¿®å¤ç©ºprompt: {prompt}")
    
    if negative_prompt is None:
        negative_prompt = ""
        print(f"âš ï¸  ä¿®å¤None negative_prompt")
    
    # ç¡®ä¿promptå’Œnegative_promptéƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
    prompt = str(prompt).strip()
    negative_prompt = str(negative_prompt).strip()
    
    print(f"ğŸ” æœ€ç»ˆå‚æ•°æ£€æŸ¥:")
    print(f"  prompt: {repr(prompt)} (type: {type(prompt)})")
    print(f"  negative_prompt: {repr(negative_prompt)} (type: {type(negative_prompt)})")
    print(f"  dimensions: {width}x{height}")
    print(f"  steps: {steps}, cfg_scale: {cfg_scale}")
    
    # ğŸš¨ è·³è¿‡Compelå¤„ç†ï¼Œç›´æ¥ä½¿ç”¨ç®€å•çš„æ–‡æœ¬
    # é¿å…å¤æ‚çš„promptå¤„ç†å¯èƒ½å¯¼è‡´çš„Noneé—®é¢˜
    print("ğŸ¯ è·³è¿‡Compelå¤„ç†ï¼Œä½¿ç”¨ç®€å•promptå¤„ç†é¿å…Noneé”™è¯¯")
    
    # ğŸš¨ ä½¿ç”¨æœ€åŸºç¡€çš„å‚æ•°é…ç½®ï¼Œé¿å…ä»»ä½•å¯èƒ½çš„Noneä¼ é€’
    generation_kwargs = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "height": int(height),
        "width": int(width),
        "num_inference_steps": int(steps),
        "guidance_scale": float(cfg_scale),
        "num_images_per_prompt": 1,  # å…ˆå¼ºåˆ¶å•å¼ ç”Ÿæˆ
        "output_type": "pil",
        "return_dict": True
    }
    
    # è®¾ç½®éšæœºç§å­
    if seed != -1:
        import torch
        generator = torch.Generator(device=txt2img_pipe.device).manual_seed(int(seed))
        generation_kwargs["generator"] = generator
    
    print(f"ğŸ¯ Generation kwargs: {list(generation_kwargs.keys())}")
    
    return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text_to_image")

def generate_images_common(generation_kwargs: dict, prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str, task_type: str) -> list:
    """é€šç”¨å›¾åƒç”Ÿæˆé€»è¾‘ - ç®€åŒ–ç‰ˆï¼Œä¸“æ³¨äºåŸºç¡€åŠŸèƒ½"""
    global txt2img_pipe, current_base_model
    
    # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸ºNoneï¼Œé¿å…NoneTypeé”™è¯¯
    if prompt is None or prompt == "":
        prompt = "masterpiece, best quality, 1boy"
        print(f"âš ï¸  ç©ºpromptï¼Œä½¿ç”¨é»˜è®¤: {prompt}")
    if negative_prompt is None:
        negative_prompt = ""
        print(f"âš ï¸  negative_promptä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²")
    
    print(f"ğŸ” Debug - prompt: {repr(prompt)}, negative_prompt: {repr(negative_prompt)}")
    
    results = []
    
    # è·å–å½“å‰æ¨¡å‹ç±»å‹ä»¥ç¡®å®šautocastç­–ç•¥
    model_config = BASE_MODELS.get(current_base_model, {})
    model_type = model_config.get("model_type", "unknown")
    
    # ğŸš¨ åŠ¨æ¼«æ¨¡å‹ç¦ç”¨autocasté¿å…LayerNormç²¾åº¦é—®é¢˜
    use_autocast = model_type == "flux"  # åªæœ‰FLUXæ¨¡å‹ä½¿ç”¨autocast
    
    # ğŸš¨ å¯¹äºåŠ¨æ¼«æ¨¡å‹ï¼Œå¼ºåˆ¶å•å¼ ç”Ÿæˆé¿å…å¤æ‚çš„æ‰¹é‡å¤„ç†
    if model_type == "diffusers" and num_images > 1:
        print(f"ğŸ¯ åŠ¨æ¼«æ¨¡å‹å¼ºåˆ¶å•å¼ ç”Ÿæˆï¼Œå¿½ç•¥æ‰¹é‡è¯·æ±‚")
        actual_num_images = 1
    else:
        actual_num_images = num_images
    
    # ç®€åŒ–ç”Ÿæˆé€»è¾‘ - ç›´æ¥è¿›è¡Œå•å¼ ç”Ÿæˆ
    try:
        print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ (æ¨¡å‹: {model_type})")
        
        # ç”Ÿæˆå›¾åƒ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨autocast
        if use_autocast:
            with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                result = txt2img_pipe(**generation_kwargs)
        else:
            print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹: è·³è¿‡autocastä½¿ç”¨float32ç²¾åº¦")
            result = txt2img_pipe(**generation_kwargs)
        
        # å¤„ç†ç»“æœ
        if hasattr(result, 'images') and result.images:
            for i, image in enumerate(result.images):
                if image is not None:
                    try:
                        # ä¸Šä¼ åˆ°R2
                        filename = f"txt2img_{current_base_model}_{int(time.time())}_{i}.png"
                        image_url = upload_to_r2(image_to_bytes(image), filename)
                        
                        results.append({
                            'url': image_url,
                            'filename': filename,
                            'prompt': prompt,
                            'model': current_base_model,
                            'width': width,
                            'height': height,
                            'steps': steps,
                            'cfg_scale': cfg_scale,
                            'seed': seed
                        })
                        print(f"âœ… å›¾åƒ {i+1} ç”ŸæˆæˆåŠŸ: {filename}")
                    except Exception as upload_error:
                        print(f"âŒ ä¸Šä¼ å›¾åƒ {i+1} å¤±è´¥: {upload_error}")
                        continue
        else:
            print("âš ï¸  ç®¡é“è¿”å›ç©ºç»“æœæˆ–æ— å›¾åƒ")
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›¾åƒå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    # å¦‚æœéœ€è¦å¤šå¼ ä½†åªç”Ÿæˆäº†ä¸€å¼ ï¼Œå¤åˆ¶ç»“æœ
    if len(results) == 1 and num_images > 1:
        print(f"ğŸ”„ ä¸ºæ»¡è¶³ {num_images} å¼ éœ€æ±‚ï¼Œå¤åˆ¶å•å¼ ç»“æœ")
        original = results[0]
        for i in range(1, num_images):
            # åˆ›å»ºæ–°çš„æ–‡ä»¶åä½†ä½¿ç”¨ç›¸åŒå›¾åƒ
            new_filename = f"txt2img_{current_base_model}_{int(time.time())}_{i}_copy.png"
            copy_result = original.copy()
            copy_result['filename'] = new_filename
            results.append(copy_result)
    
    print(f"ğŸ¯ æ€»å…±ç”Ÿæˆäº† {len(results)} å¼ å›¾åƒ")
    return results

def text_to_image(prompt: str, negative_prompt: str = "", width: int = 1024, height: int = 1024, steps: int = 12, cfg_scale: float = 1.0, seed: int = -1, num_images: int = 1, base_model: str = "realistic") -> list:
    """æ–‡æœ¬ç”Ÿæˆå›¾åƒ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹"""
    global current_base_model, txt2img_pipe
    
    print(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {base_model}, å½“å‰åŠ è½½æ¨¡å‹: {current_base_model}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢æ¨¡å‹
    if current_base_model != base_model:
        print(f"ğŸ”„ éœ€è¦åˆ‡æ¢æ¨¡å‹: {current_base_model} -> {base_model}")
        try:
            load_specific_model(base_model)
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {e}")
            raise e
    
    # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
    if txt2img_pipe is None:
        print(f"âš ï¸  æ¨¡å‹æœªåŠ è½½ï¼ŒåŠ è½½ {base_model} æ¨¡å‹...")
        load_specific_model(base_model)
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = BASE_MODELS.get(base_model)
    if not model_config:
        raise ValueError(f"Unknown base model: {base_model}")
    
    model_type = model_config["model_type"]
    print(f"ğŸ¨ ä½¿ç”¨ {model_type} ç®¡é“ç”Ÿæˆå›¾åƒ...")
    
    # ğŸ¯ æ¨¡å‹ç‰¹å®šå‚æ•°ä¼˜åŒ–
    if model_type == "flux":
        # FLUXæ¨¡å‹å‚æ•°ä¼˜åŒ– - æ ¹æ®å®˜æ–¹æ¨è https://huggingface.co/lustlyai/Flux_Lustly.ai_Uncensored_nsfw_v1
        # å®˜æ–¹æ¨è: guidance_scale=4, steps=20, 768x768åˆ†è¾¨ç‡
        
        if cfg_scale < 3.0:
            print(f"âš ï¸  FLUX CFGè¿‡ä½ ({cfg_scale})ï¼Œè°ƒæ•´ä¸º4.0 (å®˜æ–¹æ¨è)")
            cfg_scale = 4.0
        elif cfg_scale > 6.0:
            print(f"âš ï¸  FLUX CFGè¿‡é«˜ ({cfg_scale})ï¼Œè°ƒæ•´ä¸º4.0 (å®˜æ–¹æ¨è)")
            cfg_scale = 4.0
            
        if steps < 15:
            print(f"âš ï¸  FLUX stepsè¿‡ä½ ({steps})ï¼Œè°ƒæ•´ä¸º20 (å®˜æ–¹æ¨è)")
            steps = 20
        elif steps > 30:
            print(f"âš ï¸  FLUX stepsè¿‡é«˜ ({steps})ï¼Œè°ƒæ•´ä¸º20 (å®˜æ–¹æ¨è)")
            steps = 20
            
        # æ¨è768x768åˆ†è¾¨ç‡ä»¥è·å¾—æ›´å¥½è´¨é‡
        if width == 1024 and height == 1024:
            print("ğŸ’¡ FLUXæ¨è768x768åˆ†è¾¨ç‡ä»¥è·å¾—æ›´å¥½è´¨é‡")
            width = 768
            height = 768
            
        print(f"ğŸ”§ FLUXä¼˜åŒ–å‚æ•°(å®˜æ–¹æ¨è): steps={steps}, cfg_scale={cfg_scale}, size={width}x{height}")
        return generate_flux_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model)
        
    elif model_type == "diffusers":
        # åŠ¨æ¼«æ¨¡å‹å‚æ•°ä¼˜åŒ– - æ ¹æ®CivitAIå®˜æ–¹æ¨è
        # WAI-NSFW-illustrious-SDXLæ¨è: Steps: 15-30, CFG scale: 5-7, 1024x1024ä»¥ä¸Š
        
        if cfg_scale < 5.0:
            print(f"âš ï¸  åŠ¨æ¼«æ¨¡å‹CFGè¿‡ä½ ({cfg_scale})ï¼Œè°ƒæ•´ä¸º6.0 (å®˜æ–¹æ¨è5-7)")
            cfg_scale = 6.0
        elif cfg_scale > 8.0:
            print(f"âš ï¸  åŠ¨æ¼«æ¨¡å‹CFGè¿‡é«˜ ({cfg_scale})ï¼Œè°ƒæ•´ä¸º7.0 (å®˜æ–¹æ¨è5-7)")
            cfg_scale = 7.0
            
        if steps < 15:
            print(f"âš ï¸  åŠ¨æ¼«æ¨¡å‹stepsè¿‡ä½ ({steps})ï¼Œè°ƒæ•´ä¸º20 (å®˜æ–¹æ¨è15-30)")
            steps = 20
        elif steps > 35:
            print(f"âš ï¸  åŠ¨æ¼«æ¨¡å‹stepsè¿‡é«˜ ({steps})ï¼Œè°ƒæ•´ä¸º30 (å®˜æ–¹æ¨è15-30)")
            steps = 30
            
        # ç¡®ä¿ä½¿ç”¨1024x1024ä»¥ä¸Šåˆ†è¾¨ç‡
        if width < 1024 or height < 1024:
            print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹æ¨è1024x1024ä»¥ä¸Šåˆ†è¾¨ç‡")
            if width < 1024:
                width = 1024
            if height < 1024:
                height = 1024
            
        print(f"ğŸ”§ åŠ¨æ¼«æ¨¡å‹ä¼˜åŒ–å‚æ•°(CivitAIæ¨è): steps={steps}, cfg_scale={cfg_scale}, size={width}x{height}")
        return generate_diffusers_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

def image_to_image(params: dict) -> list:
    """å›¾ç”Ÿå›¾ç”Ÿæˆ - ä¼˜åŒ–ç‰ˆæœ¬"""
    global img2img_pipe, current_base_model
    
    if img2img_pipe is None:
        raise ValueError("Image-to-image model not loaded")
    
    # æå–å‚æ•°
    prompt = params.get('prompt', '')
    negative_prompt = params.get('negativePrompt', '')
    image_data = params.get('image', '')
    width = params.get('width', 512)
    height = params.get('height', 512)
    steps = params.get('steps', 20)
    cfg_scale = params.get('cfgScale', 7.0)
    seed = params.get('seed', -1)
    num_images = params.get('numImages', 1)
    denoising_strength = params.get('denoisingStrength', 0.7)
    base_model = params.get('baseModel', 'realistic')
    lora_config = params.get('lora_config', {})
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åŸºç¡€æ¨¡å‹
    if base_model != current_base_model:
        print(f"Switching base model for generation: {current_base_model} -> {base_model}")
        switch_base_model(base_model)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
    if lora_config and lora_config != current_lora_config:
        print(f"Updating LoRA config for generation: {lora_config}")
        load_multiple_loras(lora_config)
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    if isinstance(image_data, str):
        source_image = base64_to_image(image_data)
    else:
        raise ValueError("Invalid image data format")
    
    # è°ƒæ•´å›¾åƒå°ºå¯¸
    source_image = source_image.resize((width, height), Image.Resampling.LANCZOS)
    
    # ğŸ¯ é•¿æç¤ºè¯æ”¯æŒ - å…¨æ–°æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨FLUXåŸç”Ÿå¤„ç† (é€šè¿‡ pipeline.encode_prompt)
    print(f"ğŸ“ Processing prompt for Img2Img: {len(prompt)} characters")
    
    generation_kwargs = {
        "image": source_image,
        # "prompt": prompt, # Replaced by embeds
        # "negative_prompt": negative_prompt, # Replaced by embeds
        "width": width, 
        "height": height,
        "strength": denoising_strength, # For Img2Img
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating prompt embeddings for Img2Img using pipeline.encode_prompt()...")
    try:
        device = get_device()
        
        # Clear GPU cache before encoding
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory before img2img encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

        # Only try to move text encoders to CPU if device mapping is NOT enabled
        # Device mapping conflicts with manual component movement
        text_encoder_device = None
        text_encoder_2_device = None
        
        try:
            if not device_mapping_enabled:
                print("ğŸ“¦ Manual memory management mode for img2img (no device mapping)")
                # Store original devices and move text encoders to CPU
                if hasattr(img2img_pipe, 'text_encoder') and img2img_pipe.text_encoder is not None:
                    text_encoder_device = next(img2img_pipe.text_encoder.parameters()).device
                    if str(text_encoder_device) != 'cpu':
                        print("ğŸ“¦ Moving img2img text_encoder to CPU temporarily...")
                        img2img_pipe.text_encoder.to('cpu')
                        torch.cuda.empty_cache()
                        
                if hasattr(img2img_pipe, 'text_encoder_2') and img2img_pipe.text_encoder_2 is not None:
                    text_encoder_2_device = next(img2img_pipe.text_encoder_2.parameters()).device
                    if str(text_encoder_2_device) != 'cpu':
                        print("ğŸ“¦ Moving img2img text_encoder_2 to CPU temporarily...")
                        img2img_pipe.text_encoder_2.to('cpu')
                        torch.cuda.empty_cache()
                        print(f"ğŸ’¾ GPU Memory after moving img2img encoders to CPU: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            else:
                print("âš¡ Device mapping mode for img2img - trusting accelerate for memory management")

            # Encode positive prompt with memory management
            print("ğŸ”¤ Encoding positive prompt for img2img...")
            
            # ğŸ¯ ä¼˜åŒ–é•¿æç¤ºè¯å¤„ç†ï¼šä¸ºFLUXåŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
            clip_prompt, t5_prompt = process_long_prompt(prompt)
            # FLUXä¸éœ€è¦è´Ÿæç¤ºè¯åµŒå…¥ï¼Œåªå¤„ç†æ­£æç¤ºè¯
            
            with torch.cuda.amp.autocast(enabled=False):
                prompt_embeds_obj = img2img_pipe.encode_prompt(
                    prompt=clip_prompt,    # CLIPç¼–ç å™¨ä½¿ç”¨ä¼˜åŒ–åçš„promptï¼ˆæœ€å¤š77 tokensï¼‰
                    prompt_2=t5_prompt,    # T5ç¼–ç å™¨ä½¿ç”¨å®Œæ•´promptï¼ˆæœ€å¤š512 tokensï¼‰
                    device=device,
                    num_images_per_prompt=1
                )
            
            # Force move embeddings to CPU immediately
            if hasattr(prompt_embeds_obj, 'prompt_embeds'):
                prompt_embeds_cpu = prompt_embeds_obj.prompt_embeds.cpu()
                pooled_prompt_embeds_cpu = prompt_embeds_obj.pooled_prompt_embeds.cpu() if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds') else None
            else:
                prompt_embeds_cpu = prompt_embeds_obj[0].cpu() if isinstance(prompt_embeds_obj, tuple) else None
                pooled_prompt_embeds_cpu = prompt_embeds_obj[1].cpu() if isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) > 1 else None
            
            # Clear cache after positive encoding
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory after positive img2img encoding (moved to CPU): {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

            # âŒ è·³è¿‡è´Ÿæç¤ºè¯åµŒå…¥ç¼–ç ï¼ŒFLUXä¸æ”¯æŒ
            print("âš¡ Skipping negative prompt embedding encoding for img2img (FLUX doesn't support negative_prompt_embeds)")
            
        finally:
            # Restore text encoders to original devices (only if we moved them manually)
            if not device_mapping_enabled:
                if text_encoder_device is not None and hasattr(img2img_pipe, 'text_encoder') and img2img_pipe.text_encoder is not None:
                    print(f"ğŸ“¦ Restoring img2img text_encoder to {text_encoder_device}...")
                    img2img_pipe.text_encoder.to(text_encoder_device)
                    
                if text_encoder_2_device is not None and hasattr(img2img_pipe, 'text_encoder_2') and img2img_pipe.text_encoder_2 is not None:
                    print(f"ğŸ“¦ Restoring img2img text_encoder_2 to {text_encoder_2_device}...")
                    img2img_pipe.text_encoder_2.to(text_encoder_2_device)
            else:
                print("âš¡ Skipping img2img text encoder restoration (device mapping handles placement)")
                
            torch.cuda.empty_cache()

        # Now move embeddings back to GPU and assign to generation_kwargs
        print("ğŸš€ Moving img2img embeddings back to GPU for generation...")
        
        # Move embeddings back to GPU when needed
        generation_kwargs["prompt_embeds"] = prompt_embeds_cpu.to(device)
        # âŒ FLUXä¸æ”¯æŒnegative_prompt_embedså‚æ•°ï¼Œç§»é™¤
        # generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_cpu.to(device)
        
        if pooled_prompt_embeds_cpu is not None:
            generation_kwargs["pooled_prompt_embeds"] = pooled_prompt_embeds_cpu.to(device)
        # âŒ FLUXä¸æ”¯æŒnegative_pooled_prompt_embedså‚æ•°ï¼Œç§»é™¤  
        # if negative_pooled_prompt_embeds_cpu is not None:
        #     generation_kwargs["negative_pooled_prompt_embeds"] = negative_pooled_prompt_embeds_cpu.to(device)

        # FLUXä½¿ç”¨ä¼ ç»Ÿçš„guidance_scaleå‚æ•°
        generation_kwargs["guidance_scale"] = cfg_scale
        print(f"ğŸ›ï¸ Using guidance_scale: {cfg_scale}")
            
        print(f"ğŸ’¾ GPU Memory before generation: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

    except torch.cuda.OutOfMemoryError as oom_error:
        print(f"âŒ CUDA Out of Memory during img2img encode_prompt: {oom_error}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ Cleared GPU cache after img2img OOM error")
        
        raise RuntimeError(f"GPU memory insufficient for img2img prompt encoding. Please try with shorter prompts or switch to a GPU with more memory. Original error: {oom_error}")
        
    except Exception as e:
        print(f"âš ï¸ Img2Img pipeline.encode_prompt() failed: {e}. Traceback follows.")
        traceback.print_exc()
        
        # Clear cache on any error
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("Falling back to using raw prompts for Img2Img (this will likely cause the original error).")
        generation_kwargs["prompt"] = prompt
        generation_kwargs["negative_prompt"] = negative_prompt

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    # è·å–å½“å‰æ¨¡å‹ç±»å‹ä»¥ç¡®å®šautocastç­–ç•¥
    model_config = BASE_MODELS.get(current_base_model, {})
    model_type = model_config.get("model_type", "unknown")
    
    # ğŸš¨ åŠ¨æ¼«æ¨¡å‹ç¦ç”¨autocasté¿å…LayerNormç²¾åº¦é—®é¢˜
    use_autocast = model_type == "flux"  # åªæœ‰FLUXæ¨¡å‹ä½¿ç”¨autocast

    results = []
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡
    if num_images > 1 and num_images <= 4:  # é™åˆ¶æ‰¹é‡å¤§å°é¿å…å†…å­˜é—®é¢˜
        try:
            print(f"Batch generating {num_images} images for img2img...")
            # ç”Ÿæˆå›¾åƒ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨autocast
            if use_autocast:
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    result = img2img_pipe(
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        image=source_image,
                        strength=denoising_strength,
                        num_inference_steps=steps,
                        guidance_scale=cfg_scale,
                        generator=generator,
                        num_images_per_prompt=num_images
                    )
            else:
                # åŠ¨æ¼«æ¨¡å‹ä¸ä½¿ç”¨autocast
                print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹img2img: è·³è¿‡autocastä½¿ç”¨float32ç²¾åº¦")
                result = img2img_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=source_image,
                    strength=denoising_strength,
                    num_inference_steps=steps,
                    guidance_scale=cfg_scale,
                    generator=generator,
                    num_images_per_prompt=num_images
                )
            
            # å¤„ç†æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡
            for i, image in enumerate(result.images):
                try:
                    # ä¸Šä¼ åˆ° R2
                    image_id = str(uuid.uuid4())
                    filename = f"generated/{image_id}.png"
                    image_bytes = image_to_bytes(image)
                    image_url = upload_to_r2(image_bytes, filename)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    image_data = {
                        'id': image_id,
                        'url': image_url,
                        'prompt': prompt,
                        'negativePrompt': negative_prompt,
                        'seed': seed + i,
                        'width': width,
                        'height': height,
                        'steps': steps,
                        'cfgScale': cfg_scale,
                        'baseModel': base_model,
                        'createdAt': datetime.utcnow().isoformat(),
                        'type': 'image-to-image',
                        'denoisingStrength': denoising_strength
                    }
                    
                    results.append(image_data)
                    
                except Exception as e:
                    print(f"Error processing batch img2img image {i+1}: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"Batch img2img generation failed, falling back to individual generation: {str(e)}")
            # å¦‚æœæ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ ç”Ÿæˆ
            num_images = min(num_images, 1)
    
    # å•å¼ ç”Ÿæˆæˆ–æ‰¹é‡ç”Ÿæˆå¤±è´¥çš„å›é€€
    if len(results) == 0:
        for i in range(num_images):
            try:
                print(f"Generating img2img image {i+1}/{num_images}...")
                # ç”Ÿæˆå›¾åƒ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨autocast
                if use_autocast:
                    with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                        # ä¼˜åŒ–ï¼šæ¸…ç†GPUç¼“å­˜
                        if torch.cuda.is_available() and i > 0:
                            torch.cuda.empty_cache()
                            
                        result = img2img_pipe(
                            prompt=prompt,
                            negative_prompt=negative_prompt,
                            image=source_image,
                            strength=denoising_strength,
                            num_inference_steps=steps,
                            guidance_scale=cfg_scale,
                            generator=generator,
                            num_images_per_prompt=1
                        )
                else:
                    # åŠ¨æ¼«æ¨¡å‹ä¸ä½¿ç”¨autocast
                    print(f"ğŸ’¡ åŠ¨æ¼«æ¨¡å‹img2img: ç”Ÿæˆå›¾ç‰‡{i+1}ä½¿ç”¨float32ç²¾åº¦")
                    if torch.cuda.is_available() and i > 0:
                        torch.cuda.empty_cache()
                        
                    result = img2img_pipe(
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        image=source_image,
                        strength=denoising_strength,
                        num_inference_steps=steps,
                        guidance_scale=cfg_scale,
                        generator=generator,
                        num_images_per_prompt=1
                    )
                
                image = result.images[0]
                
                # ä¸Šä¼ åˆ° R2
                image_id = str(uuid.uuid4())
                filename = f"generated/{image_id}.png"
                image_bytes = image_to_bytes(image)
                image_url = upload_to_r2(image_bytes, filename)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                image_data = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed,
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'baseModel': base_model,
                    'createdAt': datetime.utcnow().isoformat(),
                    'type': 'image-to-image',
                    'denoisingStrength': denoising_strength
                }
                
                results.append(image_data)
                
                # ä¸ºä¸‹ä¸€å¼ å›¾ç‰‡æ›´æ–°ç§å­
                if i < num_images - 1:
                    seed += 1
                    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
                    
            except Exception as e:
                print(f"Error generating img2img image {i+1}: {str(e)}")
                continue
    
    # ä¼˜åŒ–ï¼šæœ€ç»ˆæ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results

def get_available_loras() -> dict:
    """è·å–å¯ç”¨çš„LoRAæ¨¡å‹åˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬ï¼ˆå‰ç«¯é™æ€æ˜¾ç¤ºï¼‰"""
    # å‰ç«¯å·²ç»æœ‰é™æ€åˆ—è¡¨ï¼Œè¿™é‡Œåªè¿”å›åŸºæœ¬ä¿¡æ¯
    return {
        "message": "å‰ç«¯ä½¿ç”¨é™æ€LoRAåˆ—è¡¨ï¼Œåç«¯åŠ¨æ€æœç´¢æ–‡ä»¶",
        "search_paths": LORA_SEARCH_PATHS,
        "current_selected": current_selected_lora,
        "current_base_model": current_base_model
    }

def get_loras_by_base_model() -> dict:
    """è·å–æŒ‰åŸºç¡€æ¨¡å‹åˆ†ç»„çš„LoRAåˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬"""
    return {
        "realistic": [
            {"id": "flux_nsfw", "name": "FLUX NSFW", "description": "NSFWçœŸäººå†…å®¹ç”Ÿæˆæ¨¡å‹"},
            {"id": "chastity_cage", "name": "Chastity Cage", "description": "è´æ“ç¬¼ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "dynamic_penis", "name": "Dynamic Penis", "description": "åŠ¨æ€ç”·æ€§è§£å‰–ç”Ÿæˆ"},
            {"id": "masturbation", "name": "Masturbation", "description": "è‡ªæ…°ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "puppy_mask", "name": "Puppy Mask", "description": "å°ç‹—é¢å…·ä¸»é¢˜å†…å®¹"},
            {"id": "butt_and_feet", "name": "Butt and Feet", "description": "è‡€éƒ¨å’Œè¶³éƒ¨ä¸»é¢˜å†…å®¹"},
            {"id": "cumshots", "name": "Cumshots", "description": "å°„ç²¾ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "uncutpenis", "name": "Uncut Penis", "description": "æœªå‰²åŒ…çš®ä¸»é¢˜å†…å®¹"},
            {"id": "doggystyle", "name": "Doggystyle", "description": "åå…¥å¼ä¸»é¢˜å†…å®¹"},
            {"id": "fisting", "name": "Fisting", "description": "æ‹³äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "on_off", "name": "On Off", "description": "ç©¿è¡£/è„±è¡£å¯¹æ¯”å†…å®¹"},
            {"id": "blowjob", "name": "Blowjob", "description": "å£äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "cum_on_face", "name": "Cum on Face", "description": "é¢œå°„ä¸»é¢˜å†…å®¹ç”Ÿæˆ"}
        ],
        "anime": [
            {"id": "gayporn", "name": "Gayporn", "description": "ç”·åŒåŠ¨æ¼«é£æ ¼å†…å®¹ç”Ÿæˆ"}
        ],
        "current_selected": {
            "realistic": current_selected_lora if current_base_model == "realistic" else "flux_nsfw",
            "anime": "gayporn" if current_base_model == "anime" else "gayporn"
        }
    }

def switch_single_lora(lora_id: str) -> bool:
    """åˆ‡æ¢å•ä¸ªLoRAæ¨¡å‹ï¼ˆä½¿ç”¨åŠ¨æ€æœç´¢ï¼‰"""
    global txt2img_pipe, img2img_pipe, current_lora_config, current_selected_lora
    
    if txt2img_pipe is None:
        raise ValueError("No pipeline loaded, cannot switch LoRA")
    
    # åŠ¨æ€æœç´¢LoRAæ–‡ä»¶
    lora_path = find_lora_file(lora_id, current_base_model)
    
    if not lora_path:
        raise ValueError(f"LoRAæ–‡ä»¶æœªæ‰¾åˆ°: {lora_id}")
    
    # å¦‚æœå·²ç»æ˜¯å½“å‰LoRAï¼Œç›´æ¥è¿”å›
    if lora_id == current_selected_lora:
        print(f"LoRA {lora_id} å·²ç»åŠ è½½ - è·³è¿‡åˆ‡æ¢")
        return True
    
    try:
        print(f"ğŸ”„ åˆ‡æ¢LoRAåˆ°: {lora_id}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {lora_path}")
        
        # å¸è½½å½“å‰LoRA
        if hasattr(txt2img_pipe, 'unload_lora_weights'):
            txt2img_pipe.unload_lora_weights()
            print("ğŸ§¹ å·²å¸è½½ä¹‹å‰çš„LoRA")
        
        # åŠ è½½æ–°çš„LoRA
        txt2img_pipe.load_lora_weights(lora_path)
        print("âœ… æ–°LoRAåŠ è½½æˆåŠŸ")
        
        # åŒæ­¥åˆ°img2imgç®¡é“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if img2img_pipe and hasattr(img2img_pipe, 'load_lora_weights'):
            try:
                if hasattr(img2img_pipe, 'unload_lora_weights'):
                    img2img_pipe.unload_lora_weights()
                img2img_pipe.load_lora_weights(lora_path)
                print("âœ… img2imgç®¡é“LoRAåŒæ­¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  img2imgç®¡é“LoRAåŒæ­¥å¤±è´¥: {e}")
        
        # æ›´æ–°å½“å‰LoRAé…ç½®
        current_lora_config = {lora_id: 1.0}
        current_selected_lora = lora_id
        
        print(f"ğŸ‰ æˆåŠŸåˆ‡æ¢åˆ°LoRA: {lora_id}")
        return True
        
    except Exception as e:
        print(f"âŒ LoRAåˆ‡æ¢å¤±è´¥: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„LoRA
        if current_selected_lora and current_selected_lora != lora_id:
            try:
                previous_lora_path = find_lora_file(current_selected_lora, current_base_model)
                if previous_lora_path:
                    if hasattr(txt2img_pipe, 'unload_lora_weights'):
                        txt2img_pipe.unload_lora_weights()
                    txt2img_pipe.load_lora_weights(previous_lora_path)
                    print(f"ğŸ”„ å·²æ¢å¤åˆ°ä¹‹å‰çš„LoRA: {current_selected_lora}")
            except Exception as recovery_error:
                print(f"âŒ LoRAæ¢å¤å¤±è´¥: {recovery_error}")
        raise RuntimeError(f"LoRAåˆ‡æ¢å¤±è´¥: {str(e)}")

def load_multiple_loras(lora_config: dict) -> bool:
    """åŠ è½½å¤šä¸ªLoRAæ¨¡å‹åˆ°ç®¡é“ä¸­ - ä½¿ç”¨åŠ¨æ€æœç´¢"""
    global txt2img_pipe, img2img_pipe, current_base_model, current_lora_config
    
    if txt2img_pipe is None:
        print("âŒ No pipeline loaded, cannot load LoRAs")
        return False
    
    if not lora_config:
        print("â„¹ï¸  No LoRA configuration provided")
        return True
    
    # è·å–å½“å‰æ¨¡å‹ç±»å‹
    current_model_type = BASE_MODELS.get(current_base_model, {}).get("model_type", "unknown")
    print(f"ğŸ¯ å½“å‰æ¨¡å‹ç±»å‹: {current_model_type}")
    
    try:
        # å…ˆæ¸…ç†ç°æœ‰çš„LoRA
        print("ğŸ§¹ Clearing existing LoRA weights...")
        try:
            txt2img_pipe.unload_lora_weights()
            if img2img_pipe:
                img2img_pipe.unload_lora_weights()
        except Exception as e:
            print(f"âš ï¸  Could not unload previous LoRAs: {e}")
        
        # åŠ¨æ€æœç´¢å¹¶è¿‡æ»¤å…¼å®¹çš„LoRA
        compatible_loras = {}
        for lora_id, weight in lora_config.items():
            if weight <= 0:
                continue
            
            # åŠ¨æ€æœç´¢LoRAæ–‡ä»¶
            lora_path = find_lora_file(lora_id, current_base_model)
            if not lora_path:
                print(f"âš ï¸  LoRAæ–‡ä»¶æœªæ‰¾åˆ°: {lora_id}")
                continue
                
            compatible_loras[lora_id] = {
                "path": lora_path,
                "weight": weight
            }
        
        if not compatible_loras:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°å…¼å®¹çš„LoRAæ¨¡å‹")
            return True
        
        print(f"ğŸ¨ Loading {len(compatible_loras)} compatible LoRA(s): {list(compatible_loras.keys())}")
        
        # åŠ è½½å…¼å®¹çš„LoRA
        lora_paths = []
        lora_weights = []
        
        for lora_id, lora_data in compatible_loras.items():
            lora_paths.append(lora_data["path"])
            lora_weights.append(lora_data["weight"])
            print(f"  ğŸ“¦ {lora_id}: {lora_data['path']} (weight: {lora_data['weight']})")
        
        # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„åŠ è½½æ–¹æ³•
        if current_model_type == "flux":
            # FLUXæ¨¡å‹ä½¿ç”¨load_lora_weights
            txt2img_pipe.load_lora_weights(
                lora_paths[0] if len(lora_paths) == 1 else lora_paths,
                weight_name=None,
                adapter_name=list(compatible_loras.keys())[0] if len(compatible_loras) == 1 else list(compatible_loras.keys())
            )
            
            # è®¾ç½®æƒé‡
            if len(compatible_loras) > 1:
                adapter_weights = {name: weight for name, weight in zip(compatible_loras.keys(), lora_weights)}
                txt2img_pipe.set_adapters(list(compatible_loras.keys()), adapter_weights=list(adapter_weights.values()))
            else:
                # å•ä¸ªLoRA
                adapter_name = list(compatible_loras.keys())[0]
                txt2img_pipe.set_adapters([adapter_name], adapter_weights=[lora_weights[0]])
                
        elif current_model_type == "diffusers":
            # æ ‡å‡†diffusersæ¨¡å‹ä½¿ç”¨load_lora_weights
            if len(compatible_loras) == 1:
                # å•ä¸ªLoRA
                lora_path = lora_paths[0]
                weight = lora_weights[0] 
                txt2img_pipe.load_lora_weights(lora_path)
                txt2img_pipe.cross_attention_kwargs = {"scale": weight}
                
                # åŒæ­¥åˆ°img2imgç®¡é“
                if img2img_pipe:
                    img2img_pipe.load_lora_weights(lora_path)
                    img2img_pipe.cross_attention_kwargs = {"scale": weight}
            else:
                print("âš ï¸  å¤šä¸ªLoRAåŠ è½½æš‚ä¸æ”¯æŒæ ‡å‡†diffusersæ¨¡å‹")
                return False
        
        # æ›´æ–°å½“å‰é…ç½®
        current_lora_config.update(lora_config)
        print(f"âœ… Successfully loaded {len(compatible_loras)} LoRA(s)")
        return True
        
    except Exception as e:
        print(f"âŒ Error loading multiple LoRAs: {e}")
        # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def switch_base_model(base_model_type: str) -> bool:
    """åˆ‡æ¢åŸºç¡€æ¨¡å‹"""
    global current_base_model
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    if current_base_model == base_model_type:
        print(f"Base model {BASE_MODELS[base_model_type]['name']} is already loaded")
        return True
    
    try:
        print(f"Switching base model from {BASE_MODELS[current_base_model]['name']} to {BASE_MODELS[base_model_type]['name']}")
        
        # é‡Šæ”¾å½“å‰æ¨¡å‹å†…å­˜
        global txt2img_pipe, img2img_pipe
        if txt2img_pipe is not None:
            del txt2img_pipe
        if img2img_pipe is not None:
            del img2img_pipe
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # åŠ è½½æ–°çš„åŸºç¡€æ¨¡å‹
        load_specific_model(base_model_type)
        
        print(f"Successfully switched to {BASE_MODELS[base_model_type]['name']}")
        return True
        
    except Exception as e:
        print(f"Failed to switch base model: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„æ¨¡å‹
        try:
            load_specific_model(current_base_model)
            print(f"Recovered to previous model: {BASE_MODELS[current_base_model]['name']}")
        except Exception as recovery_error:
            print(f"Failed to recover base model: {recovery_error}")
        raise RuntimeError(f"Failed to switch base model: {str(e)}")

def handler(job):
    """RunPod å¤„ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        job_input = job['input']
        task_type = job_input.get('task_type')
        
        if task_type == 'get-loras':
            # è·å–å¯ç”¨LoRAåˆ—è¡¨ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            available_loras = get_available_loras()
            return {
                'success': True,
                'data': {
                    'loras': available_loras,
                    'current_config': current_lora_config
                }
            }
            
        elif task_type == 'get-loras-by-model':
            # è·å–æŒ‰åŸºç¡€æ¨¡å‹åˆ†ç»„çš„LoRAåˆ—è¡¨ï¼ˆæ–°çš„å•é€‰UIï¼‰
            loras_by_model = get_loras_by_base_model()
            return {
                'success': True,
                'data': loras_by_model
            }
            
        elif task_type == 'switch-single-lora':
            # åˆ‡æ¢å•ä¸ªLoRAæ¨¡å‹ï¼ˆæ–°çš„å•é€‰æ¨¡å¼ï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            success = switch_single_lora(lora_id)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_selected_lora': current_selected_lora,
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
            
        elif task_type == 'switch-lora':
            # åˆ‡æ¢LoRAæ¨¡å‹ï¼ˆå•ä¸ªLoRAå…¼å®¹æ€§æ”¯æŒï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            # å…¼å®¹å•LoRAåˆ‡æ¢
            single_lora_config = {lora_id: 1.0}
            success = load_multiple_loras(single_lora_config)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
        
        elif task_type == 'load-loras':
            # åŠ è½½å¤šä¸ªLoRAæ¨¡å‹é…ç½®
            lora_config = job_input.get('lora_config', {})
            if not lora_config:
                return {
                    'success': False,
                    'error': 'lora_config is required'
                }
            
            success = load_multiple_loras(lora_config)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Loaded {len([k for k, v in lora_config.items() if v > 0])} LoRA models'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': 'Failed to load LoRA configuration'
                }
        
        elif task_type == 'text-to-image':
            # æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆ
            print("ğŸ“ Processing text-to-image request...")
            
            # æå–å‚æ•°
            prompt = job_input.get('prompt', '')
            negative_prompt = job_input.get('negativePrompt', '') 
            width = job_input.get('width', 1024)
            height = job_input.get('height', 1024)
            steps = job_input.get('steps', 12)
            cfg_scale = job_input.get('cfgScale', 1.0)
            seed = job_input.get('seed', -1)
            num_images = job_input.get('numImages', 1)
            base_model = job_input.get('baseModel', 'realistic')
            lora_config = job_input.get('lora_config', {})
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
            if lora_config and lora_config != current_lora_config:
                print(f"ğŸ¨ æ›´æ–°LoRAé…ç½®: {lora_config}")
                
                # æ£€æŸ¥å½“å‰æ¨¡å‹ç±»å‹
                if current_base_model:
                    model_config = BASE_MODELS.get(current_base_model, {})
                    model_type = model_config.get("model_type", "unknown")
                    print(f"ğŸ¯ å½“å‰æ¨¡å‹ç±»å‹: {model_type}")
                    
                    # æ¸…ç†ç°æœ‰LoRAæƒé‡
                    if txt2img_pipe:
                        try:
                            print("ğŸ§¹ Clearing existing LoRA weights...")
                            txt2img_pipe.unload_lora_weights()
                        except Exception as clear_error:
                            print(f"âš ï¸  æ¸…ç†LoRAæƒé‡æ—¶å‡ºé”™: {clear_error}")
                    
                    # å°è¯•åŠ è½½æ–°çš„LoRAé…ç½®
                    try:
                        if load_multiple_loras(lora_config):
                            print("âœ… LoRAé…ç½®æ›´æ–°æˆåŠŸ")
                        else:
                            print("âš ï¸  LoRAé…ç½®æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
                    except Exception as lora_load_error:
                        print(f"âš ï¸  LoRAåŠ è½½å‡ºé”™: {lora_load_error}")
                        print("â„¹ï¸  ç»§ç»­ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦åˆ‡æ¢
            if base_model != current_base_model:
                print(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {base_model}, å½“å‰åŠ è½½æ¨¡å‹: {current_base_model}")
                print(f"ğŸ”„ éœ€è¦åˆ‡æ¢æ¨¡å‹: {current_base_model} -> {base_model}")
                
                try:
                    load_specific_model(base_model)
                    print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ° {base_model} æ¨¡å‹")
                except Exception as switch_error:
                    print(f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {switch_error}")
                    return {
                        'success': False,
                        'error': f'Failed to switch to {base_model} model: {str(switch_error)}'
                    }
            
            # ğŸš¨ ç¡®ä¿æœ‰æ¨¡å‹åŠ è½½
            if not txt2img_pipe:
                print("âŒ æ²¡æœ‰åŠ è½½ä»»ä½•æ¨¡å‹")
                return {
                    'success': False,
                    'error': 'No model loaded. Please switch to a valid model first.'
                }
            
            # ç”Ÿæˆå›¾åƒ
            try:
                print(f"ğŸ¨ ä½¿ç”¨ {current_base_model} æ¨¡å‹ç”Ÿæˆå›¾åƒ...")
                model_config = BASE_MODELS.get(current_base_model, {})
                model_type = model_config.get("model_type", "unknown")
                
                if model_type == "flux":
                    print("ğŸ’¡ FLUXæ¨¡å‹æ¨è768x768åˆ†è¾¨ç‡")
                    print("ğŸ”§ FLUXæ¨¡å‹ä¼˜åŒ–å‚æ•°(å®˜æ–¹æ¨è): steps=20, cfg_scale=4, size=768x768")
                    images = generate_flux_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, current_base_model)
                elif model_type == "diffusers":
                    print("ğŸ’¡ åŠ¨æ¼«æ¨¡å‹æ¨è1024x1024ä»¥ä¸Šåˆ†è¾¨ç‡")
                    print("ğŸ”§ åŠ¨æ¼«æ¨¡å‹ä¼˜åŒ–å‚æ•°(CivitAIæ¨è): steps=20, cfg_scale=7, size=1024x1024")
                    images = generate_diffusers_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, current_base_model)
                else:
                    print(f"âŒ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
                    return {
                        'success': False,
                        'error': f'Unknown model type: {model_type}'
                    }
                
                # ğŸš¨ æ£€æŸ¥ç”Ÿæˆç»“æœæ˜¯å¦ä¸ºç©º
                if not images or len(images) == 0:
                    print("âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                    return {
                        'success': False,
                        'error': 'Image generation failed - no images were created. This may be due to model compatibility issues or parameter problems.'
                    }
                
                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(images)} å¼ å›¾åƒ")
                return {
                    'success': True,
                    'data': images
                }
                
            except Exception as generation_error:
                print(f"âŒ å›¾åƒç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {generation_error}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                return {
                    'success': False,
                    'error': f'Image generation failed: {str(generation_error)}'
                }
            
        elif task_type == 'image-to-image':
            # å›¾åƒè½¬å›¾åƒç”Ÿæˆ - ä¿®å¤å‚æ•°æå–
            print("ğŸ“ Processing image-to-image request...")
            
            # ç›´æ¥ä»job_inputæå–å‚æ•°ï¼Œè€Œä¸æ˜¯åµŒå¥—çš„paramså¯¹è±¡
            params = {
                'prompt': job_input.get('prompt', ''),
                'negativePrompt': job_input.get('negativePrompt', ''),
                'image': job_input.get('image', ''),
                'width': job_input.get('width', 512),
                'height': job_input.get('height', 512),
                'steps': job_input.get('steps', 20),
                'cfgScale': job_input.get('cfgScale', 7.0),
                'seed': job_input.get('seed', -1),
                'numImages': job_input.get('numImages', 1),
                'denoisingStrength': job_input.get('denoisingStrength', 0.7),
                'baseModel': job_input.get('baseModel', 'realistic'),
                'lora_config': job_input.get('lora_config', {})
            }
            
            requested_lora_config = params.get('lora_config', current_lora_config)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
            if requested_lora_config != current_lora_config:
                print(f"Auto-loading LoRA config for generation: {requested_lora_config}")
                load_multiple_loras(requested_lora_config)
            
            results = image_to_image(params)
            return {
                'success': True,
                'data': results
            }
            
        elif task_type == 'switch-base-model':
            # åˆ‡æ¢åŸºç¡€æ¨¡å‹
            base_model_type = job_input.get('base_model_type')
            if not base_model_type:
                return {
                    'success': False,
                    'error': 'base_model_type is required'
                }
            
            success = switch_base_model(base_model_type)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_base_model': current_base_model
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {base_model_type}'
                }
        
        else:
            return {
                'success': False,
                'error': f'Unknown task type: {task_type}'
            }
            
    except Exception as e:
        print(f"Handler error: {str(e)}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }

# Note: The serverless worker will be started by start_debug.py
# This allows for better debugging and health checks before startup 
# This allows for better debugging and health checks before startup 

# ç®€åŒ–çš„LoRAé…ç½® - å‰ç«¯é™æ€æ˜¾ç¤ºï¼Œåç«¯åŠ¨æ€æœç´¢æ–‡ä»¶
LORA_SEARCH_PATHS = {
    "realistic": [
        "/runpod-volume/lora",
        "/runpod-volume/lora/flux_nsfw",
        "/runpod-volume/lora/realistic"
    ],
    "anime": [
        "/runpod-volume/cartoon/lora",
        "/runpod-volume/anime/lora",
        "/runpod-volume/cartoon"
    ]
}

# LoRAåç§°åˆ°å¯èƒ½æ–‡ä»¶åçš„æ˜ å°„
LORA_FILE_PATTERNS = {
    # çœŸäººé£æ ¼LoRA
    "flux_nsfw": ["flux_nsfw", "flux_nsfw.safetensors"],
    "chastity_cage": ["Chastity_Cage.safetensors", "chastity_cage.safetensors", "ChastityCase.safetensors"],
    "dynamic_penis": ["DynamicPenis.safetensors", "dynamic_penis.safetensors"],
    "masturbation": ["Masturbation.safetensors", "masturbation.safetensors"],
    "puppy_mask": ["Puppy_mask.safetensors", "puppy_mask.safetensors", "PuppyMask.safetensors"],
    "butt_and_feet": ["butt-and-feet.safetensors", "butt_and_feet.safetensors", "ButtAndFeet.safetensors"],
    "cumshots": ["cumshots.safetensors", "Cumshots.safetensors"],
    "uncutpenis": ["uncutpenis.safetensors", "UncutPenis.safetensors", "uncut_penis.safetensors"],
    "doggystyle": ["Doggystyle.safetensors", "doggystyle.safetensors", "doggy_style.safetensors"],
    "fisting": ["Fisting.safetensors", "fisting.safetensors"],
    "on_off": ["OnOff.safetensors", "on_off.safetensors", "onoff.safetensors"],
    "blowjob": ["blowjob.safetensors", "Blowjob.safetensors", "blow_job.safetensors"],
    "cum_on_face": ["cumonface.safetensors", "cum_on_face.safetensors", "CumOnFace.safetensors"],
    
    # åŠ¨æ¼«é£æ ¼LoRA - åŒ…å«æ–°å¢çš„LoRA
    "gayporn": ["Gayporn.safetensor", "Gayporn.safetensors", "gayporn.safetensors", "GayPorn.safetensors"],
    "blowjob_handjob": ["Blowjob_Handjob.safetensors", "blowjob_handjob.safetensors", "BlowjobHandjob.safetensors"],
    "furry": ["Furry.safetensors", "furry.safetensors", "FURRY.safetensors"],
    "sex_slave": ["Sex_slave.safetensors", "sex_slave.safetensors", "SexSlave.safetensors"],
    "comic": ["comic.safetensors", "Comic.safetensors", "COMIC.safetensors"],
    "glory_wall": ["glory_wall.safetensors", "Glory_wall.safetensors", "GloryWall.safetensors"],
    "multiple_views": ["multiple_views.safetensors", "Multiple_views.safetensors", "MultipleViews.safetensors"],
    "pet_play": ["pet_play.safetensors", "Pet_play.safetensors", "PetPlay.safetensors"]
}

def find_lora_file(lora_id: str, base_model: str) -> str:
    """åŠ¨æ€æœç´¢LoRAæ–‡ä»¶è·¯å¾„ - å¢å¼ºæœç´¢é€»è¾‘"""
    search_paths = LORA_SEARCH_PATHS.get(base_model, [])
    file_patterns = LORA_FILE_PATTERNS.get(lora_id, [lora_id])
    
    print(f"ğŸ” æœç´¢LoRAæ–‡ä»¶: {lora_id} (æ¨¡å‹: {base_model})")
    
    for base_path in search_paths:
        if not os.path.exists(base_path):
            print(f"  âŒ è·¯å¾„ä¸å­˜åœ¨: {base_path}")
            continue
            
        print(f"  ğŸ“ æœç´¢ç›®å½•: {base_path}")
        
        # å°è¯•ç²¾ç¡®åŒ¹é…
        for pattern in file_patterns:
            full_path = os.path.join(base_path, pattern)
            if os.path.exists(full_path):
                print(f"  âœ… æ‰¾åˆ°æ–‡ä»¶: {full_path}")
                return full_path
        
        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å«lora_idï¼‰
        try:
            for filename in os.listdir(base_path):
                if filename.endswith(('.safetensors', '.safetensor', '.ckpt', '.pt')):
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«lora_idçš„å…³é”®è¯
                    name_lower = filename.lower()
                    lora_lower = lora_id.lower().replace('_', '').replace('-', '')
                    
                    if lora_lower in name_lower.replace('_', '').replace('-', ''):
                        full_path = os.path.join(base_path, filename)
                        print(f"  âœ… æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°: {full_path}")
                        return full_path
        except Exception as e:
            print(f"  âŒ æœç´¢é”™è¯¯: {e}")
    
    print(f"  âŒ æœªæ‰¾åˆ°LoRAæ–‡ä»¶: {lora_id}")
    return None

# ç§»é™¤å¤æ‚çš„åŠ¨æ€æ‰«æï¼Œä½¿ç”¨ç®€å•çš„é™æ€é…ç½®
# AVAILABLE_LORAS = None
# LORAS_LAST_SCAN = 0
# LORAS_CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜
# åŠ¨æ¼«æ¨¡å‹æ–°å¢LoRAåˆ—è¡¨
ANIME_ADDITIONAL_LORAS = {
    "blowjob_handjob": "/runpod-volume/cartoon/lora/Blowjob_Handjob.safetensors",
    "furry": "/runpod-volume/cartoon/lora/Furry.safetensors", 
    "sex_slave": "/runpod-volume/cartoon/lora/Sex_slave.safetensors",
    "comic": "/runpod-volume/cartoon/lora/comic.safetensors",
    "glory_wall": "/runpod-volume/cartoon/lora/glory_wall.safetensors",
    "multiple_views": "/runpod-volume/cartoon/lora/multiple_views.safetensors",
    "pet_play": "/runpod-volume/cartoon/lora/pet_play.safetensors"
}
