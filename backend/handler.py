import runpod
import torch
from diffusers import FluxPipeline, FluxImg2ImgPipeline, StableDiffusionPipeline, StableDiffusionImg2ImgPipeline
from PIL import Image
import base64
import io
import json
import os
import uuid
from datetime import datetime
import boto3
from botocore.config import Config
import sys
import traceback
import re
import time

# å¯¼å…¥compelç”¨äºå¤„ç†é•¿æç¤ºè¯
try:
    from compel import Compel
    COMPEL_AVAILABLE = True
    print("âœ“ Compel library loaded for long prompt support")
except ImportError:
    COMPEL_AVAILABLE = False
    print("âš ï¸  Compel library not available - long prompt support limited")

# å…¼å®¹æ€§ä¿®å¤ï¼šä¸ºæ—§ç‰ˆæœ¬PyTorchæ·»åŠ get_default_deviceå‡½æ•°
if not hasattr(torch, 'get_default_device'):
    def get_default_device():
        """Fallback implementation for torch.get_default_device()"""
        if torch.cuda.is_available():
            return torch.device('cuda')
        else:
            return torch.device('cpu')
    
    # å°†å‡½æ•°æ·»åŠ åˆ°torchæ¨¡å—ä¸­
    torch.get_default_device = get_default_device
    print("âœ“ Added fallback torch.get_default_device() function")

# æ·»åŠ å¯åŠ¨æ—¥å¿—
print("=== Starting AI Image Generation Backend ===")
print(f"Python version: {sys.version}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

# éªŒè¯å…³é”®ç¯å¢ƒå˜é‡
required_env_vars = [
    "CLOUDFLARE_R2_ACCESS_KEY",
    "CLOUDFLARE_R2_SECRET_KEY", 
    "CLOUDFLARE_R2_BUCKET",
    "CLOUDFLARE_R2_ENDPOINT"
]

missing_vars = []
for var in required_env_vars:
    if not os.getenv(var):
        missing_vars.append(var)

if missing_vars:
    print(f"WARNING: Missing environment variables: {missing_vars}")
    print("Container will start but R2 upload may fail")

# ç¯å¢ƒå˜é‡
CLOUDFLARE_R2_ACCESS_KEY = os.getenv("CLOUDFLARE_R2_ACCESS_KEY")
CLOUDFLARE_R2_SECRET_KEY = os.getenv("CLOUDFLARE_R2_SECRET_KEY") 
CLOUDFLARE_R2_BUCKET = os.getenv("CLOUDFLARE_R2_BUCKET")
CLOUDFLARE_R2_ENDPOINT = os.getenv("CLOUDFLARE_R2_ENDPOINT")
CLOUDFLARE_R2_PUBLIC_DOMAIN = os.getenv("CLOUDFLARE_R2_PUBLIC_DOMAIN")  # å¯é€‰ï¼šè‡ªå®šä¹‰å…¬å…±åŸŸå

# æ¨¡å‹è·¯å¾„
FLUX_BASE_PATH = "/runpod-volume/flux_base"
FLUX_LORA_BASE_PATH = "/runpod-volume/lora"

# é…ç½®åŸºç¡€æ¨¡å‹ç±»å‹å’Œè·¯å¾„
BASE_MODELS = {
    "realistic": {
        "name": "çœŸäººé£æ ¼",
        "path": "/runpod-volume/flux_base",
        "lora_path": "/runpod-volume/lora/flux_nsfw",
        "lora_id": "flux_nsfw",
        "model_type": "flux"
    },
    "anime": {
        "name": "åŠ¨æ¼«é£æ ¼", 
        "path": "/runpod-volume/cartoon/waiNSFWIllustrious_v130.safetensors",
        "lora_path": "/runpod-volume/cartoon/lora/Gayporn.safetensor",
        "lora_id": "gayporn",
        "model_type": "diffusers"
    }
}

# é»˜è®¤LoRAé…ç½® - æ ¹æ®åŸºç¡€æ¨¡å‹ï¼ˆå•é€‰æ¨¡å¼ï¼‰
DEFAULT_LORA_CONFIG = {
    "flux_nsfw": 1.0  # é»˜è®¤ä½¿ç”¨FLUX NSFW
}

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
txt2img_pipe = None
img2img_pipe = None
current_lora_config = DEFAULT_LORA_CONFIG.copy()
current_base_model = None  # åˆå§‹åŒ–æ—¶ä¸é¢„åŠ è½½ä»»ä½•æ¨¡å‹
device_mapping_enabled = False  # Track if device mapping is used
current_selected_lora = "flux_nsfw"  # å½“å‰é€‰æ‹©çš„å•ä¸ªLoRAï¼ˆç”¨äºçœŸäººé£æ ¼ï¼‰

# å…¨å±€å˜é‡å­˜å‚¨compelå¤„ç†å™¨
compel_proc = None
compel_proc_neg = None

# æ”¯æŒçš„LoRAæ¨¡å‹åˆ—è¡¨ - æ›´æ–°ä¸ºæ”¯æŒä¸åŒåŸºç¡€æ¨¡å‹
AVAILABLE_LORAS = None
LORAS_LAST_SCAN = 0
LORAS_CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜

# åˆå§‹åŒ– Cloudflare R2 å®¢æˆ·ç«¯
r2_client = None
if all([CLOUDFLARE_R2_ACCESS_KEY, CLOUDFLARE_R2_SECRET_KEY, CLOUDFLARE_R2_BUCKET, CLOUDFLARE_R2_ENDPOINT]):
    try:
        r2_client = boto3.client(
            's3',
            endpoint_url=CLOUDFLARE_R2_ENDPOINT,
            aws_access_key_id=CLOUDFLARE_R2_ACCESS_KEY,
            aws_secret_access_key=CLOUDFLARE_R2_SECRET_KEY,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        print("âœ“ R2 client initialized successfully")
    except Exception as e:
        print(f"âœ— Failed to initialize R2 client: {e}")
        r2_client = None
else:
    print("âœ— R2 configuration incomplete - R2 upload will be disabled")

def get_device():
    """è·å–è®¾å¤‡ï¼Œå…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬"""
    if torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def load_models():
    """æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œä¸é¢„çƒ­"""
    global txt2img_pipe, img2img_pipe, current_base_model
    
    print("âœ“ æ¨¡å‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå°†æŒ‰éœ€åŠ è½½æ¨¡å‹")
    print(f"ğŸ“ æ”¯æŒçš„æ¨¡å‹ç±»å‹: {list(BASE_MODELS.keys())}")
    
    # ä¸é¢„çƒ­ä»»ä½•æ¨¡å‹ï¼Œç­‰å¾…ç”¨æˆ·è¯·æ±‚æ—¶åŠ è½½
    current_base_model = None
    print("ğŸ¯ ç³»ç»Ÿå°±ç»ªï¼Œç­‰å¾…æ¨¡å‹åŠ è½½è¯·æ±‚...")

def load_flux_model(base_path: str, device: str) -> tuple:
    """åŠ è½½FLUXæ¨¡å‹"""
    # å†…å­˜ä¼˜åŒ–é…ç½®
    model_kwargs = {
        "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
        "use_safetensors": True,
        "low_cpu_mem_usage": True,
    }
    
    # å°è¯•ä½¿ç”¨è®¾å¤‡æ˜ å°„ä¼˜åŒ–
    if device == "cuda":
        try:
            # å…ˆå°è¯• "balanced" ç­–ç•¥
            model_kwargs_with_device_map = model_kwargs.copy()
            model_kwargs_with_device_map["device_map"] = "balanced"
            
            txt2img_pipe = FluxPipeline.from_pretrained(
                base_path,
                **model_kwargs_with_device_map
            )
            print("âœ… Device mapping enabled with 'balanced' strategy")
            
        except Exception as device_map_error:
            print(f"âš ï¸  Device mapping failed ({device_map_error}), loading without device mapping")
            # å›é€€åˆ°ä¸ä½¿ç”¨è®¾å¤‡æ˜ å°„
            txt2img_pipe = FluxPipeline.from_pretrained(
                base_path,
                **model_kwargs
            )
    else:
        # CPUæ¨¡å¼ç›´æ¥åŠ è½½
        txt2img_pipe = FluxPipeline.from_pretrained(
            base_path,
            **model_kwargs
        )
    
    # å¯ç”¨ä¼˜åŒ–
    try:
        txt2img_pipe.enable_attention_slicing()
        print("âœ… Attention slicing enabled")
    except Exception as e:
        print(f"âš ï¸  Attention slicing not available: {e}")
        
    try:
        txt2img_pipe.enable_model_cpu_offload()
        print("âœ… CPU offload enabled")
    except Exception as e:
        print(f"âš ï¸  CPU offload not available: {e}")
    
    try:
        txt2img_pipe.enable_vae_slicing()
        txt2img_pipe.enable_vae_tiling()
        print("âœ… VAE optimizations enabled")
    except Exception as e:
        print(f"âš ï¸  VAE optimizations not available: {e}")
    
    # åˆ›å»ºå›¾ç”Ÿå›¾ç®¡é“
    print("ğŸ”— Creating FLUX image-to-image pipeline (sharing components)...")
    img2img_pipe = FluxImg2ImgPipeline(
        vae=txt2img_pipe.vae,
        text_encoder=txt2img_pipe.text_encoder,
        text_encoder_2=txt2img_pipe.text_encoder_2,
        tokenizer=txt2img_pipe.tokenizer,
        tokenizer_2=txt2img_pipe.tokenizer_2,
        transformer=txt2img_pipe.transformer,
        scheduler=txt2img_pipe.scheduler,
    )
    
    return txt2img_pipe, img2img_pipe

def load_diffusers_model(base_path: str, device: str) -> tuple:
    """åŠ è½½æ ‡å‡†diffusersæ¨¡å‹ - ä¿®å¤Halfç²¾åº¦é—®é¢˜"""
    print(f"ğŸ¨ Loading diffusers model from {base_path}")
    
    # å¼ºåˆ¶ä½¿ç”¨float32ç²¾åº¦ä»¥é¿å…Halfç²¾åº¦é—®é¢˜
    torch_dtype = torch.float32  # ä¿®å¤ LayerNormKernelImpl é”™è¯¯
    
    try:
        # åŠ è½½ä¸»è¦æ–‡æœ¬åˆ°å›¾åƒç®¡é“
        txt2img_pipeline = StableDiffusionPipeline.from_single_file(
            base_path,
            torch_dtype=torch_dtype,
            use_safetensors=True,
            variant="fp32"  # å¼ºåˆ¶ä½¿ç”¨fp32å˜ä½“
        ).to(device)
        
        # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        txt2img_pipeline.enable_attention_slicing()
        txt2img_pipeline.enable_model_cpu_offload()
        
        # åˆ›å»ºå›¾åƒåˆ°å›¾åƒç®¡é“ï¼ˆå…±äº«ç»„ä»¶ï¼‰
        img2img_pipeline = StableDiffusionImg2ImgPipeline(
            vae=txt2img_pipeline.vae,
            text_encoder=txt2img_pipeline.text_encoder,
            tokenizer=txt2img_pipeline.tokenizer,
            unet=txt2img_pipeline.unet,
            scheduler=txt2img_pipeline.scheduler,
            safety_checker=txt2img_pipeline.safety_checker,
            feature_extractor=txt2img_pipeline.feature_extractor,
        ).to(device)
        
        # åŒæ ·çš„ä¼˜åŒ–
        img2img_pipeline.enable_attention_slicing()
        img2img_pipeline.enable_model_cpu_offload()
        
        return txt2img_pipeline, img2img_pipeline
        
    except Exception as e:
        print(f"âŒ Error loading diffusers model: {str(e)}")
        raise e

def load_specific_model(base_model_type: str):
    """åŠ è½½æŒ‡å®šçš„åŸºç¡€æ¨¡å‹ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹"""
    global txt2img_pipe, img2img_pipe, current_base_model, device_mapping_enabled
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    model_config = BASE_MODELS[base_model_type]
    base_path = model_config["path"]
    model_type = model_config["model_type"]
    
    print(f"ğŸ¨ Loading {model_config['name']} model ({model_type}) from {base_path}")
    start_time = datetime.now()
    
    # CUDAå…¼å®¹æ€§æ£€æŸ¥å’Œä¿®å¤
    if torch.cuda.is_available():
        try:
            # æµ‹è¯•CUDAæ˜¯å¦å¯ç”¨
            test_tensor = torch.tensor([1.0]).cuda()
            print("âœ“ CUDA test successful")
            del test_tensor
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"âš ï¸  CUDA error detected: {e}")
            print("âš ï¸  Falling back to CPU mode")
            # å¼ºåˆ¶ä½¿ç”¨CPU
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    device = get_device()
    print(f"ğŸ“± Using device: {device}")
    
    # GPUå†…å­˜ä¼˜åŒ–
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"ğŸ’¾ GPU Memory before loading: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
    
    try:
        # ğŸ¯ æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„åŠ è½½ç­–ç•¥
        if model_type == "flux":
            # FLUXæ¨¡å‹åŠ è½½é€»è¾‘
            print("âš¡ Loading FLUX pipeline with optimizations...")
            txt2img_pipe, img2img_pipe = load_flux_model(base_path, device)
            device_mapping_enabled = True  # FLUXä½¿ç”¨è®¾å¤‡æ˜ å°„
            
        elif model_type == "diffusers":
            # æ ‡å‡†Diffusersæ¨¡å‹åŠ è½½é€»è¾‘
            print("âš¡ Loading standard diffusion pipeline...")
            txt2img_pipe, img2img_pipe = load_diffusers_model(base_path, device)
            device_mapping_enabled = False  # æ ‡å‡†æ¨¡å‹ä¸ä½¿ç”¨è®¾å¤‡æ˜ å°„
            
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        loading_time = (datetime.now() - start_time).total_seconds()
        print(f"â±ï¸  Base model loaded in {loading_time:.2f}s")
        
        # åŠ è½½å¯¹åº”çš„é»˜è®¤ LoRA æƒé‡
        lora_start_time = datetime.now()
        default_lora_path = model_config["lora_path"]
        if os.path.exists(default_lora_path):
            print(f"ğŸ¨ Loading default LoRA for {model_config['name']}: {default_lora_path}")
            try:
                txt2img_pipe.load_lora_weights(default_lora_path)
                lora_time = (datetime.now() - lora_start_time).total_seconds()
                print(f"âœ… LoRA loaded in {lora_time:.2f}s")
                
                # æ›´æ–°å½“å‰LoRAé…ç½®
                global current_lora_config, current_selected_lora
                lora_id = model_config["lora_id"]
                current_lora_config = {lora_id: 1.0}
                current_selected_lora = lora_id
                
            except Exception as e:
                print(f"âš ï¸  LoRA loading failed: {e}")
                print("Continuing without LoRA...")
                current_lora_config = {}
                current_selected_lora = "flux_nsfw" if base_model_type == "realistic" else "gayporn"
        else:
            print(f"âš ï¸  LoRA weights not found at {default_lora_path}")
            current_lora_config = {}
            current_selected_lora = "flux_nsfw" if base_model_type == "realistic" else "gayporn"
        
        # æ›´æ–°å½“å‰åŸºç¡€æ¨¡å‹
        current_base_model = base_model_type
        
        # æœ€ç»ˆå†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            print(f"ğŸ’¾ GPU Memory after loading: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            print(f"ğŸ’¾ GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f}GB")
        
        total_time = (datetime.now() - start_time).total_seconds()
        print(f"ğŸ‰ {model_config['name']} model loaded successfully in {total_time:.2f}s!")
        
        # ğŸ¯ é¢„çƒ­æ¨ç† (å¯é€‰)
        try:
            print("ğŸ”¥ Warming up models with test inference...")
            warmup_start = datetime.now()
            with torch.no_grad():
                # å°å°ºå¯¸é¢„çƒ­æ¨ç†
                test_result = txt2img_pipe(
                    prompt="test",
                    width=512,
                    height=512,
                    num_inference_steps=1,
                    guidance_scale=1.0 if model_type == "flux" else 7.5
                )
            warmup_time = (datetime.now() - warmup_start).total_seconds()
            print(f"âœ… Model warmup completed in {warmup_time:.2f}s")
        except Exception as e:
            print(f"âš ï¸  Model warmup failed (ä¸å½±å“æ­£å¸¸ä½¿ç”¨): {e}")
        
        print(f"ğŸš€ {model_config['name']} system ready for image generation!")

    except Exception as e:
        print(f"âŒ Error loading {model_config['name']} model: {str(e)}")
        traceback.print_exc()
        raise e

def upload_to_r2(image_data: bytes, filename: str) -> str:
    """ä¸Šä¼ å›¾ç‰‡åˆ° Cloudflare R2"""
    try:
        # æ£€æŸ¥R2å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if r2_client is None:
            raise RuntimeError("R2 client not available - check environment variables")
            
        # ç¡®ä¿ image_data æ˜¯ bytes ç±»å‹
        if not isinstance(image_data, bytes):
            raise TypeError(f"Expected bytes, got {type(image_data)}")
            
        # éªŒè¯æ–‡ä»¶å¤§å°
        if len(image_data) == 0:
            raise ValueError("Image data is empty")
            
        if len(image_data) > 50 * 1024 * 1024:  # 50MB limit
            raise ValueError(f"Image too large: {len(image_data)} bytes")
            
        print(f"Uploading {len(image_data)} bytes to R2 as {filename}")
        
        r2_client.put_object(
            Bucket=CLOUDFLARE_R2_BUCKET,
            Key=filename,
            Body=image_data,
            ContentType='image/png',
            ACL='public-read'
        )
        
        # æ„å»ºæ­£ç¡®çš„å…¬å…± URL æ ¼å¼ - Cloudflare R2 public URL format
        # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰å…¬å…±åŸŸåï¼ˆæ¨èï¼Œå¯é¿å…CORSé—®é¢˜ï¼‰
        if CLOUDFLARE_R2_PUBLIC_DOMAIN:
            public_url = f"{CLOUDFLARE_R2_PUBLIC_DOMAIN.rstrip('/')}/{filename}"
            print(f"âœ“ Successfully uploaded to (custom domain): {public_url}")
        else:
            # å›é€€åˆ°æ ‡å‡†R2æ ¼å¼
            # æ­£ç¡®æ ¼å¼: https://{bucket}.{account_id}.r2.cloudflarestorage.com/{filename}
            # ä»endpoint URLä¸­æå–account ID
            account_id = CLOUDFLARE_R2_ENDPOINT.split('//')[1].split('.')[0]
            public_url = f"https://{CLOUDFLARE_R2_BUCKET}.{account_id}.r2.cloudflarestorage.com/{filename}"
            print(f"âœ“ Successfully uploaded to (standard R2): {public_url}")
        
        return public_url
        
    except Exception as e:
        print(f"âœ— Error uploading to R2: {str(e)}")
        print(f"Image data type: {type(image_data)}, size: {len(image_data) if hasattr(image_data, '__len__') else 'unknown'}")
        
        # å¯¹äºæ¼”ç¤ºç›®çš„ï¼Œè¿”å›ä¸€ä¸ªå ä½ç¬¦URLè€Œä¸æ˜¯å¤±è´¥
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æŠ›å‡ºå¼‚å¸¸
        placeholder_url = f"https://via.placeholder.com/512x512/cccccc/666666?text=Upload+Failed"
        print(f"Returning placeholder URL: {placeholder_url}")
        return placeholder_url

def image_to_bytes(image: Image.Image) -> bytes:
    """å°† PIL Image è½¬æ¢ä¸ºå­—èŠ‚"""
    try:
        buffer = io.BytesIO()
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image.save(buffer, format='PNG', quality=95, optimize=True)
        buffer.seek(0)  # é‡ç½®bufferä½ç½®
        return buffer.getvalue()
    except Exception as e:
        print(f"Error converting image to bytes: {str(e)}")
        raise e

def base64_to_image(base64_str: str) -> Image.Image:
    """å°† base64 å­—ç¬¦ä¸²è½¬æ¢ä¸º PIL Image"""
    if base64_str.startswith('data:image'):
        base64_str = base64_str.split(',')[1]
    
    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image.convert('RGB')

def process_long_prompt(prompt: str, max_clip_tokens: int = 75, max_t5_tokens: int = 500) -> tuple:
    """
    å¤„ç†é•¿æç¤ºè¯ï¼Œä¸ºFLUXçš„åŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
    
    Args:
        prompt: è¾“å…¥æç¤ºè¯
        max_clip_tokens: CLIPç¼–ç å™¨æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤75ï¼Œç•™2ä¸ªç‰¹æ®Štokenç©ºé—´ï¼‰
        max_t5_tokens: T5ç¼–ç å™¨æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤500ï¼Œç•™ç©ºé—´ç»™ç‰¹æ®Štokenï¼‰
    
    Returns:
        tuple: (clip_prompt, t5_prompt)
    """
    if not prompt:
        return "", ""
    
    # ğŸ¯ æ›´å‡†ç¡®çš„tokenä¼°ç®—ï¼šè€ƒè™‘æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    # ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼ã€é€—å·ã€æ ‡ç‚¹ç¬¦å·åˆ†å‰²
    token_pattern = r'\w+|[^\w\s]'  # æå–regexæ¨¡å¼é¿å…f-stringä¸­çš„åæ–œæ 
    tokens = re.findall(token_pattern, prompt.lower())
    estimated_tokens = len(tokens)
    
    print(f"ğŸ“ Prompt analysis: {len(prompt)} chars, ~{estimated_tokens} tokens (improved estimation)")
    
    if estimated_tokens <= max_clip_tokens:
        # çŸ­promptï¼šä¸¤ä¸ªç¼–ç å™¨éƒ½ä½¿ç”¨å®Œæ•´prompt
        print("âœ… Short prompt: using full prompt for both CLIP and T5")
        return prompt, prompt
    else:
        # é•¿promptï¼šCLIPä½¿ç”¨æˆªæ–­ç‰ˆæœ¬ï¼ŒT5ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬
        if estimated_tokens <= max_t5_tokens:
            # ğŸ¯ æ›´æ™ºèƒ½çš„CLIPæˆªæ–­ï¼šä¿æŒå®Œæ•´çš„è¯­ä¹‰å•å…ƒ
            words = prompt.split()
            
            # ä»å‰å¾€åç´¯ç§¯tokenï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
            clip_words = []
            current_tokens = 0
            
            for word in words:
                # ä¼°ç®—å½“å‰å•è¯çš„tokenæ•°ï¼ˆè€ƒè™‘æ ‡ç‚¹ç¬¦å·ï¼‰
                word_tokens = len(re.findall(token_pattern, word.lower()))
                
                if current_tokens + word_tokens <= max_clip_tokens:
                    clip_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # å¦‚æœæˆªæ–­ç‚¹ä¸ç†æƒ³ï¼Œå°è¯•åœ¨å¥å·æˆ–é€—å·å¤„æˆªæ–­
            if len(clip_words) > 10:  # åªåœ¨æœ‰è¶³å¤Ÿè¯æ±‡æ—¶ä¼˜åŒ–æˆªæ–­ç‚¹
                for i in range(len(clip_words) - 1, max(0, len(clip_words) - 5), -1):
                    if clip_words[i].endswith(('.', ',', ';', '!')):
                        clip_words = clip_words[:i+1]
                        break
            
            clip_prompt = ' '.join(clip_words)
            clip_token_count = len(re.findall(token_pattern, clip_prompt.lower()))
            
            print(f"ğŸ“ Long prompt optimization:")
            print(f"   CLIP prompt: ~{len(clip_words)} words â†’ {clip_token_count} tokens (safe truncation)")
            print(f"   T5 prompt: ~{estimated_tokens} tokens (full prompt)")
            return clip_prompt, prompt
        else:
            # è¶…é•¿promptï¼šä¸¤ä¸ªç¼–ç å™¨éƒ½éœ€è¦æˆªæ–­
            words = prompt.split()
            
            # CLIPæˆªæ–­
            clip_words = []
            current_tokens = 0
            for word in words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if current_tokens + word_tokens <= max_clip_tokens:
                    clip_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # T5æˆªæ–­
            t5_words = []
            current_tokens = 0
            for word in words:
                word_tokens = len(re.findall(token_pattern, word.lower()))
                if current_tokens + word_tokens <= max_t5_tokens:
                    t5_words.append(word)
                    current_tokens += word_tokens
                else:
                    break
            
            # ä¼˜åŒ–æˆªæ–­ç‚¹
            if len(clip_words) > 10:
                for i in range(len(clip_words) - 1, max(0, len(clip_words) - 5), -1):
                    if clip_words[i].endswith(('.', ',', ';')):
                        clip_words = clip_words[:i+1]
                        break
                        
            if len(t5_words) > 20:
                for i in range(len(t5_words) - 1, max(0, len(t5_words) - 10), -1):
                    if t5_words[i].endswith(('.', ',', ';')):
                        t5_words = t5_words[:i+1]
                        break
            
            clip_prompt = ' '.join(clip_words)
            t5_prompt = ' '.join(t5_words)
            
            clip_token_count = len(re.findall(token_pattern, clip_prompt.lower()))
            t5_token_count = len(re.findall(token_pattern, t5_prompt.lower()))
            
            print(f"âš ï¸  Ultra-long prompt: both encoders truncated intelligently")
            print(f"   CLIP prompt: ~{len(clip_words)} words â†’ {clip_token_count} tokens")
            print(f"   T5 prompt: ~{len(t5_words)} words â†’ {t5_token_count} tokens")
            return clip_prompt, t5_prompt

def generate_flux_images(prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str) -> list:
    """FLUXæ¨¡å‹å›¾åƒç”Ÿæˆ"""
    global txt2img_pipe, device_mapping_enabled
    
    # FLUXæ¨¡å‹åŸç”Ÿæ”¯æŒé•¿æç¤ºè¯ï¼Œä½¿ç”¨ä¼˜åŒ–çš„embeddingå¤„ç†
    generation_kwargs = {
        "width": width,
        "height": height,
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating FLUX prompt embeddings using pipeline.encode_prompt()...")
    try:
        device = get_device()
        
        # Clear GPU cache before encoding
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory before encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            
        # Only try to move text encoders to CPU if device mapping is NOT enabled
        text_encoder_device = None
        text_encoder_2_device = None
        
        try:
            if not device_mapping_enabled:
                print("ğŸ“¦ Manual memory management mode (no device mapping)")
                # Store original devices and move text encoders to CPU
                if hasattr(txt2img_pipe, 'text_encoder') and txt2img_pipe.text_encoder is not None:
                    text_encoder_device = next(txt2img_pipe.text_encoder.parameters()).device
                    if str(text_encoder_device) != 'cpu':
                        print("ğŸ“¦ Moving text_encoder to CPU temporarily to save GPU memory...")
                        txt2img_pipe.text_encoder.to('cpu')
                        torch.cuda.empty_cache()
                        
                if hasattr(txt2img_pipe, 'text_encoder_2') and txt2img_pipe.text_encoder_2 is not None:
                    text_encoder_2_device = next(txt2img_pipe.text_encoder_2.parameters()).device
                    if str(text_encoder_2_device) != 'cpu':
                        print("ğŸ“¦ Moving text_encoder_2 to CPU temporarily to save GPU memory...")
                        txt2img_pipe.text_encoder_2.to('cpu')
                        torch.cuda.empty_cache()
                        print(f"ğŸ’¾ GPU Memory after moving encoders to CPU: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            else:
                print("âš¡ Device mapping mode - trusting accelerate for memory management")

            # Encode positive prompt with memory management
            print("ğŸ”¤ Encoding positive prompt...")
            
            # ğŸ¯ ä¼˜åŒ–é•¿æç¤ºè¯å¤„ç†ï¼šä¸ºFLUXåŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
            clip_prompt, t5_prompt = process_long_prompt(prompt)
            
            with torch.cuda.amp.autocast(enabled=False):  # Disable autocast to reduce memory
                prompt_embeds_obj = txt2img_pipe.encode_prompt(
                    prompt=clip_prompt,    # CLIPç¼–ç å™¨ä½¿ç”¨ä¼˜åŒ–åçš„promptï¼ˆæœ€å¤š77 tokensï¼‰
                    prompt_2=t5_prompt,    # T5ç¼–ç å™¨ä½¿ç”¨å®Œæ•´promptï¼ˆæœ€å¤š512 tokensï¼‰
                    device=device,
                    num_images_per_prompt=1 
                )
            
            # Force move embeddings to CPU immediately
            if hasattr(prompt_embeds_obj, 'prompt_embeds'):
                prompt_embeds_cpu = prompt_embeds_obj.prompt_embeds.cpu()
                pooled_prompt_embeds_cpu = prompt_embeds_obj.pooled_prompt_embeds.cpu() if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds') else None
            else:
                # Handle tuple case
                prompt_embeds_cpu = prompt_embeds_obj[0].cpu() if isinstance(prompt_embeds_obj, tuple) else None
                pooled_prompt_embeds_cpu = prompt_embeds_obj[1].cpu() if isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) > 1 else None
            
            # Clear GPU memory after positive encoding
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory after positive encoding (moved to CPU): {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

            print("âš¡ Skipping negative prompt embedding encoding (FLUX doesn't support negative_prompt_embeds)")
            
        finally:
            # Restore text encoders to original devices (only if we moved them manually)
            if not device_mapping_enabled:
                if text_encoder_device is not None and hasattr(txt2img_pipe, 'text_encoder') and txt2img_pipe.text_encoder is not None:
                    print(f"ğŸ“¦ Restoring text_encoder to {text_encoder_device}...")
                    txt2img_pipe.text_encoder.to(text_encoder_device)
                    
                if text_encoder_2_device is not None and hasattr(txt2img_pipe, 'text_encoder_2') and txt2img_pipe.text_encoder_2 is not None:
                    print(f"ğŸ“¦ Restoring text_encoder_2 to {text_encoder_2_device}...")
                    txt2img_pipe.text_encoder_2.to(text_encoder_2_device)
            else:
                print("âš¡ Skipping text encoder restoration (device mapping handles placement)")
                
            torch.cuda.empty_cache()

        # Now move embeddings back to GPU and assign to generation_kwargs
        print("ğŸš€ Moving embeddings back to GPU for generation...")
        
        # Move embeddings back to GPU when needed  
        generation_kwargs["prompt_embeds"] = prompt_embeds_cpu.to(device)
        
        if pooled_prompt_embeds_cpu is not None:
            generation_kwargs["pooled_prompt_embeds"] = pooled_prompt_embeds_cpu.to(device)

        # FLUXä½¿ç”¨ä¼ ç»Ÿçš„guidance_scaleå‚æ•°
        generation_kwargs["guidance_scale"] = cfg_scale
        print(f"ğŸ›ï¸ Using guidance_scale: {cfg_scale}")
            
        print(f"ğŸ’¾ GPU Memory before generation: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

    except Exception as e:
        print(f"âš ï¸ FLUX pipeline.encode_prompt() failed: {e}. Using raw prompts.")
        generation_kwargs["prompt"] = prompt
        generation_kwargs["negative_prompt"] = negative_prompt

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text-to-image")

def generate_diffusers_images(prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str) -> list:
    """ä½¿ç”¨æ ‡å‡†diffusersç®¡é“ç”Ÿæˆå›¾åƒ - æ”¯æŒé•¿Promptå¤„ç†"""
    global txt2img_pipe
    
    if txt2img_pipe is None:
        raise RuntimeError("Diffusers pipeline not loaded")
    
    # åŠ¨æ¼«æ¨¡å‹ä¹Ÿæ”¯æŒé•¿Promptå¤„ç†
    print(f"ğŸ“ Processing long prompts for anime model...")
    
    # å¤„ç†é•¿Prompt - ä½¿ç”¨Compelåº“æ¥æ”¯æŒæ›´é•¿çš„tokens
    try:
        # ä½¿ç”¨Compelå¤„ç†é•¿prompt
        global compel_proc, compel_proc_neg
        
        if compel_proc is None:
            from compel import Compel
            compel_proc = Compel(
                tokenizer=txt2img_pipe.tokenizer,
                text_encoder=txt2img_pipe.text_encoder,
                truncate_long_prompts=False  # ä¸æˆªæ–­é•¿prompt
            )
            compel_proc_neg = compel_proc  # ä½¿ç”¨åŒä¸€ä¸ªå¤„ç†å™¨
        
        # å¤„ç†æ­£é¢prompt
        print(f"ğŸ”¤ åŸå§‹prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        prompt_embeds = compel_proc(prompt)
        
        # å¤„ç†è´Ÿé¢prompt
        if negative_prompt:
            print(f"ğŸ”¤ åŸå§‹negative prompté•¿åº¦: {len(negative_prompt)} å­—ç¬¦") 
            negative_prompt_embeds = compel_proc_neg(negative_prompt)
        else:
            negative_prompt_embeds = compel_proc_neg("")
            
        print("âœ… é•¿promptå¤„ç†å®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸  Compelå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†å¤„ç†: {e}")
        # å›é€€åˆ°æ ‡å‡†å¤„ç†
        prompt_embeds = None
        negative_prompt_embeds = None
    
    generation_kwargs = {
        "width": width,
        "height": height,
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "num_images_per_prompt": num_images,
        "generator": torch.manual_seed(seed) if seed != -1 else None,
    }
    
    # ä½¿ç”¨prompt embedså¦‚æœå¯ç”¨ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹prompt
    if prompt_embeds is not None:
        generation_kwargs["prompt_embeds"] = prompt_embeds
        generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds
    else:
        generation_kwargs["prompt"] = prompt
        generation_kwargs["negative_prompt"] = negative_prompt
    
    return generate_images_common(generation_kwargs, prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model, "text_to_image")

def generate_images_common(generation_kwargs: dict, prompt: str, negative_prompt: str, width: int, height: int, steps: int, cfg_scale: float, seed: int, num_images: int, base_model: str, task_type: str) -> list:
    """é€šç”¨å›¾åƒç”Ÿæˆé€»è¾‘"""
    global txt2img_pipe
    
    results = []
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡ï¼Œè€Œä¸æ˜¯å¾ªç¯
    if num_images > 1 and num_images <= 4:  # é™åˆ¶æ‰¹é‡å¤§å°é¿å…å†…å­˜é—®é¢˜
        try:
            print(f"Batch generating {num_images} images...")
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                batch_kwargs = generation_kwargs.copy()
                batch_kwargs["num_images_per_prompt"] = num_images
                result = txt2img_pipe(**batch_kwargs)
            
            # å¤„ç†æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡
            for i, image in enumerate(result.images):
                try:
                    # ä¸Šä¼ åˆ° R2
                    image_id = str(uuid.uuid4())
                    filename = f"generated/{image_id}.png"
                    image_bytes = image_to_bytes(image)
                    image_url = upload_to_r2(image_bytes, filename)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    image_data = {
                        'id': image_id,
                        'url': image_url,
                        'prompt': prompt,
                        'negativePrompt': negative_prompt,
                        'seed': seed + i,  # æ¯å¼ å›¾ç‰‡ä¸åŒçš„ç§å­
                        'width': width,
                        'height': height,
                        'steps': steps,
                        'cfgScale': cfg_scale,
                        'baseModel': base_model,
                        'createdAt': datetime.utcnow().isoformat(),
                        'type': task_type
                    }
                    
                    results.append(image_data)
                    
                except Exception as e:
                    print(f"Error processing batch image {i+1}: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"Batch generation failed, falling back to individual generation: {str(e)}")
            # å¦‚æœæ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ ç”Ÿæˆ
            num_images = min(num_images, 1)
    
    # å•å¼ ç”Ÿæˆæˆ–æ‰¹é‡ç”Ÿæˆå¤±è´¥çš„å›é€€
    if len(results) == 0:
        for i in range(num_images):
            try:
                print(f"Generating image {i+1}/{num_images}...")
                # ç”Ÿæˆå›¾åƒ
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    # ä¼˜åŒ–ï¼šæ¸…ç†GPUç¼“å­˜
                    if torch.cuda.is_available() and i > 0:
                        torch.cuda.empty_cache()
                        
                    single_kwargs = generation_kwargs.copy()
                    single_kwargs["num_images_per_prompt"] = 1
                    result = txt2img_pipe(**single_kwargs)
                
                image = result.images[0]
                
                # ä¸Šä¼ åˆ° R2
                image_id = str(uuid.uuid4())
                filename = f"generated/{image_id}.png"
                image_bytes = image_to_bytes(image)
                image_url = upload_to_r2(image_bytes, filename)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                image_data = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed,
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'baseModel': base_model,
                    'createdAt': datetime.utcnow().isoformat(),
                    'type': task_type
                }
                
                results.append(image_data)
                
                # ä¸ºä¸‹ä¸€å¼ å›¾ç‰‡æ›´æ–°ç§å­
                if i < num_images - 1:
                    seed += 1
                    generator = torch.Generator(device=txt2img_pipe.device).manual_seed(seed)
                    generation_kwargs["generator"] = generator
                    
            except Exception as e:
                print(f"Error generating image {i+1}: {str(e)}")
                continue
    
    # ä¼˜åŒ–ï¼šæœ€ç»ˆæ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results

def text_to_image(prompt: str, negative_prompt: str = "", width: int = 1024, height: int = 1024, steps: int = 4, cfg_scale: float = 0.0, seed: int = -1, num_images: int = 1, base_model: str = "realistic") -> list:
    """æ–‡æœ¬ç”Ÿæˆå›¾åƒ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹"""
    global current_base_model, txt2img_pipe
    
    print(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {base_model}, å½“å‰åŠ è½½æ¨¡å‹: {current_base_model}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢æ¨¡å‹
    if current_base_model != base_model:
        print(f"ğŸ”„ éœ€è¦åˆ‡æ¢æ¨¡å‹: {current_base_model} -> {base_model}")
        try:
            load_specific_model(base_model)
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {e}")
            raise e
    
    # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
    if txt2img_pipe is None:
        print(f"âš ï¸  æ¨¡å‹æœªåŠ è½½ï¼ŒåŠ è½½ {base_model} æ¨¡å‹...")
        load_specific_model(base_model)
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = BASE_MODELS.get(base_model)
    if not model_config:
        raise ValueError(f"Unknown base model: {base_model}")
    
    model_type = model_config["model_type"]
    print(f"ğŸ¨ ä½¿ç”¨ {model_type} ç®¡é“ç”Ÿæˆå›¾åƒ...")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ç›¸åº”çš„ç”Ÿæˆå‡½æ•°
    if model_type == "flux":
        return generate_flux_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model)
    elif model_type == "diffusers":
        return generate_diffusers_images(prompt, negative_prompt, width, height, steps, cfg_scale, seed, num_images, base_model)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

def image_to_image(params: dict) -> list:
    """å›¾ç”Ÿå›¾ç”Ÿæˆ - ä¼˜åŒ–ç‰ˆæœ¬"""
    global img2img_pipe, current_base_model
    
    if img2img_pipe is None:
        raise ValueError("Image-to-image model not loaded")
    
    # æå–å‚æ•°
    prompt = params.get('prompt', '')
    negative_prompt = params.get('negativePrompt', '')
    image_data = params.get('image', '')
    width = params.get('width', 512)
    height = params.get('height', 512)
    steps = params.get('steps', 20)
    cfg_scale = params.get('cfgScale', 7.0)
    seed = params.get('seed', -1)
    num_images = params.get('numImages', 1)
    denoising_strength = params.get('denoisingStrength', 0.7)
    base_model = params.get('baseModel', 'realistic')
    lora_config = params.get('lora_config', {})
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åŸºç¡€æ¨¡å‹
    if base_model != current_base_model:
        print(f"Switching base model for generation: {current_base_model} -> {base_model}")
        switch_base_model(base_model)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
    if lora_config and lora_config != current_lora_config:
        print(f"Updating LoRA config for generation: {lora_config}")
        load_multiple_loras(lora_config)
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    if isinstance(image_data, str):
        source_image = base64_to_image(image_data)
    else:
        raise ValueError("Invalid image data format")
    
    # è°ƒæ•´å›¾åƒå°ºå¯¸
    source_image = source_image.resize((width, height), Image.Resampling.LANCZOS)
    
    # ğŸ¯ é•¿æç¤ºè¯æ”¯æŒ - å…¨æ–°æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨FLUXåŸç”Ÿå¤„ç† (é€šè¿‡ pipeline.encode_prompt)
    print(f"ğŸ“ Processing prompt for Img2Img: {len(prompt)} characters")
    
    generation_kwargs = {
        "image": source_image,
        # "prompt": prompt, # Replaced by embeds
        # "negative_prompt": negative_prompt, # Replaced by embeds
        "width": width, 
        "height": height,
        "strength": denoising_strength, # For Img2Img
        "num_inference_steps": steps,
        "guidance_scale": cfg_scale,
        "generator": None,  # ç¨åè®¾ç½®
    }

    # Generate embeds using the pipeline's own encoder for robustness
    print("ğŸ§¬ Generating prompt embeddings for Img2Img using pipeline.encode_prompt()...")
    try:
        device = get_device()
        
        # Clear GPU cache before encoding
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory before img2img encoding: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

        # Only try to move text encoders to CPU if device mapping is NOT enabled
        # Device mapping conflicts with manual component movement
        text_encoder_device = None
        text_encoder_2_device = None
        
        try:
            if not device_mapping_enabled:
                print("ğŸ“¦ Manual memory management mode for img2img (no device mapping)")
                # Store original devices and move text encoders to CPU
                if hasattr(img2img_pipe, 'text_encoder') and img2img_pipe.text_encoder is not None:
                    text_encoder_device = next(img2img_pipe.text_encoder.parameters()).device
                    if str(text_encoder_device) != 'cpu':
                        print("ğŸ“¦ Moving img2img text_encoder to CPU temporarily...")
                        img2img_pipe.text_encoder.to('cpu')
                        torch.cuda.empty_cache()
                        
                if hasattr(img2img_pipe, 'text_encoder_2') and img2img_pipe.text_encoder_2 is not None:
                    text_encoder_2_device = next(img2img_pipe.text_encoder_2.parameters()).device
                    if str(text_encoder_2_device) != 'cpu':
                        print("ğŸ“¦ Moving img2img text_encoder_2 to CPU temporarily...")
                        img2img_pipe.text_encoder_2.to('cpu')
                        torch.cuda.empty_cache()
                        print(f"ğŸ’¾ GPU Memory after moving img2img encoders to CPU: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            else:
                print("âš¡ Device mapping mode for img2img - trusting accelerate for memory management")

            # Encode positive prompt with memory management
            print("ğŸ”¤ Encoding positive prompt for img2img...")
            
            # ğŸ¯ ä¼˜åŒ–é•¿æç¤ºè¯å¤„ç†ï¼šä¸ºFLUXåŒç¼–ç å™¨ç³»ç»Ÿä¼˜åŒ–
            clip_prompt, t5_prompt = process_long_prompt(prompt)
            # FLUXä¸éœ€è¦è´Ÿæç¤ºè¯åµŒå…¥ï¼Œåªå¤„ç†æ­£æç¤ºè¯
            
            with torch.cuda.amp.autocast(enabled=False):
                prompt_embeds_obj = img2img_pipe.encode_prompt(
                    prompt=clip_prompt,    # CLIPç¼–ç å™¨ä½¿ç”¨ä¼˜åŒ–åçš„promptï¼ˆæœ€å¤š77 tokensï¼‰
                    prompt_2=t5_prompt,    # T5ç¼–ç å™¨ä½¿ç”¨å®Œæ•´promptï¼ˆæœ€å¤š512 tokensï¼‰
                    device=device,
                    num_images_per_prompt=1
                )
            
            # Force move embeddings to CPU immediately
            if hasattr(prompt_embeds_obj, 'prompt_embeds'):
                prompt_embeds_cpu = prompt_embeds_obj.prompt_embeds.cpu()
                pooled_prompt_embeds_cpu = prompt_embeds_obj.pooled_prompt_embeds.cpu() if hasattr(prompt_embeds_obj, 'pooled_prompt_embeds') else None
            else:
                prompt_embeds_cpu = prompt_embeds_obj[0].cpu() if isinstance(prompt_embeds_obj, tuple) else None
                pooled_prompt_embeds_cpu = prompt_embeds_obj[1].cpu() if isinstance(prompt_embeds_obj, tuple) and len(prompt_embeds_obj) > 1 else None
            
            # Clear cache after positive encoding
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU Memory after positive img2img encoding (moved to CPU): {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

            # âŒ è·³è¿‡è´Ÿæç¤ºè¯åµŒå…¥ç¼–ç ï¼ŒFLUXä¸æ”¯æŒ
            print("âš¡ Skipping negative prompt embedding encoding for img2img (FLUX doesn't support negative_prompt_embeds)")
            
        finally:
            # Restore text encoders to original devices (only if we moved them manually)
            if not device_mapping_enabled:
                if text_encoder_device is not None and hasattr(img2img_pipe, 'text_encoder') and img2img_pipe.text_encoder is not None:
                    print(f"ğŸ“¦ Restoring img2img text_encoder to {text_encoder_device}...")
                    img2img_pipe.text_encoder.to(text_encoder_device)
                    
                if text_encoder_2_device is not None and hasattr(img2img_pipe, 'text_encoder_2') and img2img_pipe.text_encoder_2 is not None:
                    print(f"ğŸ“¦ Restoring img2img text_encoder_2 to {text_encoder_2_device}...")
                    img2img_pipe.text_encoder_2.to(text_encoder_2_device)
            else:
                print("âš¡ Skipping img2img text encoder restoration (device mapping handles placement)")
                
            torch.cuda.empty_cache()

        # Now move embeddings back to GPU and assign to generation_kwargs
        print("ğŸš€ Moving img2img embeddings back to GPU for generation...")
        
        # Move embeddings back to GPU when needed
        generation_kwargs["prompt_embeds"] = prompt_embeds_cpu.to(device)
        # âŒ FLUXä¸æ”¯æŒnegative_prompt_embedså‚æ•°ï¼Œç§»é™¤
        # generation_kwargs["negative_prompt_embeds"] = negative_prompt_embeds_cpu.to(device)
        
        if pooled_prompt_embeds_cpu is not None:
            generation_kwargs["pooled_prompt_embeds"] = pooled_prompt_embeds_cpu.to(device)
        # âŒ FLUXä¸æ”¯æŒnegative_pooled_prompt_embedså‚æ•°ï¼Œç§»é™¤  
        # if negative_pooled_prompt_embeds_cpu is not None:
        #     generation_kwargs["negative_pooled_prompt_embeds"] = negative_pooled_prompt_embeds_cpu.to(device)

        # FLUXä½¿ç”¨ä¼ ç»Ÿçš„guidance_scaleå‚æ•°
        generation_kwargs["guidance_scale"] = cfg_scale
        print(f"ğŸ›ï¸ Using guidance_scale: {cfg_scale}")
            
        print(f"ğŸ’¾ GPU Memory before generation: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

    except torch.cuda.OutOfMemoryError as oom_error:
        print(f"âŒ CUDA Out of Memory during img2img encode_prompt: {oom_error}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ Cleared GPU cache after img2img OOM error")
        
        raise RuntimeError(f"GPU memory insufficient for img2img prompt encoding. Please try with shorter prompts or switch to a GPU with more memory. Original error: {oom_error}")
        
    except Exception as e:
        print(f"âš ï¸ Img2Img pipeline.encode_prompt() failed: {e}. Traceback follows.")
        traceback.print_exc()
        
        # Clear cache on any error
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("Falling back to using raw prompts for Img2Img (this will likely cause the original error).")
        generation_kwargs["prompt"] = prompt
        generation_kwargs["negative_prompt"] = negative_prompt

    # è®¾ç½®éšæœºç§å­
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    
    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
    generation_kwargs["generator"] = generator

    results = []
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæ—¶ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡
    if num_images > 1 and num_images <= 4:  # é™åˆ¶æ‰¹é‡å¤§å°é¿å…å†…å­˜é—®é¢˜
        try:
            print(f"Batch generating {num_images} images for img2img...")
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                result = img2img_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=source_image,
                    strength=denoising_strength,
                    num_inference_steps=steps,
                    guidance_scale=cfg_scale,
                    generator=generator,
                    num_images_per_prompt=num_images
                )
            
            # å¤„ç†æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡
            for i, image in enumerate(result.images):
                try:
                    # ä¸Šä¼ åˆ° R2
                    image_id = str(uuid.uuid4())
                    filename = f"generated/{image_id}.png"
                    image_bytes = image_to_bytes(image)
                    image_url = upload_to_r2(image_bytes, filename)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    image_data = {
                        'id': image_id,
                        'url': image_url,
                        'prompt': prompt,
                        'negativePrompt': negative_prompt,
                        'seed': seed + i,
                        'width': width,
                        'height': height,
                        'steps': steps,
                        'cfgScale': cfg_scale,
                        'baseModel': base_model,
                        'createdAt': datetime.utcnow().isoformat(),
                        'type': 'image-to-image',
                        'denoisingStrength': denoising_strength
                    }
                    
                    results.append(image_data)
                    
                except Exception as e:
                    print(f"Error processing batch img2img image {i+1}: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"Batch img2img generation failed, falling back to individual generation: {str(e)}")
            # å¦‚æœæ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ ç”Ÿæˆ
            num_images = min(num_images, 1)
    
    # å•å¼ ç”Ÿæˆæˆ–æ‰¹é‡ç”Ÿæˆå¤±è´¥çš„å›é€€
    if len(results) == 0:
        for i in range(num_images):
            try:
                print(f"Generating img2img image {i+1}/{num_images}...")
                # ç”Ÿæˆå›¾åƒ
                with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):
                    # ä¼˜åŒ–ï¼šæ¸…ç†GPUç¼“å­˜
                    if torch.cuda.is_available() and i > 0:
                        torch.cuda.empty_cache()
                        
                    result = img2img_pipe(
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        image=source_image,
                        strength=denoising_strength,
                        num_inference_steps=steps,
                        guidance_scale=cfg_scale,
                        generator=generator,
                        num_images_per_prompt=1
                    )
                
                image = result.images[0]
                
                # ä¸Šä¼ åˆ° R2
                image_id = str(uuid.uuid4())
                filename = f"generated/{image_id}.png"
                image_bytes = image_to_bytes(image)
                image_url = upload_to_r2(image_bytes, filename)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                image_data = {
                    'id': image_id,
                    'url': image_url,
                    'prompt': prompt,
                    'negativePrompt': negative_prompt,
                    'seed': seed,
                    'width': width,
                    'height': height,
                    'steps': steps,
                    'cfgScale': cfg_scale,
                    'baseModel': base_model,
                    'createdAt': datetime.utcnow().isoformat(),
                    'type': 'image-to-image',
                    'denoisingStrength': denoising_strength
                }
                
                results.append(image_data)
                
                # ä¸ºä¸‹ä¸€å¼ å›¾ç‰‡æ›´æ–°ç§å­
                if i < num_images - 1:
                    seed += 1
                    generator = torch.Generator(device=img2img_pipe.device).manual_seed(seed)
                    
            except Exception as e:
                print(f"Error generating img2img image {i+1}: {str(e)}")
                continue
    
    # ä¼˜åŒ–ï¼šæœ€ç»ˆæ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results

def get_available_loras() -> dict:
    """è·å–å¯ç”¨çš„LoRAæ¨¡å‹åˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬ï¼ˆå‰ç«¯é™æ€æ˜¾ç¤ºï¼‰"""
    # å‰ç«¯å·²ç»æœ‰é™æ€åˆ—è¡¨ï¼Œè¿™é‡Œåªè¿”å›åŸºæœ¬ä¿¡æ¯
    return {
        "message": "å‰ç«¯ä½¿ç”¨é™æ€LoRAåˆ—è¡¨ï¼Œåç«¯åŠ¨æ€æœç´¢æ–‡ä»¶",
        "search_paths": LORA_SEARCH_PATHS,
        "current_selected": current_selected_lora,
        "current_base_model": current_base_model
    }

def get_loras_by_base_model() -> dict:
    """è·å–æŒ‰åŸºç¡€æ¨¡å‹åˆ†ç»„çš„LoRAåˆ—è¡¨ - ç®€åŒ–ç‰ˆæœ¬"""
    return {
        "realistic": [
            {"id": "flux_nsfw", "name": "FLUX NSFW", "description": "NSFWçœŸäººå†…å®¹ç”Ÿæˆæ¨¡å‹"},
            {"id": "chastity_cage", "name": "Chastity Cage", "description": "è´æ“ç¬¼ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "dynamic_penis", "name": "Dynamic Penis", "description": "åŠ¨æ€ç”·æ€§è§£å‰–ç”Ÿæˆ"},
            {"id": "masturbation", "name": "Masturbation", "description": "è‡ªæ…°ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "puppy_mask", "name": "Puppy Mask", "description": "å°ç‹—é¢å…·ä¸»é¢˜å†…å®¹"},
            {"id": "butt_and_feet", "name": "Butt and Feet", "description": "è‡€éƒ¨å’Œè¶³éƒ¨ä¸»é¢˜å†…å®¹"},
            {"id": "cumshots", "name": "Cumshots", "description": "å°„ç²¾ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "uncutpenis", "name": "Uncut Penis", "description": "æœªå‰²åŒ…çš®ä¸»é¢˜å†…å®¹"},
            {"id": "doggystyle", "name": "Doggystyle", "description": "åå…¥å¼ä¸»é¢˜å†…å®¹"},
            {"id": "fisting", "name": "Fisting", "description": "æ‹³äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "on_off", "name": "On Off", "description": "ç©¿è¡£/è„±è¡£å¯¹æ¯”å†…å®¹"},
            {"id": "blowjob", "name": "Blowjob", "description": "å£äº¤ä¸»é¢˜å†…å®¹ç”Ÿæˆ"},
            {"id": "cum_on_face", "name": "Cum on Face", "description": "é¢œå°„ä¸»é¢˜å†…å®¹ç”Ÿæˆ"}
        ],
        "anime": [
            {"id": "gayporn", "name": "Gayporn", "description": "ç”·åŒåŠ¨æ¼«é£æ ¼å†…å®¹ç”Ÿæˆ"}
        ],
        "current_selected": {
            "realistic": current_selected_lora if current_base_model == "realistic" else "flux_nsfw",
            "anime": "gayporn" if current_base_model == "anime" else "gayporn"
        }
    }

def switch_single_lora(lora_id: str) -> bool:
    """åˆ‡æ¢å•ä¸ªLoRAæ¨¡å‹ï¼ˆä½¿ç”¨åŠ¨æ€æœç´¢ï¼‰"""
    global txt2img_pipe, img2img_pipe, current_lora_config, current_selected_lora
    
    if txt2img_pipe is None:
        raise ValueError("No pipeline loaded, cannot switch LoRA")
    
    # åŠ¨æ€æœç´¢LoRAæ–‡ä»¶
    lora_path = find_lora_file(lora_id, current_base_model)
    
    if not lora_path:
        raise ValueError(f"LoRAæ–‡ä»¶æœªæ‰¾åˆ°: {lora_id}")
    
    # å¦‚æœå·²ç»æ˜¯å½“å‰LoRAï¼Œç›´æ¥è¿”å›
    if lora_id == current_selected_lora:
        print(f"LoRA {lora_id} å·²ç»åŠ è½½ - è·³è¿‡åˆ‡æ¢")
        return True
    
    try:
        print(f"ğŸ”„ åˆ‡æ¢LoRAåˆ°: {lora_id}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {lora_path}")
        
        # å¸è½½å½“å‰LoRA
        if hasattr(txt2img_pipe, 'unload_lora_weights'):
            txt2img_pipe.unload_lora_weights()
            print("ğŸ§¹ å·²å¸è½½ä¹‹å‰çš„LoRA")
        
        # åŠ è½½æ–°çš„LoRA
        txt2img_pipe.load_lora_weights(lora_path)
        print("âœ… æ–°LoRAåŠ è½½æˆåŠŸ")
        
        # åŒæ­¥åˆ°img2imgç®¡é“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if img2img_pipe and hasattr(img2img_pipe, 'load_lora_weights'):
            try:
                if hasattr(img2img_pipe, 'unload_lora_weights'):
                    img2img_pipe.unload_lora_weights()
                img2img_pipe.load_lora_weights(lora_path)
                print("âœ… img2imgç®¡é“LoRAåŒæ­¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  img2imgç®¡é“LoRAåŒæ­¥å¤±è´¥: {e}")
        
        # æ›´æ–°å½“å‰LoRAé…ç½®
        current_lora_config = {lora_id: 1.0}
        current_selected_lora = lora_id
        
        print(f"ğŸ‰ æˆåŠŸåˆ‡æ¢åˆ°LoRA: {lora_id}")
        return True
        
    except Exception as e:
        print(f"âŒ LoRAåˆ‡æ¢å¤±è´¥: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„LoRA
        if current_selected_lora and current_selected_lora != lora_id:
            try:
                previous_lora_path = find_lora_file(current_selected_lora, current_base_model)
                if previous_lora_path:
                    if hasattr(txt2img_pipe, 'unload_lora_weights'):
                        txt2img_pipe.unload_lora_weights()
                    txt2img_pipe.load_lora_weights(previous_lora_path)
                    print(f"ğŸ”„ å·²æ¢å¤åˆ°ä¹‹å‰çš„LoRA: {current_selected_lora}")
            except Exception as recovery_error:
                print(f"âŒ LoRAæ¢å¤å¤±è´¥: {recovery_error}")
        raise RuntimeError(f"LoRAåˆ‡æ¢å¤±è´¥: {str(e)}")

def load_multiple_loras(lora_config: dict) -> bool:
    """åŠ è½½å¤šä¸ªLoRAæ¨¡å‹åˆ°ç®¡é“ä¸­ - ä½¿ç”¨åŠ¨æ€æœç´¢"""
    global txt2img_pipe, img2img_pipe, current_base_model, current_lora_config
    
    if txt2img_pipe is None:
        print("âŒ No pipeline loaded, cannot load LoRAs")
        return False
    
    if not lora_config:
        print("â„¹ï¸  No LoRA configuration provided")
        return True
    
    # è·å–å½“å‰æ¨¡å‹ç±»å‹
    current_model_type = BASE_MODELS.get(current_base_model, {}).get("model_type", "unknown")
    print(f"ğŸ¯ å½“å‰æ¨¡å‹ç±»å‹: {current_model_type}")
    
    try:
        # å…ˆæ¸…ç†ç°æœ‰çš„LoRA
        print("ğŸ§¹ Clearing existing LoRA weights...")
        try:
            txt2img_pipe.unload_lora_weights()
            if img2img_pipe:
                img2img_pipe.unload_lora_weights()
        except Exception as e:
            print(f"âš ï¸  Could not unload previous LoRAs: {e}")
        
        # åŠ¨æ€æœç´¢å¹¶è¿‡æ»¤å…¼å®¹çš„LoRA
        compatible_loras = {}
        for lora_id, weight in lora_config.items():
            if weight <= 0:
                continue
            
            # åŠ¨æ€æœç´¢LoRAæ–‡ä»¶
            lora_path = find_lora_file(lora_id, current_base_model)
            if not lora_path:
                print(f"âš ï¸  LoRAæ–‡ä»¶æœªæ‰¾åˆ°: {lora_id}")
                continue
                
            compatible_loras[lora_id] = {
                "path": lora_path,
                "weight": weight
            }
        
        if not compatible_loras:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°å…¼å®¹çš„LoRAæ¨¡å‹")
            return True
        
        print(f"ğŸ¨ Loading {len(compatible_loras)} compatible LoRA(s): {list(compatible_loras.keys())}")
        
        # åŠ è½½å…¼å®¹çš„LoRA
        lora_paths = []
        lora_weights = []
        
        for lora_id, lora_data in compatible_loras.items():
            lora_paths.append(lora_data["path"])
            lora_weights.append(lora_data["weight"])
            print(f"  ğŸ“¦ {lora_id}: {lora_data['path']} (weight: {lora_data['weight']})")
        
        # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„åŠ è½½æ–¹æ³•
        if current_model_type == "flux":
            # FLUXæ¨¡å‹ä½¿ç”¨load_lora_weights
            txt2img_pipe.load_lora_weights(
                lora_paths[0] if len(lora_paths) == 1 else lora_paths,
                weight_name=None,
                adapter_name=list(compatible_loras.keys())[0] if len(compatible_loras) == 1 else list(compatible_loras.keys())
            )
            
            # è®¾ç½®æƒé‡
            if len(compatible_loras) > 1:
                adapter_weights = {name: weight for name, weight in zip(compatible_loras.keys(), lora_weights)}
                txt2img_pipe.set_adapters(list(compatible_loras.keys()), adapter_weights=list(adapter_weights.values()))
            else:
                # å•ä¸ªLoRA
                adapter_name = list(compatible_loras.keys())[0]
                txt2img_pipe.set_adapters([adapter_name], adapter_weights=[lora_weights[0]])
                
        elif current_model_type == "diffusers":
            # æ ‡å‡†diffusersæ¨¡å‹ä½¿ç”¨load_lora_weights
            if len(compatible_loras) == 1:
                # å•ä¸ªLoRA
                lora_path = lora_paths[0]
                weight = lora_weights[0] 
                txt2img_pipe.load_lora_weights(lora_path)
                txt2img_pipe.cross_attention_kwargs = {"scale": weight}
                
                # åŒæ­¥åˆ°img2imgç®¡é“
                if img2img_pipe:
                    img2img_pipe.load_lora_weights(lora_path)
                    img2img_pipe.cross_attention_kwargs = {"scale": weight}
            else:
                print("âš ï¸  å¤šä¸ªLoRAåŠ è½½æš‚ä¸æ”¯æŒæ ‡å‡†diffusersæ¨¡å‹")
                return False
        
        # æ›´æ–°å½“å‰é…ç½®
        current_lora_config.update(lora_config)
        print(f"âœ… Successfully loaded {len(compatible_loras)} LoRA(s)")
        return True
        
    except Exception as e:
        print(f"âŒ Error loading multiple LoRAs: {e}")
        # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def switch_base_model(base_model_type: str) -> bool:
    """åˆ‡æ¢åŸºç¡€æ¨¡å‹"""
    global current_base_model
    
    if base_model_type not in BASE_MODELS:
        raise ValueError(f"Unknown base model type: {base_model_type}")
    
    if current_base_model == base_model_type:
        print(f"Base model {BASE_MODELS[base_model_type]['name']} is already loaded")
        return True
    
    try:
        print(f"Switching base model from {BASE_MODELS[current_base_model]['name']} to {BASE_MODELS[base_model_type]['name']}")
        
        # é‡Šæ”¾å½“å‰æ¨¡å‹å†…å­˜
        global txt2img_pipe, img2img_pipe
        if txt2img_pipe is not None:
            del txt2img_pipe
        if img2img_pipe is not None:
            del img2img_pipe
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # åŠ è½½æ–°çš„åŸºç¡€æ¨¡å‹
        load_specific_model(base_model_type)
        
        print(f"Successfully switched to {BASE_MODELS[base_model_type]['name']}")
        return True
        
    except Exception as e:
        print(f"Failed to switch base model: {str(e)}")
        # å°è¯•æ¢å¤åˆ°ä¹‹å‰çš„æ¨¡å‹
        try:
            load_specific_model(current_base_model)
            print(f"Recovered to previous model: {BASE_MODELS[current_base_model]['name']}")
        except Exception as recovery_error:
            print(f"Failed to recover base model: {recovery_error}")
        raise RuntimeError(f"Failed to switch base model: {str(e)}")

def handler(job):
    """RunPod å¤„ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        job_input = job['input']
        task_type = job_input.get('task_type')
        
        if task_type == 'get-loras':
            # è·å–å¯ç”¨LoRAåˆ—è¡¨ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            available_loras = get_available_loras()
            return {
                'success': True,
                'data': {
                    'loras': available_loras,
                    'current_config': current_lora_config
                }
            }
            
        elif task_type == 'get-loras-by-model':
            # è·å–æŒ‰åŸºç¡€æ¨¡å‹åˆ†ç»„çš„LoRAåˆ—è¡¨ï¼ˆæ–°çš„å•é€‰UIï¼‰
            loras_by_model = get_loras_by_base_model()
            return {
                'success': True,
                'data': loras_by_model
            }
            
        elif task_type == 'switch-single-lora':
            # åˆ‡æ¢å•ä¸ªLoRAæ¨¡å‹ï¼ˆæ–°çš„å•é€‰æ¨¡å¼ï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            success = switch_single_lora(lora_id)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_selected_lora': current_selected_lora,
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
            
        elif task_type == 'switch-lora':
            # åˆ‡æ¢LoRAæ¨¡å‹ï¼ˆå•ä¸ªLoRAå…¼å®¹æ€§æ”¯æŒï¼‰
            lora_id = job_input.get('lora_id')
            if not lora_id:
                return {
                    'success': False,
                    'error': 'lora_id is required'
                }
            
            # å…¼å®¹å•LoRAåˆ‡æ¢
            single_lora_config = {lora_id: 1.0}
            success = load_multiple_loras(single_lora_config)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Switched to {AVAILABLE_LORAS[lora_id]["name"]}'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {lora_id}'
                }
        
        elif task_type == 'load-loras':
            # åŠ è½½å¤šä¸ªLoRAæ¨¡å‹é…ç½®
            lora_config = job_input.get('lora_config', {})
            if not lora_config:
                return {
                    'success': False,
                    'error': 'lora_config is required'
                }
            
            success = load_multiple_loras(lora_config)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_config': current_lora_config,
                        'message': f'Loaded {len([k for k, v in lora_config.items() if v > 0])} LoRA models'
                    }
                }
            else:
                return {
                    'success': False,
                    'error': 'Failed to load LoRA configuration'
                }
        
        elif task_type == 'text-to-image':
            # æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆ
            print("ğŸ“ Processing text-to-image request...")
            
            # æå–å‚æ•°
            prompt = job_input.get('prompt', '')
            negative_prompt = job_input.get('negativePrompt', '') 
            width = job_input.get('width', 1024)
            height = job_input.get('height', 1024)
            steps = job_input.get('steps', 4)
            cfg_scale = job_input.get('cfgScale', 0.0)
            seed = job_input.get('seed', -1)
            num_images = job_input.get('numImages', 1)
            base_model = job_input.get('baseModel', 'realistic')
            lora_config = job_input.get('lora_config', {})
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
            if lora_config and lora_config != current_lora_config:
                print(f"ğŸ¨ æ›´æ–°LoRAé…ç½®: {lora_config}")
                load_multiple_loras(lora_config)
            
            # ç”Ÿæˆå›¾åƒ
            images = text_to_image(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                steps=steps,
                cfg_scale=cfg_scale,
                seed=seed,
                num_images=num_images,
                base_model=base_model
            )
            return {
                'success': True,
                'data': images
            }
            
        elif task_type == 'image-to-image':
            # ä¼˜åŒ–ï¼šæ”¯æŒå¤šLoRAé…ç½®
            params = job_input.get('params', {})
            requested_lora_config = params.get('lora_config', current_lora_config)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°LoRAé…ç½®
            if requested_lora_config != current_lora_config:
                print(f"Auto-loading LoRA config for generation: {requested_lora_config}")
                load_multiple_loras(requested_lora_config)
            
            results = image_to_image(params)
            return {
                'success': True,
                'data': results
            }
            
        elif task_type == 'switch-base-model':
            # åˆ‡æ¢åŸºç¡€æ¨¡å‹
            base_model_type = job_input.get('base_model_type')
            if not base_model_type:
                return {
                    'success': False,
                    'error': 'base_model_type is required'
                }
            
            success = switch_base_model(base_model_type)
            
            if success:
                return {
                    'success': True,
                    'data': {
                        'current_base_model': current_base_model
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to switch to {base_model_type}'
                }
        
        else:
            return {
                'success': False,
                'error': f'Unknown task type: {task_type}'
            }
            
    except Exception as e:
        print(f"Handler error: {str(e)}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }

# Note: The serverless worker will be started by start_debug.py
# This allows for better debugging and health checks before startup 
# This allows for better debugging and health checks before startup 

# ç®€åŒ–çš„LoRAé…ç½® - å‰ç«¯é™æ€æ˜¾ç¤ºï¼Œåç«¯åŠ¨æ€æœç´¢æ–‡ä»¶
LORA_SEARCH_PATHS = {
    "realistic": [
        "/runpod-volume/lora",
        "/runpod-volume/lora/realistic"
    ],
    "anime": [
        "/runpod-volume/cartoon/lora",
        "/runpod-volume/anime/lora"
    ]
}

# LoRAåç§°åˆ°å¯èƒ½æ–‡ä»¶åçš„æ˜ å°„
LORA_FILE_PATTERNS = {
    # çœŸäººé£æ ¼LoRA
    "flux_nsfw": ["flux_nsfw", "flux_nsfw.safetensors"],
    "chastity_cage": ["Chastity_Cage.safetensors", "chastity_cage.safetensors", "ChastityCase.safetensors"],
    "dynamic_penis": ["DynamicPenis.safetensors", "dynamic_penis.safetensors"],
    "masturbation": ["Masturbation.safetensors", "masturbation.safetensors"],
    "puppy_mask": ["Puppy_mask.safetensors", "puppy_mask.safetensors", "PuppyMask.safetensors"],
    "butt_and_feet": ["butt-and-feet.safetensors", "butt_and_feet.safetensors", "ButtAndFeet.safetensors"],
    "cumshots": ["cumshots.safetensors", "Cumshots.safetensors"],
    "uncutpenis": ["uncutpenis.safetensors", "UncutPenis.safetensors", "uncut_penis.safetensors"],
    "doggystyle": ["Doggystyle.safetensors", "doggystyle.safetensors", "doggy_style.safetensors"],
    "fisting": ["Fisting.safetensors", "fisting.safetensors"],
    "on_off": ["OnOff.safetensors", "on_off.safetensors", "onoff.safetensors"],
    "blowjob": ["blowjob.safetensors", "Blowjob.safetensors", "blow_job.safetensors"],
    "cum_on_face": ["cumonface.safetensors", "cum_on_face.safetensors", "CumOnFace.safetensors"],
    
    # åŠ¨æ¼«é£æ ¼LoRA
    "gayporn": ["Gayporn.safetensor", "gayporn.safetensors", "GayPorn.safetensors"]
}

def find_lora_file(lora_id: str, base_model: str) -> str:
    """åŠ¨æ€æœç´¢LoRAæ–‡ä»¶è·¯å¾„"""
    search_paths = LORA_SEARCH_PATHS.get(base_model, [])
    file_patterns = LORA_FILE_PATTERNS.get(lora_id, [lora_id])
    
    print(f"ğŸ” æœç´¢LoRAæ–‡ä»¶: {lora_id} (æ¨¡å‹: {base_model})")
    
    for base_path in search_paths:
        if not os.path.exists(base_path):
            print(f"  âŒ è·¯å¾„ä¸å­˜åœ¨: {base_path}")
            continue
            
        print(f"  ğŸ“ æœç´¢ç›®å½•: {base_path}")
        
        # å°è¯•ç²¾ç¡®åŒ¹é…
        for pattern in file_patterns:
            full_path = os.path.join(base_path, pattern)
            if os.path.exists(full_path):
                print(f"  âœ… æ‰¾åˆ°æ–‡ä»¶: {full_path}")
                return full_path
        
        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å«lora_idï¼‰
        try:
            for filename in os.listdir(base_path):
                if filename.endswith(('.safetensors', '.ckpt', '.pt')):
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«lora_idçš„å…³é”®è¯
                    name_lower = filename.lower()
                    lora_lower = lora_id.lower().replace('_', '').replace('-', '')
                    
                    if lora_lower in name_lower.replace('_', '').replace('-', ''):
                        full_path = os.path.join(base_path, filename)
                        print(f"  âœ… æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°: {full_path}")
                        return full_path
        except Exception as e:
            print(f"  âŒ æœç´¢é”™è¯¯: {e}")
    
    print(f"  âŒ æœªæ‰¾åˆ°LoRAæ–‡ä»¶: {lora_id}")
    return None

# ç§»é™¤å¤æ‚çš„åŠ¨æ€æ‰«æï¼Œä½¿ç”¨ç®€å•çš„é™æ€é…ç½®
# AVAILABLE_LORAS = None
# LORAS_LAST_SCAN = 0
# LORAS_CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜