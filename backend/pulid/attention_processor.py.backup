"""
PuLIDæ³¨æ„åŠ›å¤„ç†å™¨ - å®ç°äº†é€‚ç”¨äºFLUXæ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å¤„ç†
ä¿®å¤ç‰ˆæœ¬ï¼šæ­£ç¡®é›†æˆPerceiverAttentionCAï¼Œå®ç°é¢éƒ¨ç‰¹å¾æ³¨å…¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional

class FluxAttenProc:
    """
    FLUXæ¨¡å‹çš„PuLIDæ³¨æ„åŠ›å¤„ç†å™¨ï¼Œç”¨äºåœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­æ³¨å…¥èº«ä»½ç‰¹å¾ã€‚
    ä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨PerceiverAttentionCAå®ç°çœŸæ­£çš„ç‰¹å¾æ³¨å…¥ã€‚
    """
    
    def __init__(self, id_embedding=None, id_weight=1.0, ca_layer=None):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›å¤„ç†å™¨
        
        Args:
            id_embedding: ID embeddingå¼ é‡ [1, 32, 768]
            id_weight: ID embeddingçš„æƒé‡/å¼ºåº¦
            ca_layer: (å·²å¼ƒç”¨ï¼Œä¸ºå…¼å®¹æ€§ä¿ç•™) ä¸å†ä½¿ç”¨å¤æ‚çš„CAå±‚
        """
        self.id_embedding = id_embedding
        self.id_weight = id_weight
        self.ca_layer = ca_layer  # ä¸ºäº†æµ‹è¯•å…¼å®¹æ€§ä¿ç•™
        
        # ğŸ”§ åˆå§‹åŒ–æ³¨å…¥æ§åˆ¶å™¨ï¼Œé˜²æ­¢è¿‡åº¦æ³¨å…¥
        self._injection_counter = True  # å¯ç”¨æ³¨å…¥æ§åˆ¶
        self._current_step = 0
        
        if self.id_embedding is not None:
            print(f"ğŸ”§ PuLIDå¤„ç†å™¨åˆå§‹åŒ–: embeddingå½¢çŠ¶={id_embedding.shape}, æƒé‡={id_weight}, ä¿å®ˆæ³¨å…¥æ¨¡å¼")
    
    def __call__(
        self,
        attn: nn.Module,
        hidden_states: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        temb: Optional[torch.Tensor] = None,
        scale: float = 1.0,
        **cross_attention_kwargs
    ):
        """
        å¤„ç†æ³¨æ„åŠ›æœºåˆ¶ - PuLIDæ³¨å…¥å·²ç¦ç”¨ä»¥é˜²æ­¢å›¾åƒæŸå
        """
        # ğŸ”§ é™é»˜å¤„ç†FLUXç‰¹æœ‰å‚æ•°ï¼Œé¿å…æ—¥å¿—æ±¡æŸ“
        # å¸¸è§çš„FLUXå‚æ•°ï¼šimage_rotary_emb, rope_scalingç­‰
        if cross_attention_kwargs and len(cross_attention_kwargs) > 0:
            # åªåœ¨ç¬¬ä¸€æ¬¡é‡åˆ°æ—¶æ‰“å°è­¦å‘Šï¼Œé¿å…æ—¥å¿—spam
            if not hasattr(self, '_kwargs_warned'):
                self._kwargs_warned = True
                print(f"â„¹ï¸ FLUXç‰¹æœ‰å‚æ•°å·²å¿½ç•¥: {list(cross_attention_kwargs.keys())}")
        
        # ğŸ”§ å®Œå…¨ç¦ç”¨PuLIDæ³¨å…¥ï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†FLUXæ³¨æ„åŠ›
        # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸åŸç”ŸFLUXå®Œå…¨å…¼å®¹ï¼Œé¿å…ä»»ä½•å›¾åƒæŸå
        try:
            return self._safe_flux_attention(attn, hidden_states, encoder_hidden_states, attention_mask, temb, scale)
        except Exception as e:
            print(f"âš ï¸ FLUXæ³¨æ„åŠ›å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ³¨æ„åŠ›: {e}")
            # å›é€€åˆ°æœ€ç®€å•çš„æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
            return self._fallback_attention(attn, hidden_states, encoder_hidden_states, attention_mask, temb)
    
    def _safe_flux_attention(
        self,
        attn: nn.Module,
        hidden_states: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        temb: Optional[torch.Tensor] = None,
        scale: float = 1.0
    ):
        """
        çº¯FLUXæ³¨æ„åŠ›å¤„ç† - æ— PuLIDæ³¨å…¥ä»¥ç¡®ä¿å›¾åƒè´¨é‡
        """
        # å…ˆæ‰§è¡Œæ ‡å‡†çš„æ³¨æ„åŠ›é¢„å¤„ç†
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
            
        input_ndim = hidden_states.ndim

        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)

        batch_size, sequence_length, _ = (
            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        )
        
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])

        # å¤„ç†éšè—çŠ¶æ€ä»¥è·å–æŸ¥è¯¢ã€é”®å’Œå€¼
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
            
        query = attn.to_q(hidden_states)
        
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        
        # ğŸ”§ ç´§æ€¥ä¿®å¤ï¼šå®Œå…¨ç¦ç”¨PuLIDæ³¨å…¥ä»¥é˜²æ­¢å›¾åƒæŸå
        # æ ¹æ®ç½‘ä¸Šåé¦ˆï¼ŒFLUX+PuLIDæ³¨å…¥ä¼šå¯¼è‡´é©¬èµ›å…‹å›¾å’Œæ®‹å›¾é—®é¢˜
        # æš‚æ—¶å®Œå…¨ç¦ç”¨æ³¨å…¥ï¼Œåªä¿ç•™promptå¢å¼ºåŠŸèƒ½
        
        # å¢åŠ è®¡æ•°å™¨ç”¨äºè°ƒè¯•ï¼ˆä½†ä¸è¿›è¡Œæ³¨å…¥ï¼‰
        if not hasattr(self, '_current_step'):
            self._current_step = 0
        self._current_step += 1
        
        # ğŸ”§ æš‚æ—¶å®Œå…¨è·³è¿‡æ³¨å…¥ï¼Œé¿å…å›¾åƒæŸå
        # TODO: æœªæ¥éœ€è¦ç ”ç©¶æ›´å®‰å…¨çš„æ³¨å…¥æ–¹æ³•
        if self.id_embedding is not None and self._current_step % 50 == 0:
            print(f"ğŸ“¢ PuLIDæ³¨å…¥å·²ç¦ç”¨ä»¥é˜²å›¾åƒæŸå (step {self._current_step})")
        
        # ä¸è¿›è¡Œä»»ä½•æ³¨å…¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹çš„encoder_hidden_states

        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads

        # é‡å¡‘æŸ¥è¯¢ã€é”®å’Œå€¼
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scale = head_dim ** -0.5
        attn_output = self._attention(query, key, value, attention_mask, scale)
        
        # é‡å¡‘è¾“å‡ºå¹¶åº”ç”¨æŠ•å½±
        attn_output = attn_output.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†attn.to_out
        if hasattr(attn, 'to_out') and attn.to_out is not None:
            if isinstance(attn.to_out, torch.nn.ModuleList):
                # å¦‚æœæ˜¯ModuleListï¼Œé€šå¸¸ç¬¬ä¸€ä¸ªæ˜¯çº¿æ€§å±‚ï¼Œç¬¬äºŒä¸ªæ˜¯dropout
                attn_output = attn.to_out[0](attn_output)
                if len(attn.to_out) > 1:
                    attn_output = attn.to_out[1](attn_output)
            else:
                # å¦‚æœæ˜¯å•ä¸ªæ¨¡å—ï¼ˆé€šå¸¸æ˜¯Sequentialæˆ–Linearï¼‰
                attn_output = attn.to_out(attn_output)

        # å¦‚æœè¾“å…¥æ˜¯4Dçš„ï¼Œå°†è¾“å‡ºé‡å¡‘ä¸º4D
        if input_ndim == 4:
            attn_output = attn_output.transpose(-1, -2).reshape(batch_size, channel, height, width)
        
        # ğŸ”§ FLUXå…¼å®¹æ€§ä¿®å¤ï¼šFLUX transformeræœŸæœ›è¿”å›ä¸¤ä¸ªå€¼
        # ç¬¬ä¸€ä¸ªå€¼æ˜¯hidden_stateså¤„ç†ç»“æœï¼Œç¬¬äºŒä¸ªå€¼å¿…é¡»ä¸encoder_hidden_statesç»´åº¦åŒ¹é…
        
        # ç¡®ä¿ç¬¬äºŒä¸ªè¿”å›å€¼ä¸encoder_hidden_statesç»´åº¦å®Œå…¨åŒ¹é…
        if encoder_hidden_states is not None and encoder_hidden_states is not hidden_states:
            # Cross-attention case: ç¬¬äºŒä¸ªè¿”å›å€¼å¿…é¡»ä¸encoder_hidden_statesç»´åº¦åŒ¹é…
            encoder_seq_len = encoder_hidden_states.shape[1]
            encoder_dim = encoder_hidden_states.shape[2]
            
            # å¦‚æœattn_outputç»´åº¦ä¸encoder_hidden_statesä¸åŒ¹é…ï¼Œéœ€è¦è°ƒæ•´
            if (attn_output.shape[1] != encoder_seq_len or 
                attn_output.shape[2] != encoder_dim):
                
                # åˆ›å»ºä¸encoder_hidden_statesç»´åº¦åŒ¹é…çš„è¾“å‡º
                context_attn_output = torch.zeros_like(encoder_hidden_states)
                
                # å°†attn_outputçš„å†…å®¹é€‚é…åˆ°context_attn_output
                min_seq_len = min(attn_output.shape[1], encoder_seq_len)
                min_dim = min(attn_output.shape[2], encoder_dim)
                
                context_attn_output[:, :min_seq_len, :min_dim] = attn_output[:, :min_seq_len, :min_dim]
            else:
                context_attn_output = attn_output
        else:
            # Self-attention case: ä¸¤ä¸ªè¾“å‡ºç›¸åŒ
            context_attn_output = attn_output
        
        return attn_output, context_attn_output
    
    def _fallback_attention(
        self,
        attn: nn.Module,
        hidden_states: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        temb: Optional[torch.Tensor] = None
    ):
        """
        æœ€ç®€å•çš„å›é€€æ³¨æ„åŠ›è®¡ç®—ï¼Œä¸FLUXå®Œå…¨å…¼å®¹
        """
        # ğŸ”§ ç›´æ¥ä½¿ç”¨åŸå§‹çš„FLUXæ³¨æ„åŠ›å¤„ç†å™¨é€»è¾‘
        from diffusers.models.attention_processor import AttnProcessor2_0
        
        # åˆ›å»ºé»˜è®¤å¤„ç†å™¨å¹¶è°ƒç”¨
        default_processor = AttnProcessor2_0()
        result = default_processor(attn, hidden_states, encoder_hidden_states, attention_mask, temb)
        
        # ğŸ”§ ç¡®ä¿è¿”å›ä¸¤ä¸ªå€¼ä»¥æ»¡è¶³FLUX transformerçš„æœŸæœ›
        if isinstance(result, tuple) and len(result) == 2:
            return result
        else:
            # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå¤åˆ¶ä¸ºä¸¤ä¸ªå€¼
            return result, result
    
    def _attention(self, query, key, value, attention_mask=None, scale=None):
        """
        è®¡ç®—æ³¨æ„åŠ›
        """
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_weights = torch.matmul(query * scale, key.transpose(-1, -2))
        
        # åº”ç”¨æ³¨æ„åŠ›æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask
            
        # åº”ç”¨softmaxè·å–æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.softmax(attn_weights, dim=-1)
        
        # è®¡ç®—æ³¨æ„åŠ›è¾“å‡º
        output = torch.matmul(attn_weights, value)
        
        return output


def create_flux_attn_processor(id_embedding=None, id_weight=1.0, ca_layer=None):
    """
    åˆ›å»ºé€‚ç”¨äºFLUXçš„PuLIDæ³¨æ„åŠ›å¤„ç†å™¨
    
    Args:
        id_embedding: ID embeddingå¼ é‡
        id_weight: ID embeddingæƒé‡/å¼ºåº¦
        ca_layer: PerceiverAttentionCAå±‚
        
    Returns:
        FluxAttenProc: é…ç½®å¥½çš„æ³¨æ„åŠ›å¤„ç†å™¨
    """
    return FluxAttenProc(id_embedding=id_embedding, id_weight=id_weight, ca_layer=ca_layer) 