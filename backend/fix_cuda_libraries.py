#!/usr/bin/env python3
"""
CUDAåº“ä¾èµ–ä¿®å¤è„šæœ¬
Fix CUDA Library Dependencies for Face Swap Optimization
"""

import os
import sys
import subprocess
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_cuda_libraries():
    """æ£€æŸ¥CUDAåº“çš„å¯ç”¨æ€§"""
    logger.info("ğŸ” Checking CUDA libraries...")
    
    required_libs = [
        "libcublasLt.so.12",
        "libcublas.so.12", 
        "libcudnn.so.8",
        "libcurand.so.10",
        "libcusolver.so.11",
        "libcusparse.so.12",
        "libnvjitlink.so.12"
    ]
    
    missing_libs = []
    
    for lib in required_libs:
        try:
            # å°è¯•ä½¿ç”¨ldconfigæŸ¥æ‰¾åº“
            result = subprocess.run(['ldconfig', '-p'], capture_output=True, text=True)
            if lib not in result.stdout:
                missing_libs.append(lib)
                logger.warning(f"âš ï¸ Missing library: {lib}")
            else:
                logger.info(f"âœ… Found library: {lib}")
        except Exception as e:
            logger.warning(f"âš ï¸ Error checking {lib}: {e}")
            missing_libs.append(lib)
    
    return missing_libs

def install_cuda_packages():
    """å®‰è£…CUDAç›¸å…³åŒ…"""
    logger.info("ğŸ”§ Installing CUDA packages...")
    
    cuda_packages = [
        "nvidia-cublas-cu12",
        "nvidia-cudnn-cu12", 
        "nvidia-cufft-cu12",
        "nvidia-curand-cu12",
        "nvidia-cusolver-cu12",
        "nvidia-cusparse-cu12",
        "nvidia-nvjitlink-cu12"
    ]
    
    for package in cuda_packages:
        try:
            logger.info(f"ğŸ“¦ Installing {package}...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                package, "--no-cache-dir", "--quiet"
            ])
            logger.info(f"âœ… Successfully installed {package}")
        except subprocess.CalledProcessError as e:
            logger.warning(f"âš ï¸ Failed to install {package}: {e}")
        except Exception as e:
            logger.warning(f"âš ï¸ Error installing {package}: {e}")

def update_onnxruntime():
    """æ›´æ–°onnxruntimeåˆ°GPUç‰ˆæœ¬"""
    logger.info("ğŸ”§ Updating onnxruntime...")
    
    try:
        # å¸è½½CPUç‰ˆæœ¬
        subprocess.check_call([
            sys.executable, "-m", "pip", "uninstall", 
            "onnxruntime", "-y", "--quiet"
        ])
        logger.info("âœ… Uninstalled CPU onnxruntime")
    except:
        logger.info("â„¹ï¸ CPU onnxruntime not found")
    
    try:
        # å®‰è£…GPUç‰ˆæœ¬
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", 
            "onnxruntime-gpu", "--no-cache-dir", "--quiet"
        ])
        logger.info("âœ… Installed GPU onnxruntime")
    except subprocess.CalledProcessError as e:
        logger.warning(f"âš ï¸ Failed to install onnxruntime-gpu: {e}")
        
        # å›é€€åˆ°CPUç‰ˆæœ¬
        try:
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                "onnxruntime", "--no-cache-dir", "--quiet"
            ])
            logger.info("âœ… Fallback: Installed CPU onnxruntime")
        except Exception as fallback_error:
            logger.error(f"âŒ Failed to install fallback onnxruntime: {fallback_error}")

def test_onnx_cuda():
    """æµ‹è¯•ONNX Runtime CUDAåŠŸèƒ½"""
    logger.info("ğŸ§ª Testing ONNX Runtime CUDA...")
    
    try:
        import onnxruntime as ort
        
        # æ£€æŸ¥å¯ç”¨æä¾›ç¨‹åº
        providers = ort.get_available_providers()
        logger.info(f"ğŸ“ Available providers: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            logger.info("âœ… CUDA provider is available")
            
            # å°è¯•åˆ›å»ºCUDAä¼šè¯
            try:
                # åˆ›å»ºç®€å•çš„æµ‹è¯•ä¼šè¯
                import tempfile
                import numpy as np
                
                # åˆ›å»ºæœ€å°çš„ONNXæ¨¡å‹è¿›è¡Œæµ‹è¯•
                try:
                    import onnx
                    from onnx import helper, TensorProto
                    
                    # åˆ›å»ºæ’ç­‰æ“ä½œæ¨¡å‹
                    input_tensor = helper.make_tensor_value_info('input', TensorProto.FLOAT, [1, 3, 64, 64])
                    output_tensor = helper.make_tensor_value_info('output', TensorProto.FLOAT, [1, 3, 64, 64])
                    identity_node = helper.make_node('Identity', ['input'], ['output'])
                    graph = helper.make_graph([identity_node], 'test_model', [input_tensor], [output_tensor])
                    model = helper.make_model(graph)
                    
                    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                    with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as temp_file:
                        onnx.save(model, temp_file.name)
                        
                        # æµ‹è¯•CUDAä¼šè¯
                        session = ort.InferenceSession(
                            temp_file.name,
                            providers=['CUDAExecutionProvider']
                        )
                        
                        # è¿è¡Œæ¨ç†æµ‹è¯•
                        input_data = np.random.randn(1, 3, 64, 64).astype(np.float32)
                        output = session.run(None, {'input': input_data})
                        
                        logger.info("âœ… CUDA inference test passed")
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        os.unlink(temp_file.name)
                        
                except ImportError:
                    logger.warning("âš ï¸ ONNX not available for detailed testing")
                except Exception as test_error:
                    logger.warning(f"âš ï¸ CUDA inference test failed: {test_error}")
                    
            except Exception as session_error:
                logger.warning(f"âš ï¸ CUDA session creation failed: {session_error}")
                
        else:
            logger.warning("âš ï¸ CUDA provider not available")
            
    except ImportError:
        logger.error("âŒ onnxruntime not available")
    except Exception as e:
        logger.error(f"âŒ ONNX Runtime test failed: {e}")

def create_cuda_fix_summary():
    """åˆ›å»ºCUDAä¿®å¤æ€»ç»“"""
    summary = """
# CUDAåº“ä¿®å¤æ€»ç»“

## æ‰§è¡Œçš„ä¿®å¤æ“ä½œ

### 1. æ£€æŸ¥CUDAåº“ä¾èµ–
- æ£€æŸ¥äº†å…³é”®çš„CUDA 12åº“æ–‡ä»¶
- è¯†åˆ«ç¼ºå¤±çš„åº“æ–‡ä»¶

### 2. å®‰è£…CUDAåŒ…
- nvidia-cublas-cu12: CUDA BLASåº“
- nvidia-cudnn-cu12: CUDAæ·±åº¦ç¥ç»ç½‘ç»œåº“  
- nvidia-cufft-cu12: CUDA FFTåº“
- nvidia-curand-cu12: CUDAéšæœºæ•°ç”Ÿæˆåº“
- nvidia-cusolver-cu12: CUDAçº¿æ€§ä»£æ•°åº“
- nvidia-cusparse-cu12: CUDAç¨€ç–çŸ©é˜µåº“
- nvidia-nvjitlink-cu12: CUDA JITé“¾æ¥åº“

### 3. æ›´æ–°ONNX Runtime
- å¸è½½CPUç‰ˆæœ¬çš„onnxruntime
- å®‰è£…GPUç‰ˆæœ¬çš„onnxruntime-gpu
- å¦‚æœå¤±è´¥åˆ™å›é€€åˆ°CPUç‰ˆæœ¬

### 4. æµ‹è¯•CUDAåŠŸèƒ½
- éªŒè¯CUDAæä¾›ç¨‹åºå¯ç”¨æ€§
- æµ‹è¯•CUDAæ¨ç†åŠŸèƒ½

## é¢„æœŸæ•ˆæœ

ä¿®å¤åçš„ç³»ç»Ÿåº”è¯¥èƒ½å¤Ÿï¼š
1. **å¯ç”¨GPUåŠ é€Ÿ** - æ¢è„¸æ¨¡å‹ä½¿ç”¨CUDAæ‰§è¡Œ
2. **æå‡æ€§èƒ½** - é€Ÿåº¦æå‡10-100å€
3. **æé«˜è´¨é‡** - GPUè®¡ç®—æ”¯æŒæ›´ç²¾ç»†çš„é¢éƒ¨å¤„ç†
4. **å‡å°‘é”™è¯¯** - è§£å†³libcublasLt.so.12ç¼ºå¤±é—®é¢˜

## éªŒè¯æ–¹æ³•

è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯ä¿®å¤æ•ˆæœï¼š
```bash
python -c "import onnxruntime as ort; print('CUDA available:', 'CUDAExecutionProvider' in ort.get_available_providers())"
```

å¦‚æœè¾“å‡º `CUDA available: True`ï¼Œåˆ™ä¿®å¤æˆåŠŸã€‚
"""
    
    with open('/tmp/cuda_fix_summary.md', 'w') as f:
        f.write(summary)
    
    logger.info("ğŸ“ CUDA fix summary saved to /tmp/cuda_fix_summary.md")

def main():
    """ä¸»ä¿®å¤æµç¨‹"""
    logger.info("ğŸš€ Starting CUDA library fix...")
    
    # 1. æ£€æŸ¥å½“å‰çŠ¶æ€
    missing_libs = check_cuda_libraries()
    
    # 2. å®‰è£…CUDAåŒ…
    install_cuda_packages()
    
    # 3. æ›´æ–°ONNX Runtime
    update_onnxruntime()
    
    # 4. æµ‹è¯•åŠŸèƒ½
    test_onnx_cuda()
    
    # 5. åˆ›å»ºæ€»ç»“
    create_cuda_fix_summary()
    
    # 6. é‡æ–°æ£€æŸ¥
    logger.info("ğŸ” Re-checking CUDA libraries after fix...")
    remaining_missing = check_cuda_libraries()
    
    if len(remaining_missing) < len(missing_libs):
        logger.info(f"âœ… Improvement: {len(missing_libs) - len(remaining_missing)} libraries fixed")
    
    if not remaining_missing:
        logger.info("ğŸ‰ All CUDA libraries are now available!")
    else:
        logger.warning(f"âš ï¸ Still missing {len(remaining_missing)} libraries: {remaining_missing}")
    
    logger.info("ğŸ CUDA library fix completed")

if __name__ == "__main__":
    main() 