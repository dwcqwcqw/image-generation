#!/usr/bin/env python3
"""
æµ‹è¯•åŠ¨æ¼«æ¨¡å‹é…ç½®ä¿®å¤ï¼ˆç®€åŒ–ç‰ˆï¼‰
åªæµ‹è¯•é…ç½®é€»è¾‘ï¼Œä¸å¯¼å…¥æœ‰é—®é¢˜çš„æ¨¡å—
"""

def test_base_models_config():
    """æµ‹è¯•åŸºç¡€æ¨¡å‹é…ç½®"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€æ¨¡å‹é…ç½®...")
    
    # å¤åˆ¶é…ç½®æ¥æµ‹è¯•
    BASE_MODELS = {
        "realistic": {
            "name": "çœŸäººé£æ ¼",
            "path": "/runpod-volume/flux_base",
            "lora_path": "/runpod-volume/lora/flux_nsfw",
            "lora_id": "flux_nsfw",
            "model_type": "flux"
        },
        "anime": {
            "name": "åŠ¨æ¼«é£æ ¼", 
            "path": "/runpod-volume/cartoon/waiNSFWIllustrious_v130.safetensors",
            "lora_path": "/runpod-volume/cartoon/lora/Gayporn.safetensor",
            "lora_id": "gayporn",
            "model_type": "diffusers"
        }
    }
    
    # éªŒè¯é…ç½®ç»“æ„
    for model_id, config in BASE_MODELS.items():
        required_keys = ["name", "path", "lora_path", "lora_id", "model_type"]
        for key in required_keys:
            if key not in config:
                print(f"âŒ æ¨¡å‹ {model_id} ç¼ºå°‘å¿…éœ€é”®: {key}")
                return False
        
        # éªŒè¯æ¨¡å‹ç±»å‹
        if config["model_type"] not in ["flux", "diffusers"]:
            print(f"âŒ æ¨¡å‹ {model_id} ç±»å‹æ— æ•ˆ: {config['model_type']}")
            return False
        
        print(f"âœ… {model_id}: {config['name']} ({config['model_type']})")
    
    return True

def test_lora_configs():
    """æµ‹è¯•LoRAé…ç½®åˆ†ç¦»"""
    print("\nğŸ§ª æµ‹è¯•LoRAé…ç½®åˆ†ç¦»...")
    
    # LoRAé…ç½®ç¤ºä¾‹
    AVAILABLE_LORAS = {
        # çœŸäººé£æ ¼LoRA
        "flux_nsfw": {"base_model": "realistic"},
        "chastity_cage": {"base_model": "realistic"},
        "dynamic_penis": {"base_model": "realistic"},
        
        # åŠ¨æ¼«é£æ ¼LoRA
        "gayporn": {"base_model": "anime"}
    }
    
    BASE_MODELS = {
        "realistic": {"model_type": "flux"},
        "anime": {"model_type": "diffusers"}
    }
    
    # åˆ†ç»„æµ‹è¯•
    flux_loras = []
    diffusers_loras = []
    
    for lora_id, lora_info in AVAILABLE_LORAS.items():
        base_model = lora_info.get("base_model")
        model_type = BASE_MODELS.get(base_model, {}).get("model_type")
        
        if model_type == "flux":
            flux_loras.append(lora_id)
        elif model_type == "diffusers":
            diffusers_loras.append(lora_id)
    
    print(f"âœ… FLUX LoRAs: {flux_loras}")
    print(f"âœ… Diffusers LoRAs: {diffusers_loras}")
    
    # éªŒè¯åˆ†ç¦»æ­£ç¡®
    if len(flux_loras) > 0 and len(diffusers_loras) > 0:
        print("âœ… LoRAåˆ†ç¦»æ­£ç¡®")
        return True
    else:
        print("âŒ LoRAåˆ†ç¦»å¤±è´¥")
        return False

def test_model_switching_logic():
    """æµ‹è¯•æ¨¡å‹åˆ‡æ¢é€»è¾‘"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åˆ‡æ¢é€»è¾‘...")
    
    # æ¨¡æ‹Ÿå½“å‰çŠ¶æ€
    current_base_model = None  # åˆå§‹çŠ¶æ€ï¼šæ— æ¨¡å‹
    
    def should_switch_model(requested_model, current_model):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ‡æ¢æ¨¡å‹"""
        return requested_model != current_model
    
    # æµ‹è¯•åœºæ™¯
    scenarios = [
        ("realistic", None, True, "é¦–æ¬¡åŠ è½½çœŸäººæ¨¡å‹"),
        ("anime", None, True, "é¦–æ¬¡åŠ è½½åŠ¨æ¼«æ¨¡å‹"),  
        ("realistic", "realistic", False, "åŒä¸€æ¨¡å‹ä¸åˆ‡æ¢"),
        ("anime", "realistic", True, "åˆ‡æ¢åˆ°åŠ¨æ¼«æ¨¡å‹"),
        ("realistic", "anime", True, "åˆ‡æ¢åˆ°çœŸäººæ¨¡å‹")
    ]
    
    for requested, current, expected, description in scenarios:
        result = should_switch_model(requested, current)
        status = "âœ…" if result == expected else "âŒ"
        print(f"{status} {description}: {result}")
        
        if result != expected:
            return False
    
    return True

def test_precision_fix():
    """æµ‹è¯•ç²¾åº¦ä¿®å¤é€»è¾‘"""
    print("\nğŸ§ª æµ‹è¯•ç²¾åº¦ä¿®å¤é€»è¾‘...")
    
    # æµ‹è¯•float32å¼ºåˆ¶ä½¿ç”¨
    def get_torch_dtype_for_anime():
        """åŠ¨æ¼«æ¨¡å‹ä½¿ç”¨çš„ç²¾åº¦ç±»å‹"""
        import torch
        return torch.float32  # å¼ºåˆ¶float32ï¼Œé¿å…Halfç²¾åº¦é—®é¢˜
    
    try:
        import torch
        dtype = get_torch_dtype_for_anime()
        if dtype == torch.float32:
            print("âœ… åŠ¨æ¼«æ¨¡å‹å¼ºåˆ¶ä½¿ç”¨float32ï¼Œé¿å…LayerNormKernelImplé”™è¯¯")
            return True
        else:
            print(f"âŒ é¢„æœŸfloat32ï¼Œå®é™…: {dtype}")
            return False
    except Exception as e:
        print(f"âŒ ç²¾åº¦æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œé…ç½®æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹åŠ¨æ¼«æ¨¡å‹é…ç½®éªŒè¯...")
    
    tests = [
        ("åŸºç¡€æ¨¡å‹é…ç½®", test_base_models_config),
        ("LoRAé…ç½®åˆ†ç¦»", test_lora_configs),
        ("æ¨¡å‹åˆ‡æ¢é€»è¾‘", test_model_switching_logic),
        ("ç²¾åº¦ä¿®å¤é€»è¾‘", test_precision_fix)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ“Š é…ç½®æµ‹è¯•ç»“æœ:")
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ‰€æœ‰é…ç½®æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âš ï¸  å­˜åœ¨é…ç½®é—®é¢˜éœ€è¦ä¿®å¤")
        return False

if __name__ == "__main__":
    success = main()
    print("\nğŸ’¡ ä¸»è¦ä¿®å¤å†…å®¹:")
    print("1. âœ… ç§»é™¤å¯åŠ¨æ—¶çš„æ¨¡å‹é¢„çƒ­ï¼Œæ”¹ä¸ºæŒ‰éœ€åŠ è½½")
    print("2. âœ… åŠ¨æ¼«æ¨¡å‹å¼ºåˆ¶ä½¿ç”¨float32ç²¾åº¦ï¼Œé¿å…Halfç²¾åº¦é”™è¯¯")
    print("3. âœ… LoRAæŒ‰æ¨¡å‹ç±»å‹åˆ†ç¦»ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜")
    print("4. âœ… åŠ¨æ¼«æ¨¡å‹æ”¯æŒCompelé•¿promptå¤„ç†")
    print("5. âœ… ä¿®å¤æ¨¡å‹åˆ‡æ¢é€»è¾‘å’Œè·¯å¾„é…ç½®")
    
    exit(0 if success else 1) 