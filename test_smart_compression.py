#!/usr/bin/env python3
"""
æ™ºèƒ½Promptå‹ç¼©æµ‹è¯•è„šæœ¬
æµ‹è¯•å°†è¶…é•¿promptå‹ç¼©åˆ°77 tokenä»¥å†…çš„æ•ˆæœ
"""

import re

def compress_prompt_to_77_tokens(prompt: str, max_tokens: int = 75) -> str:
    """
    æ™ºèƒ½å‹ç¼©promptåˆ°æŒ‡å®štokenæ•°é‡ä»¥å†…
    ä¿ç•™æœ€é‡è¦çš„å…³é”®è¯å’Œæè¿°
    """
    # è®¡ç®—å½“å‰tokenæ•°é‡
    token_pattern = r'\w+|[^\w\s]'
    current_tokens = len(re.findall(token_pattern, prompt.lower()))
    
    if current_tokens <= max_tokens:
        return prompt
    
    print(f"ğŸ”§ å‹ç¼©prompt: {current_tokens} tokens -> {max_tokens} tokens")
    
    # å®šä¹‰é‡è¦æ€§æƒé‡
    priority_keywords = {
        # è´¨é‡æ ‡ç­¾ - æœ€é«˜ä¼˜å…ˆçº§
        'quality': ['masterpiece', 'best quality', 'amazing quality', 'high quality', 'ultra quality'],
        # ä¸»ä½“æè¿° - é«˜ä¼˜å…ˆçº§  
        'subject': ['man', 'boy', 'male', 'muscular', 'handsome', 'lean', 'naked', 'nude'],
        # èº«ä½“éƒ¨ä½ - ä¸­é«˜ä¼˜å…ˆçº§
        'anatomy': ['torso', 'chest', 'abs', 'penis', 'erect', 'flaccid', 'body'],
        # åŠ¨ä½œå§¿æ€ - ä¸­ä¼˜å…ˆçº§
        'pose': ['reclining', 'lying', 'sitting', 'standing', 'pose', 'position'],
        # ç¯å¢ƒé“å…· - ä¸­ä¼˜å…ˆçº§
        'environment': ['bed', 'sheets', 'satin', 'luxurious', 'room', 'background'],
        # å…‰å½±æ•ˆæœ - ä½ä¼˜å…ˆçº§
        'lighting': ['lighting', 'illuminated', 'soft', 'moody', 'warm', 'cinematic'],
        # æƒ…æ„Ÿè¡¨è¾¾ - ä½ä¼˜å…ˆçº§
        'emotion': ['serene', 'intense', 'confident', 'contemplation', 'allure']
    }
    
    # åˆ†è¯å¹¶åˆ†ç±»
    words = prompt.split()
    categorized_words = {category: [] for category in priority_keywords.keys()}
    uncategorized_words = []
    
    for word in words:
        word_lower = word.lower().strip('.,!?;:')
        categorized = False
        
        for category, keywords in priority_keywords.items():
            if any(keyword in word_lower for keyword in keywords):
                categorized_words[category].append(word)
                categorized = True
                break
        
        if not categorized:
            uncategorized_words.append(word)
    
    # æŒ‰ä¼˜å…ˆçº§é‡å»ºprompt
    compressed_parts = []
    remaining_tokens = max_tokens
    
    # ä¼˜å…ˆçº§é¡ºåº
    priority_order = ['quality', 'subject', 'anatomy', 'pose', 'environment', 'lighting', 'emotion']
    
    for category in priority_order:
        if remaining_tokens <= 0:
            break
            
        category_words = categorized_words[category]
        if category_words:
            # è®¡ç®—è¿™ä¸ªç±»åˆ«çš„tokenæ•°
            category_text = ' '.join(category_words)
            category_tokens = len(re.findall(token_pattern, category_text.lower()))
            
            if category_tokens <= remaining_tokens:
                compressed_parts.extend(category_words)
                remaining_tokens -= category_tokens
            else:
                # éƒ¨åˆ†æ·»åŠ æœ€é‡è¦çš„è¯
                for word in category_words:
                    word_tokens = len(re.findall(token_pattern, word.lower()))
                    if word_tokens <= remaining_tokens:
                        compressed_parts.append(word)
                        remaining_tokens -= word_tokens
                    else:
                        break
    
    # å¦‚æœè¿˜æœ‰å‰©ä½™ç©ºé—´ï¼Œæ·»åŠ æœªåˆ†ç±»çš„é‡è¦è¯
    for word in uncategorized_words:
        if remaining_tokens <= 0:
            break
        word_tokens = len(re.findall(token_pattern, word.lower()))
        if word_tokens <= remaining_tokens:
            compressed_parts.append(word)
            remaining_tokens -= word_tokens
    
    compressed_prompt = ' '.join(compressed_parts)
    final_tokens = len(re.findall(token_pattern, compressed_prompt.lower()))
    
    print(f"âœ… å‹ç¼©å®Œæˆ: '{compressed_prompt}' ({final_tokens} tokens)")
    return compressed_prompt

def count_tokens(text: str) -> int:
    """è®¡ç®—tokenæ•°é‡"""
    token_pattern = r'\w+|[^\w\s]'
    return len(re.findall(token_pattern, text.lower()))

def test_compression():
    """æµ‹è¯•å‹ç¼©åŠŸèƒ½"""
    print("ğŸ§ª æ™ºèƒ½Promptå‹ç¼©æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            'name': 'è¶…é•¿prompt (114 tokens)',
            'prompt': 'masterpiece, best quality, amazing quality, A lean, muscular, and handsome man reclining on a bed with luxurious satin sheets. His chiseled torso is partially illuminated by soft, moody lighting that accentuates the contours of his muscles. One arm is tucked under his head, creating a relaxed yet confident pose. His expression is serene yet intense, with a gaze that suggests quiet contemplation. The satin sheets gently reflect the light, adding a sense of elegance and intimacy to the scene. The overall atmosphere is warm, sensual, and cinematic, evoking a timeless allure. erect penis, flaccid penis'
        },
        {
            'name': 'ä¸­ç­‰é•¿åº¦prompt (47 tokens)',
            'prompt': 'masterpiece, best quality, muscular handsome man, naked, sitting on bed, soft lighting, detailed anatomy, erect penis'
        },
        {
            'name': 'çŸ­prompt (11 tokens)',
            'prompt': 'masterpiece, best quality, amazing quality, a naked boy'
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {test_case['name']}")
        print("-" * 40)
        
        original = test_case['prompt']
        original_tokens = count_tokens(original)
        
        print(f"åŸå§‹prompt ({original_tokens} tokens):")
        print(f"'{original}'")
        print()
        
        if original_tokens > 75:
            compressed = compress_prompt_to_77_tokens(original, max_tokens=75)
            compressed_tokens = count_tokens(compressed)
            
            print(f"\nğŸ“Š å‹ç¼©ç»“æœ:")
            print(f"  åŸå§‹: {original_tokens} tokens")
            print(f"  å‹ç¼©: {compressed_tokens} tokens")
            print(f"  å‹ç¼©ç‡: {((original_tokens - compressed_tokens) / original_tokens * 100):.1f}%")
            
            # åˆ†æä¿ç•™çš„å…³é”®è¯
            original_words = set(original.lower().split())
            compressed_words = set(compressed.lower().split())
            preserved_words = compressed_words.intersection(original_words)
            lost_words = original_words - compressed_words
            
            print(f"  ä¿ç•™è¯æ±‡: {len(preserved_words)}/{len(original_words)} ({len(preserved_words)/len(original_words)*100:.1f}%)")
            if lost_words:
                print(f"  ä¸¢å¤±è¯æ±‡: {', '.join(sorted(lost_words)[:10])}{'...' if len(lost_words) > 10 else ''}")
        else:
            print("âœ… æ— éœ€å‹ç¼©ï¼Œå·²åœ¨75 tokené™åˆ¶å†…")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æµ‹è¯•å®Œæˆï¼æ™ºèƒ½å‹ç¼©åŠŸèƒ½å¯ä»¥æœ‰æ•ˆé¿å…é»‘å›¾é—®é¢˜")

if __name__ == "__main__":
    test_compression() 