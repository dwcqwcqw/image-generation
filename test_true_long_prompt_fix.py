#!/usr/bin/env python3
"""
çœŸæ­£çš„é•¿promptå¤„ç†ä¿®å¤æµ‹è¯•è„šæœ¬
æµ‹è¯•åˆ†æ®µç¼–ç +åˆå¹¶æ–¹æ¡ˆï¼Œå½»åº•è§£å†³77 tokenæˆªæ–­é—®é¢˜
"""

import os
import json
import time
import re

def analyze_prompt_complexity(prompt: str):
    """åˆ†æpromptçš„å¤æ‚åº¦"""
    # è®¡ç®—tokenæ•°é‡
    token_pattern = r'\w+|[^\w\s]'
    tokens = re.findall(token_pattern, prompt.lower())
    
    # åˆ†æå†…å®¹ç±»å‹
    words = prompt.lower().split()
    artistic_terms = ['masterpiece', 'detailed', 'quality', 'illustration', 'artwork', 'professional']
    character_terms = ['muscular', 'handsome', 'warrior', 'hero', 'man', 'boy']
    environment_terms = ['forest', 'lighting', 'background', 'atmospheric', 'cinematic']
    style_terms = ['anime', 'fantasy', 'realistic', 'photorealistic', '4k', 'hd']
    
    categories = {
        'artistic': sum(1 for term in artistic_terms if term in words),
        'character': sum(1 for term in character_terms if term in words),
        'environment': sum(1 for term in environment_terms if term in words),
        'style': sum(1 for term in style_terms if term in words)
    }
    
    return {
        'total_chars': len(prompt),
        'total_tokens': len(tokens),
        'word_count': len(words),
        'categories': categories,
        'complexity_score': len(tokens) + sum(categories.values())
    }

def create_ultra_long_prompt():
    """åˆ›å»ºè¶…é•¿promptæµ‹è¯•ï¼ˆ>200 tokensï¼‰"""
    prompt = """masterpiece, best quality, amazing quality, ultra detailed, extremely detailed, 
    professional anime artwork, studio quality illustration, high resolution 4K rendering,
    handsome muscular warrior man with detailed facial features and expressions, 
    strong defined jawline, piercing blue eyes with detailed iris patterns,
    detailed flowing hair with individual strand rendering, perfect anatomy proportions,
    wearing intricate fantasy armor with golden engravings and detailed metalwork,
    ornate shoulder plates with dragon motifs, detailed chainmail underneath,
    standing heroically in a mystical enchanted ancient forest setting,
    warm golden sunlight filtering through massive tree branches and leaves,
    atmospheric volumetric lighting with god rays and particle effects,
    magical floating glowing orbs and sparkles in the air around him,
    detailed background featuring ancient stone ruins covered in glowing magical runes,
    moss-covered pillars and archways with intricate carvings and weathering,
    fantasy art style with photorealistic texture work and shading,
    dynamic powerful heroic pose with confident stance and determined expression,
    detailed texture work on armor reflecting light and showing wear patterns,
    rich vibrant colors with perfect color harmony and saturation,
    detailed shadows and highlights with perfect contrast and depth,
    cinematic composition with rule of thirds and leading lines,
    dramatic lighting effects creating mood and atmosphere,
    photorealistic rendering style with perfect material properties,
    professional digital art techniques with advanced shading methods"""
    
    return prompt

def test_true_long_prompt_processing():
    """æµ‹è¯•çœŸæ­£çš„é•¿promptå¤„ç†"""
    print("ğŸ¯ çœŸæ­£çš„é•¿promptå¤„ç†ä¿®å¤æµ‹è¯•")
    print("=" * 70)
    
    # åˆ›å»ºè¶…é•¿prompt
    ultra_long_prompt = create_ultra_long_prompt()
    analysis = analyze_prompt_complexity(ultra_long_prompt)
    
    print(f"ğŸ“Š è¶…é•¿promptåˆ†æ:")
    print(f"  æ€»å­—ç¬¦æ•°: {analysis['total_chars']}")
    print(f"  æ€»tokenæ•°: {analysis['total_tokens']} (è¿œè¶…77é™åˆ¶)")
    print(f"  å•è¯æ•°é‡: {analysis['word_count']}")
    print(f"  å¤æ‚åº¦è¯„åˆ†: {analysis['complexity_score']}")
    print(f"  å†…å®¹åˆ†ç±»: {analysis['categories']}")
    
    # é¢„è®¡ç®—åˆ†æ®µæ•°é‡
    expected_segments = (analysis['total_tokens'] + 74) // 75  # å‘ä¸Šå–æ•´
    print(f"  é¢„æœŸåˆ†æ®µæ•°: {expected_segments} æ®µ")
    
    # æµ‹è¯•1ï¼šåŠ¨æ¼«æ¨¡å‹ + LoRA + è¶…é•¿prompt (>200 tokens)
    test_ultra_long = {
        "task_type": "text-to-image",
        "prompt": ultra_long_prompt,
        "negativePrompt": "bad quality, worst quality, blur, sketch, monochrome, lowres",
        "width": 1024,
        "height": 1024,
        "steps": 25,
        "cfgScale": 7.0,
        "seed": 42,
        "numImages": 1,
        "baseModel": "anime",
        "lora_config": {"multiple_views": 1.0}
    }
    
    # æµ‹è¯•2ï¼šä¸­ç­‰é•¿åº¦promptæµ‹è¯• (50-75 tokens)
    medium_prompt = """masterpiece, best quality, detailed anime illustration, 
    handsome muscular man, detailed facial features, strong jawline, blue eyes,
    fantasy armor with golden details, heroic pose, mystical forest background,
    warm lighting, atmospheric effects, professional artwork, high quality"""
    
    medium_analysis = analyze_prompt_complexity(medium_prompt)
    
    test_medium_length = {
        "task_type": "text-to-image",
        "prompt": medium_prompt,
        "negativePrompt": "bad quality, worst quality",
        "width": 1024,
        "height": 1024,
        "steps": 25,
        "cfgScale": 7.0,
        "seed": 42,
        "numImages": 1,
        "baseModel": "anime",
        "lora_config": {"gayporn": 1.0}
    }
    
    print(f"\nğŸ§ª æµ‹è¯•ç”¨ä¾‹:")
    print(f"1ï¸âƒ£ è¶…é•¿promptæµ‹è¯•:")
    print(f"   Tokens: {analysis['total_tokens']} (é¢„æœŸåˆ†æ®µ: {expected_segments})")
    print(f"   æœŸæœ›æ—¥å¿—: 'ä½¿ç”¨åˆ†æ®µç¼–ç å¤„ç†è¶…é•¿prompt'")
    print(f"   æœŸæœ›è¡Œä¸º: æ— æˆªæ–­è­¦å‘Šï¼Œåˆ†æ®µå¤„ç†")
    
    print(f"\n2ï¸âƒ£ ä¸­ç­‰é•¿åº¦promptæµ‹è¯•:")
    print(f"   Tokens: {medium_analysis['total_tokens']} (é¢„æœŸ: æ ‡å‡†å¤„ç†)")
    print(f"   æœŸæœ›æ—¥å¿—: 'æ ‡å‡†promptå¤„ç†ï¼ˆLoRAå…¼å®¹ï¼‰'")
    print(f"   æœŸæœ›è¡Œä¸º: æ­£å¸¸å•æ¬¡ç¼–ç ")
    
    return test_ultra_long, test_medium_length, {
        'ultra_analysis': analysis,
        'medium_analysis': medium_analysis
    }

def show_fix_details():
    """æ˜¾ç¤ºä¿®å¤æŠ€æœ¯ç»†èŠ‚"""
    print(f"\nğŸ”§ çœŸæ­£é•¿promptå¤„ç†çš„æ ¸å¿ƒæŠ€æœ¯:")
    print(f"=" * 70)
    print(f"ä¿®å¤ç­–ç•¥:")
    print(f"  ğŸ¯ é—®é¢˜æ ¹å› : CLIP tokenizerç¡¬æ€§77 tokené™åˆ¶")
    print(f"  ğŸ’¡ è§£å†³æ–¹æ¡ˆ: åˆ†æ®µç¼–ç  + embeddingåˆå¹¶")
    print(f"  ğŸ”§ æŠ€æœ¯è·¯å¾„: æ™ºèƒ½åˆ†è¯ â†’ æ®µè½ç¼–ç  â†’ å‘é‡å¹³å‡")
    
    print(f"\næ ¸å¿ƒç®—æ³•:")
    print(f"  1ï¸âƒ£ æ™ºèƒ½åˆ†è¯:")
    print(f"     â”£â” æŒ‰å•è¯è¾¹ç•Œåˆ†å‰²prompt")
    print(f"     â”£â” è®¡ç®—æ¯ä¸ªå•è¯çš„tokenæ•°é‡")
    print(f"     â”—â” ç¡®ä¿æ¯æ®µä¸è¶…è¿‡75 tokens")
    
    print(f"  2ï¸âƒ£ åˆ†æ®µç¼–ç :")
    print(f"     â”£â” ä¸ºæ¯æ®µç‹¬ç«‹ç”Ÿæˆembeddings")
    print(f"     â”£â” ä¿æŒLoRAå…¼å®¹æ€§")
    print(f"     â”—â” åŒ…å«pooled embeddings")
    
    print(f"  3ï¸âƒ£ å‘é‡åˆå¹¶:")
    print(f"     â”£â” ä½¿ç”¨torch.mean()å¹³å‡åˆå¹¶")
    print(f"     â”£â” ä¿æŒembeddingç»´åº¦ä¸€è‡´")
    print(f"     â”—â” è¯­ä¹‰ä¿¡æ¯ç»¼åˆä¿ç•™")
    
    print(f"\nå…³é”®ä»£ç é€»è¾‘:")
    print(f"  ```python")
    print(f"  # åˆ†æ®µå¤„ç†")
    print(f"  segments = split_prompt_by_tokens(prompt, max_tokens=75)")
    print(f"  embeddings = [encode_segment(seg) for seg in segments]")
    print(f"  combined = torch.mean(torch.cat(embeddings), dim=0)")
    print(f"  ```")

def show_expected_logs():
    """æ˜¾ç¤ºé¢„æœŸçš„æ—¥å¿—æ¨¡å¼"""
    print(f"\nğŸ“ å…³é”®æ—¥å¿—æ ‡è¯† (ä¿®å¤å):")
    print(f"=" * 70)
    print(f"âœ… æˆåŠŸæ ‡è¯†:")
    print(f"  ğŸ” 'ğŸ§¬ ä½¿ç”¨åˆ†æ®µç¼–ç å¤„ç†è¶…é•¿prompt...'")
    print(f"  ğŸ” 'ğŸ“ é•¿promptåˆ†ä¸º X æ®µå¤„ç†'")
    print(f"  ğŸ” 'ğŸ”¤ å¤„ç†æ®µ 1/X: XXX chars'")
    print(f"  ğŸ” 'âœ… åˆ†æ®µé•¿promptå¤„ç†å®Œæˆï¼ˆLoRAå…¼å®¹ï¼‰'")
    print(f"  ğŸ” æ— ä»»ä½•æˆªæ–­è­¦å‘Šä¿¡æ¯")
    
    print(f"\nâŒ é—®é¢˜æ ‡è¯†:")
    print(f"  ğŸš¨ 'Token indices sequence length is longer than...'")
    print(f"  ğŸš¨ 'The following part of your input was truncated...'")
    print(f"  ğŸš¨ 'âš ï¸  åˆ†æ®µé•¿promptå¤„ç†å¤±è´¥'")
    print(f"  ğŸš¨ 'ğŸ“ å›é€€åˆ°æ ‡å‡†å¤„ç†æ¨¡å¼'")
    
    print(f"\nğŸ¯ éªŒè¯è¦ç‚¹:")
    print(f"  âœ… è¶…é•¿prompt (>200 tokens) å®Œå…¨æ— æˆªæ–­")
    print(f"  âœ… å›¾åƒåŒ…å«æ‰€æœ‰promptæè¿°çš„å…ƒç´ ")
    print(f"  âœ… LoRAæ•ˆæœæ­£å¸¸ä½“ç°")
    print(f"  âœ… ç”Ÿæˆé€Ÿåº¦åˆç†ï¼Œæ— æ˜æ˜¾å»¶è¿Ÿ")

def show_benefits():
    """æ˜¾ç¤ºä¿®å¤å¸¦æ¥çš„å¥½å¤„"""
    print(f"\nğŸŒŸ ä¿®å¤åçš„ä¼˜åŠ¿:")
    print(f"=" * 70)
    print(f"åˆ›ä½œè‡ªç”±åº¦:")
    print(f"  ğŸ¨ å¯ä»¥å†™å‡ºéå¸¸è¯¦ç»†çš„è‰ºæœ¯æè¿°")
    print(f"  ğŸ“ æ”¯æŒå¤æ‚çš„åœºæ™¯å’Œè§’è‰²è®¾å®š")
    print(f"  ğŸ­ å…è®¸å¤šå±‚æ¬¡çš„é£æ ¼æŒ‡å¯¼")
    print(f"  ğŸ–¼ï¸  èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶å›¾åƒç»†èŠ‚")
    
    print(f"\næŠ€æœ¯ä¼˜åŠ¿:")
    print(f"  âš¡ çœŸæ­£è§£å†³tokené™åˆ¶é—®é¢˜")
    print(f"  ğŸ”§ ä¸LoRAå®Œå…¨å…¼å®¹")
    print(f"  ğŸ›¡ï¸  æœ‰å®Œæ•´çš„é”™è¯¯å¤„ç†")
    print(f"  ğŸ“Š æ”¯æŒä»»æ„é•¿åº¦çš„prompt")
    
    print(f"\nç”¨æˆ·ä½“éªŒ:")
    print(f"  ğŸš€ æ— éœ€æ‹…å¿ƒpromptè¢«æˆªæ–­")
    print(f"  ğŸ’« åˆ›æ„è¡¨è¾¾ä¸å—æŠ€æœ¯é™åˆ¶")
    print(f"  ğŸ¯ ä¸“ä¸šçº§åˆ«çš„ç²¾ç»†æ§åˆ¶")
    print(f"  ğŸ”® è§£é”é«˜çº§åˆ›ä½œå¯èƒ½æ€§")

if __name__ == "__main__":
    print("ğŸš€ çœŸæ­£çš„é•¿promptå¤„ç†ä¿®å¤æµ‹è¯•")
    print("=" * 70)
    
    # ç”Ÿæˆæµ‹è¯•
    ultra_test, medium_test, analyses = test_true_long_prompt_processing()
    
    # æ˜¾ç¤ºæŠ€æœ¯ç»†èŠ‚
    show_fix_details()
    
    # æ˜¾ç¤ºé¢„æœŸæ—¥å¿—
    show_expected_logs()
    
    # æ˜¾ç¤ºå¥½å¤„
    show_benefits()
    
    print(f"\nğŸ¯ æ€»ç»“:")
    print(f"è¿™æ¬¡ä¿®å¤å½»åº•è§£å†³äº†é•¿promptæˆªæ–­é—®é¢˜ï¼Œ")
    print(f"é€šè¿‡åˆ†æ®µç¼–ç +åˆå¹¶çš„æ–¹å¼ï¼Œå®ç°äº†çœŸæ­£çš„é•¿promptæ”¯æŒï¼Œ")
    print(f"è®©ç”¨æˆ·å¯ä»¥è‡ªç”±è¡¨è¾¾å¤æ‚çš„åˆ›æ„æƒ³æ³•ï¼")
    
    # ä¿å­˜æµ‹è¯•å‚æ•°
    with open('true_long_prompt_tests.json', 'w', encoding='utf-8') as f:
        json.dump({
            'ultra_long_test': ultra_test,
            'medium_length_test': medium_test,
            'analyses': analyses
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æµ‹è¯•å‚æ•°å·²ä¿å­˜åˆ° true_long_prompt_tests.json") 